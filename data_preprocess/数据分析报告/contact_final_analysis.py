#!/usr/bin/env python
"""
Contactæ•°æ®é›†æœ€ç»ˆåˆ†æè„šæœ¬
æ•´åˆäº†ä¹‹å‰ç‰ˆæœ¬çš„ä¼˜ç‚¹ï¼Œæä¾›å¯é çš„åˆ†æç»“æœ
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

def analyze_contact_data():
    """
    Contactæ•°æ®é›†çš„ç¡®å®šæ€§åˆ†æ
    """
    print("ğŸ” Contactæ•°æ®é›† - æœ€ç»ˆåˆ†ææŠ¥å‘Š")
    print("="*60)
    
    # 1. åŠ è½½æ•°æ®
    try:
        train_df = pd.read_parquet("./dataset/anomaly_filtered_Traindata/train_normal.parquet")
        test_df = pd.read_parquet("./dataset/split_data/test.parquet")
        test_labels = pd.read_parquet("./dataset/anomaly_labeld_testData/test_labeled.parquet")
        print("âœ… æ•°æ®åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # 2. æ•°æ®åŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(f"   è®­ç»ƒæ•°æ®: {train_df.shape}")
    print(f"   æµ‹è¯•æ•°æ®: {test_df.shape}")
    print(f"   æµ‹è¯•æ ‡ç­¾: {test_labels.shape}")
    
    # 3. è·å–æœ‰æ•ˆç‰¹å¾åˆ—
    exclude_cols = ['TimeStamp', 'timestamp', 'index', 'Unnamed: 0']
    numeric_cols = [col for col in train_df.select_dtypes(include=[np.number]).columns 
                   if col not in exclude_cols]
    print(f"   æœ‰æ•ˆç‰¹å¾æ•°: {len(numeric_cols)}")
    
    # 4. æ•°æ®è´¨é‡æ£€æŸ¥
    train_nans = train_df[numeric_cols].isnull().sum().sum()
    test_nans = test_df[numeric_cols].isnull().sum().sum()
    print(f"   è®­ç»ƒæ•°æ®NaN: {train_nans}")
    print(f"   æµ‹è¯•æ•°æ®NaN: {test_nans}")
    
    # 6. å¼‚å¸¸åˆ†å¸ƒåˆ†æ
    normal_mask = test_labels['label'] == 0
    anomaly_mask = test_labels['label'] == 1
    
    print(f"\nğŸ¯ å¼‚å¸¸åˆ†å¸ƒ:")
    print(f"   æ­£å¸¸æ ·æœ¬: {normal_mask.sum()} ({normal_mask.sum()/len(test_labels)*100:.1f}%)")
    print(f"   å¼‚å¸¸æ ·æœ¬: {anomaly_mask.sum()} ({anomaly_mask.sum()/len(test_labels)*100:.1f}%)")
    
    # 7. ç‰¹å¾åŒºåˆ†åŠ›åˆ†æ
    print(f"\nğŸ” ç‰¹å¾åŒºåˆ†åŠ›åˆ†æ:")
    feature_scores = []
    
    for col in numeric_cols:
        normal_data = test_df[normal_mask][col]
        anomaly_data = test_df[anomaly_mask][col]
        
        # æ ‡å‡†åŒ–å‡å€¼å·®å¼‚
        mean_diff = abs(normal_data.mean() - anomaly_data.mean())
        std_pooled = np.sqrt((normal_data.std()**2 + anomaly_data.std()**2) / 2) + 1e-8
        
        discrimination_score = mean_diff / std_pooled
        feature_scores.append((col, discrimination_score))
    
    # æ’åºç‰¹å¾
    feature_scores.sort(key=lambda x: x[1], reverse=True)
    
    print(f"   Top 5 æœ€æœ‰åŒºåˆ†åŠ›çš„ç‰¹å¾:")
    for i, (feature, score) in enumerate(feature_scores[:5]):
        print(f"     {i+1}. {feature}: {score:.4f}")
    
    avg_score = np.mean([score for _, score in feature_scores])
    print(f"   å¹³å‡åŒºåˆ†åŠ›: {avg_score:.4f}")
    
    # 8. ç›¸å…³æ€§åˆ†æ
    print(f"\nğŸ”— ç›¸å…³æ€§åˆ†æ:")
    corr_matrix = train_df[numeric_cols].corr()
    high_corr_count = 0
    
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > 0.9:
                high_corr_count += 1
    
    print(f"   é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ (>0.9): {high_corr_count}")
    
    # 9. PCAåˆ†æ
    print(f"\nğŸ§® PCAåˆ†æ:")
    scaler = StandardScaler()
    train_scaled = scaler.fit_transform(train_df[numeric_cols])
    test_scaled = scaler.transform(test_df[numeric_cols])
    
    n_components = min(10, len(numeric_cols))
    pca = PCA(n_components=n_components)
    train_pca = pca.fit_transform(train_scaled)
    test_pca = pca.transform(test_scaled)
    
    print(f"   å‰5ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[:5]}")
    print(f"   ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.3f}")
    
    # 10. æ—¶åºæ¨¡å¼åˆ†æï¼ˆä½¿ç”¨æœ€ä½³ç‰¹å¾ï¼‰
    print(f"\nâ° æ—¶åºæ¨¡å¼åˆ†æ:")
    best_feature = feature_scores[0][0]  # æœ€æœ‰åŒºåˆ†åŠ›çš„ç‰¹å¾
    print(f"   åˆ†æç‰¹å¾: {best_feature}")
    
    # é‡‡æ ·æ•°æ®è¿›è¡Œè‡ªç›¸å…³åˆ†æ
    sample_size = min(1000, normal_mask.sum())
    normal_sample = test_df[normal_mask][best_feature].iloc[:sample_size]
    anomaly_sample = test_df[anomaly_mask][best_feature].iloc[:min(1000, anomaly_mask.sum())]
    
    def autocorr(x, max_lag=50):
        """è®¡ç®—è‡ªç›¸å…³"""
        x = x - x.mean()  # å»ä¸­å¿ƒåŒ–
        result = np.correlate(x, x, mode='full')
        result = result[len(result)//2:]
        if result[0] != 0:
            result = result / result[0]
        return result[:max_lag]
    
    normal_autocorr = autocorr(normal_sample.values)
    anomaly_autocorr = autocorr(anomaly_sample.values)
    
    autocorr_diff = np.mean(np.abs(normal_autocorr - anomaly_autocorr))
    print(f"   æ—¶åºæ¨¡å¼å·®å¼‚: {autocorr_diff:.4f}")
    
    # 11. æœ€ç»ˆå»ºè®®
    print(f"\nğŸ¯ æœ€ç»ˆå»ºè®®:")
    
    if avg_score < 0.1:
        difficulty = "å›°éš¾"
        seq_len_rec = "300-500"
        epochs_rec = "25-30"
        model_rec = "æ·±åº¦æ¨¡å‹ (e_layers>=6)"
    elif avg_score < 0.2:
        difficulty = "ä¸­ç­‰"
        seq_len_rec = "200-300"
        epochs_rec = "20-25"
        model_rec = "æ ‡å‡†æ¨¡å‹ (e_layers=4-6)"
    else:
        difficulty = "ç›¸å¯¹å®¹æ˜“"
        seq_len_rec = "100-200"
        epochs_rec = "15-20"
        model_rec = "æ ‡å‡†æ¨¡å‹å³å¯"
    
    print(f"   å¼‚å¸¸æ£€æµ‹éš¾åº¦: {difficulty}")
    print(f"   æ¨èåºåˆ—é•¿åº¦: {seq_len_rec}")
    print(f"   æ¨èè®­ç»ƒè½®æ•°: {epochs_rec}")
    print(f"   æ¨èæ¨¡å‹é…ç½®: {model_rec}")
    
    if autocorr_diff < 0.1:
        print(f"   âš ï¸  æ—¶åºæ¨¡å¼å·®å¼‚è¾ƒå°ï¼Œå»ºè®®å¢å¼ºç‰¹å¾å·¥ç¨‹")
    else:
        print(f"   âœ… æ—¶åºæ¨¡å¼æœ‰å·®å¼‚ï¼Œæ¨¡å‹åº”è¯¥èƒ½å¤Ÿå­¦ä¹ ")
    
    if high_corr_count > 20:
        print(f"   âš ï¸  ç‰¹å¾å†—ä½™è¾ƒå¤šï¼Œå»ºè®®ç‰¹å¾é€‰æ‹©")
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼åŸºäºè¿™ä¸ªåˆ†æç»“æœè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚")
    
    # è¿”å›å…³é”®æŒ‡æ ‡
    return {
        'avg_discrimination': avg_score,
        'temporal_difference': autocorr_diff,
        'high_correlation_pairs': high_corr_count,
        'best_feature': best_feature,
        'anomaly_ratio': anomaly_mask.sum() / len(test_labels),
        'difficulty': difficulty
    }

if __name__ == "__main__":
    results = analyze_contact_data() 