"""
è¯Šæ–­TimesNeté‡æ„å¼‚å¸¸æ£€æµ‹å¤±è´¥çš„æ·±å±‚åŸå› 
åˆ†æä¸ºä»€ä¹ˆContactæ•°æ®é›†ä¸Šçš„æ— ç›‘ç£é‡æ„æ–¹æ³•è¡¨ç°ä¸ä½³
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from utils.logger import logger
import os

class ReconstructionAnomalyDiagnostic:
    def __init__(self):
        self.results = {}
        
    def load_contact_data(self):
        """åŠ è½½Contactæ•°æ®"""
        logger.info("ğŸ“‚ åŠ è½½Contactæ•°æ®...")
        
        # åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆæ­£å¸¸æ ·æœ¬ï¼‰
        train_data = pd.read_parquet('./dataset/anomaly_filtered_Traindata/train_normal.parquet')
        
        # åŠ è½½æµ‹è¯•æ•°æ®å’Œæ ‡ç­¾
        test_data = pd.read_parquet('./dataset/split_data/test.parquet')
        test_labels = pd.read_parquet('./dataset/anomaly_labeld_testData/test_labeled.parquet')
        
        # æ¨¡æ‹ŸContactLoaderçš„é¢„å¤„ç†
        train_features = train_data.values[:, 1:]  # è·³è¿‡TimeStamp
        test_features = test_data.values[:, 1:]
        test_labels_array = test_labels.values[:, 1:].flatten()
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        train_features = scaler.fit_transform(train_features)
        test_features = scaler.transform(test_features)
        
        logger.info(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_features.shape}")
        logger.info(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_features.shape}")
        logger.info(f"å¼‚å¸¸æ¯”ä¾‹: {test_labels_array.sum()/len(test_labels_array)*100:.2f}%")
        
        return train_features, test_features, test_labels_array, scaler
    
    def analyze_data_characteristics(self, train_data, test_data, test_labels):
        """åˆ†ææ•°æ®ç‰¹æ€§å¯¹é‡æ„çš„å½±å“"""
        logger.info("ğŸ” åˆ†ææ•°æ®ç‰¹æ€§...")
        
        normal_mask = test_labels == 0
        anomaly_mask = test_labels == 1
        
        normal_data = test_data[normal_mask]
        anomaly_data = test_data[anomaly_mask]
        
        # 1. æ•°æ®å¤æ‚æ€§åˆ†æ
        logger.info("=== æ•°æ®å¤æ‚æ€§åˆ†æ ===")
        
        # ç‰¹å¾é—´ç›¸å…³æ€§
        correlation_matrix = np.corrcoef(train_data.T)
        high_corr_pairs = np.sum(np.abs(correlation_matrix) > 0.9) - train_data.shape[1]  # æ’é™¤å¯¹è§’çº¿
        logger.info(f"é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹æ•°é‡ (>0.9): {high_corr_pairs // 2}")
        
        # ç‰¹å¾æ–¹å·®åˆ†æ
        feature_variances = np.var(train_data, axis=0)
        low_variance_features = np.sum(feature_variances < 0.01)
        logger.info(f"ä½æ–¹å·®ç‰¹å¾æ•°é‡ (<0.01): {low_variance_features}")
        
        # æ•°æ®åˆ†å¸ƒååº¦
        from scipy.stats import skew, kurtosis
        feature_skewness = [skew(train_data[:, i]) for i in range(train_data.shape[1])]
        high_skew_features = np.sum(np.abs(feature_skewness) > 2)
        logger.info(f"é«˜ååº¦ç‰¹å¾æ•°é‡ (|skew|>2): {high_skew_features}")
        
        # 2. æ­£å¸¸vså¼‚å¸¸æ ·æœ¬çš„å¯é‡æ„æ€§åˆ†æ
        logger.info("\n=== å¯é‡æ„æ€§åˆ†æ ===")
        
        # è®¡ç®—æ ·æœ¬å†…éƒ¨æ–¹å·®ï¼ˆå¤æ‚åº¦ï¼‰
        normal_complexity = np.mean([np.var(sample) for sample in normal_data])
        anomaly_complexity = np.mean([np.var(sample) for sample in anomaly_data])
        
        logger.info(f"æ­£å¸¸æ ·æœ¬å¹³å‡å¤æ‚åº¦: {normal_complexity:.6f}")
        logger.info(f"å¼‚å¸¸æ ·æœ¬å¹³å‡å¤æ‚åº¦: {anomaly_complexity:.6f}")
        logger.info(f"å¤æ‚åº¦æ¯”ä¾‹: {anomaly_complexity/normal_complexity:.3f}")
        
        # è®¡ç®—ä¸è®­ç»ƒé›†çš„ç›¸ä¼¼æ€§
        from sklearn.metrics.pairwise import cosine_similarity
        
        # éšæœºé‡‡æ ·è®¡ç®—ç›¸ä¼¼æ€§ï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
        sample_size = min(1000, len(train_data))
        train_sample = train_data[np.random.choice(len(train_data), sample_size, replace=False)]
        
        normal_similarities = []
        anomaly_similarities = []
        
        for i in range(min(100, len(normal_data))):
            sim = cosine_similarity([normal_data[i]], train_sample)[0]
            normal_similarities.append(np.mean(sim))
            
        for i in range(min(100, len(anomaly_data))):
            sim = cosine_similarity([anomaly_data[i]], train_sample)[0]
            anomaly_similarities.append(np.mean(sim))
        
        logger.info(f"æ­£å¸¸æ ·æœ¬ä¸è®­ç»ƒé›†å¹³å‡ç›¸ä¼¼æ€§: {np.mean(normal_similarities):.4f}")
        logger.info(f"å¼‚å¸¸æ ·æœ¬ä¸è®­ç»ƒé›†å¹³å‡ç›¸ä¼¼æ€§: {np.mean(anomaly_similarities):.4f}")
        
        self.results['data_characteristics'] = {
            'high_correlation_pairs': high_corr_pairs // 2,
            'low_variance_features': low_variance_features,
            'high_skew_features': high_skew_features,
            'normal_complexity': normal_complexity,
            'anomaly_complexity': anomaly_complexity,
            'complexity_ratio': anomaly_complexity/normal_complexity,
            'normal_similarity': np.mean(normal_similarities),
            'anomaly_similarity': np.mean(anomaly_similarities)
        }
        
    def test_simple_reconstruction_baselines(self, train_data, test_data, test_labels):
        """æµ‹è¯•ç®€å•é‡æ„åŸºçº¿æ–¹æ³•"""
        logger.info("ğŸ§ª æµ‹è¯•ç®€å•é‡æ„åŸºçº¿...")
        
        results = {}
        
        # 1. çº¿æ€§è‡ªç¼–ç å™¨
        logger.info("æµ‹è¯•çº¿æ€§è‡ªç¼–ç å™¨...")
        
        class LinearAutoEncoder(nn.Module):
            def __init__(self, input_dim, hidden_dim):
                super().__init__()
                self.encoder = nn.Linear(input_dim, hidden_dim)
                self.decoder = nn.Linear(hidden_dim, input_dim)
                
            def forward(self, x):
                encoded = torch.relu(self.encoder(x))
                decoded = self.decoder(encoded)
                return decoded
        
        # è®­ç»ƒçº¿æ€§è‡ªç¼–ç å™¨
        input_dim = train_data.shape[1]
        hidden_dim = input_dim // 2
        
        model = LinearAutoEncoder(input_dim, hidden_dim)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        train_tensor = torch.FloatTensor(train_data)
        test_tensor = torch.FloatTensor(test_data)
        
        # ç®€å•è®­ç»ƒ
        model.train()
        for epoch in range(50):
            optimizer.zero_grad()
            reconstructed = model(train_tensor)
            loss = criterion(reconstructed, train_tensor)
            loss.backward()
            optimizer.step()
        
        # æµ‹è¯•é‡æ„è¯¯å·®
        model.eval()
        with torch.no_grad():
            test_reconstructed = model(test_tensor)
            reconstruction_errors = torch.mean((test_tensor - test_reconstructed) ** 2, dim=1).numpy()
        
        # è®¡ç®—å¼‚å¸¸æ£€æµ‹æ€§èƒ½
        thresholds = np.percentile(reconstruction_errors, [90, 95, 99, 99.5])
        
        for i, thresh in enumerate(thresholds):
            predictions = (reconstruction_errors > thresh).astype(int)
            # ç¡®ä¿æ ‡ç­¾æ ¼å¼æ­£ç¡®
            test_labels_clean = test_labels.astype(int)
            precision, recall, f1, _ = precision_recall_fscore_support(test_labels_clean, predictions, average='binary')
            results[f'linear_ae_p{[90, 95, 99, 99.5][i]}'] = {
                'threshold': thresh,
                'precision': precision,
                'recall': recall,
                'f1': f1
            }
            logger.info(f"  P{[90, 95, 99, 99.5][i]}é˜ˆå€¼: F1={f1:.4f}, P={precision:.4f}, R={recall:.4f}")
        
        # 2. PCAé‡æ„
        logger.info("æµ‹è¯•PCAé‡æ„...")
        from sklearn.decomposition import PCA
        
        for n_components in [10, 20, input_dim//2]:
            if n_components >= input_dim:
                continue
                
            pca = PCA(n_components=n_components)
            pca.fit(train_data)
            
            # é‡æ„æµ‹è¯•æ•°æ®
            test_transformed = pca.transform(test_data)
            test_reconstructed_pca = pca.inverse_transform(test_transformed)
            
            pca_errors = np.mean((test_data - test_reconstructed_pca) ** 2, axis=1)
            
            thresh = np.percentile(pca_errors, 95)
            predictions = (pca_errors > thresh).astype(int)
            test_labels_clean = test_labels.astype(int)
            precision, recall, f1, _ = precision_recall_fscore_support(test_labels_clean, predictions, average='binary')
            
            results[f'pca_{n_components}'] = {
                'explained_variance': pca.explained_variance_ratio_.sum(),
                'precision': precision,
                'recall': recall,
                'f1': f1
            }
            logger.info(f"  PCA-{n_components}: F1={f1:.4f}, è§£é‡Šæ–¹å·®={pca.explained_variance_ratio_.sum():.3f}")
        
        # 3. ç»Ÿè®¡åŸºçº¿ï¼šé©¬å“ˆæ‹‰è¯ºæ¯”æ–¯è·ç¦»
        logger.info("æµ‹è¯•é©¬å“ˆæ‹‰è¯ºæ¯”æ–¯è·ç¦»...")
        
        # è®¡ç®—è®­ç»ƒæ•°æ®çš„å‡å€¼å’Œåæ–¹å·®
        train_mean = np.mean(train_data, axis=0)
        train_cov = np.cov(train_data.T)
        
        # åŠ å…¥æ­£åˆ™åŒ–é¿å…å¥‡å¼‚çŸ©é˜µ
        train_cov_reg = train_cov + np.eye(train_cov.shape[0]) * 1e-6
        train_cov_inv = np.linalg.inv(train_cov_reg)
        
        # è®¡ç®—é©¬å“ˆæ‹‰è¯ºæ¯”æ–¯è·ç¦»
        def mahalanobis_distance(x, mean, cov_inv):
            diff = x - mean
            return np.sqrt(np.sum(diff @ cov_inv * diff, axis=1))
        
        mahal_distances = mahalanobis_distance(test_data, train_mean, train_cov_inv)
        
        thresh = np.percentile(mahal_distances, 95)
        predictions = (mahal_distances > thresh).astype(int)
        test_labels_clean = test_labels.astype(int)
        precision, recall, f1, _ = precision_recall_fscore_support(test_labels_clean, predictions, average='binary')
        
        results['mahalanobis'] = {
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
        logger.info(f"  é©¬å“ˆæ‹‰è¯ºæ¯”æ–¯è·ç¦»: F1={f1:.4f}")
        
        self.results['baseline_methods'] = results
        
        # åˆ†æä¸ºä»€ä¹ˆé‡æ„æ–¹æ³•å¤±è´¥
        logger.info("\n=== é‡æ„å¤±è´¥åŸå› åˆ†æ ===")
        
        # æ£€æŸ¥é‡æ„è¯¯å·®åˆ†å¸ƒ
        test_labels_clean = test_labels.astype(int)
        normal_errors = reconstruction_errors[test_labels_clean == 0]
        anomaly_errors = reconstruction_errors[test_labels_clean == 1]
        
        logger.info(f"æ­£å¸¸æ ·æœ¬é‡æ„è¯¯å·®: å‡å€¼={np.mean(normal_errors):.6f}, æ ‡å‡†å·®={np.std(normal_errors):.6f}")
        logger.info(f"å¼‚å¸¸æ ·æœ¬é‡æ„è¯¯å·®: å‡å€¼={np.mean(anomaly_errors):.6f}, æ ‡å‡†å·®={np.std(anomaly_errors):.6f}")
        logger.info(f"è¯¯å·®åˆ†ç¦»åº¦: {(np.mean(anomaly_errors) - np.mean(normal_errors)) / np.std(normal_errors):.3f}")
        
        # å¦‚æœå¼‚å¸¸æ ·æœ¬çš„é‡æ„è¯¯å·®ä¸æ˜¾è‘—é«˜äºæ­£å¸¸æ ·æœ¬ï¼Œè¯´æ˜æ¨¡å‹å­¦ä¹ äº†é”™è¯¯çš„è¡¨ç¤º
        if np.mean(anomaly_errors) <= np.mean(normal_errors) * 1.1:
            logger.warning("âš ï¸ å¼‚å¸¸æ ·æœ¬çš„é‡æ„è¯¯å·®ä¸æ­£å¸¸æ ·æœ¬ç›¸è¿‘ï¼Œæ¨¡å‹å¯èƒ½è¿‡åº¦æ³›åŒ–")
            
    def analyze_timesnet_specific_issues(self, train_data, test_data, test_labels):
        """åˆ†æTimesNetç‰¹å®šçš„é—®é¢˜"""
        logger.info("ğŸ” åˆ†æTimesNetç‰¹å®šé—®é¢˜...")
        
        # 1. æ—¶åºé•¿åº¦é—®é¢˜
        logger.info("=== æ—¶åºé•¿åº¦åˆ†æ ===")
        
        # Contactæ•°æ®çš„seq_lené€šå¸¸æ˜¯100ï¼Œæ£€æŸ¥è¿™ä¸ªé•¿åº¦æ˜¯å¦åˆé€‚
        seq_lens_to_test = [50, 100, 200]
        
        for seq_len in seq_lens_to_test:
            if seq_len > len(test_data):
                continue
                
            # æ¨¡æ‹Ÿæ—¶åºçª—å£
            num_windows = len(test_data) - seq_len + 1
            
            # è®¡ç®—æ—¶åºçª—å£å†…çš„æ–¹å·®ï¼ˆå¤æ‚åº¦ï¼‰
            window_variances = []
            for i in range(min(100, num_windows)):  # åªè®¡ç®—å‰100ä¸ªçª—å£
                window = test_data[i:i+seq_len]
                window_variance = np.var(window)
                window_variances.append(window_variance)
            
            logger.info(f"  seq_len={seq_len}: çª—å£å¹³å‡æ–¹å·®={np.mean(window_variances):.6f}")
        
        # 2. å‘¨æœŸæ€§åˆ†æ
        logger.info("\n=== å‘¨æœŸæ€§åˆ†æ ===")
        
        # æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„å‘¨æœŸæ€§æ¨¡å¼
        from scipy.fft import fft, fftfreq
        
        # å¯¹å‰å‡ ä¸ªç‰¹å¾è¿›è¡ŒFFTåˆ†æ
        for feat_idx in range(min(3, train_data.shape[1])):
            signal = train_data[:min(1440, len(train_data)), feat_idx]  # æœ€å¤šå–1å¤©çš„æ•°æ®
            
            # FFTåˆ†æ
            fft_values = fft(signal)
            freqs = fftfreq(len(signal))
            
            # æ‰¾åˆ°ä¸»è¦é¢‘ç‡æˆåˆ†
            magnitude = np.abs(fft_values)
            dominant_freq_idx = np.argsort(magnitude[1:len(magnitude)//2])[-3:]  # å‰3ä¸ªä¸»è¦é¢‘ç‡
            
            logger.info(f"  ç‰¹å¾{feat_idx}çš„ä¸»è¦å‘¨æœŸ: {[1/freqs[idx+1] if freqs[idx+1] != 0 else 'inf' for idx in dominant_freq_idx]}")
        
        # 3. ç‰¹å¾é‡è¦æ€§åˆ†æ
        logger.info("\n=== ç‰¹å¾é‡è¦æ€§åˆ†æ ===")
        
        # ä½¿ç”¨æ–¹å·®å’ŒåŒºåˆ†åŠ›åˆ†æç‰¹å¾é‡è¦æ€§
        feature_importance = {}
        
        normal_data = test_data[test_labels == 0]
        anomaly_data = test_data[test_labels == 1]
        
        for i in range(train_data.shape[1]):
            # æ–¹å·®ï¼ˆä¿¡æ¯é‡ï¼‰
            variance = np.var(train_data[:, i])
            
            # åŒºåˆ†åŠ›ï¼ˆæ­£å¸¸vså¼‚å¸¸ï¼‰
            normal_mean = np.mean(normal_data[:, i])
            anomaly_mean = np.mean(anomaly_data[:, i])
            pooled_std = np.sqrt((np.var(normal_data[:, i]) + np.var(anomaly_data[:, i])) / 2)
            discrimination = abs(normal_mean - anomaly_mean) / (pooled_std + 1e-8)
            
            feature_importance[i] = {
                'variance': variance,
                'discrimination': discrimination,
                'combined_score': variance * discrimination
            }
        
        # æ’åºç‰¹å¾
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1]['combined_score'], reverse=True)
        
        logger.info("Top 5 é‡è¦ç‰¹å¾:")
        for feat_idx, metrics in sorted_features[:5]:
            logger.info(f"  ç‰¹å¾{feat_idx}: æ–¹å·®={metrics['variance']:.4f}, åŒºåˆ†åŠ›={metrics['discrimination']:.4f}")
        
        # 4. æ•°æ®è´¨é‡é—®é¢˜
        logger.info("\n=== æ•°æ®è´¨é‡é—®é¢˜ ===")
        
        quality_issues = []
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        outlier_ratios = []
        for i in range(train_data.shape[1]):
            Q1 = np.percentile(train_data[:, i], 25)
            Q3 = np.percentile(train_data[:, i], 75)
            IQR = Q3 - Q1
            outliers = np.sum((train_data[:, i] < Q1 - 1.5*IQR) | (train_data[:, i] > Q3 + 1.5*IQR))
            outlier_ratio = outliers / len(train_data)
            outlier_ratios.append(outlier_ratio)
        
        avg_outlier_ratio = np.mean(outlier_ratios)
        logger.info(f"å¹³å‡å¼‚å¸¸å€¼æ¯”ä¾‹: {avg_outlier_ratio*100:.2f}%")
        
        if avg_outlier_ratio > 0.1:
            quality_issues.append("è®­ç»ƒæ•°æ®åŒ…å«è¿‡å¤šå¼‚å¸¸å€¼")
        
        # æ£€æŸ¥æ•°æ®å¹³ç¨³æ€§
        non_stationary_features = 0
        for i in range(min(10, train_data.shape[1])):
            # ç®€å•çš„å¹³ç¨³æ€§æ£€æŸ¥ï¼šå‰ååŠæ®µå‡å€¼å·®å¼‚
            first_half = train_data[:len(train_data)//2, i]
            second_half = train_data[len(train_data)//2:, i]
            
            mean_diff = abs(np.mean(first_half) - np.mean(second_half))
            std_pooled = np.sqrt((np.var(first_half) + np.var(second_half)) / 2)
            
            if mean_diff > 2 * std_pooled:
                non_stationary_features += 1
        
        logger.info(f"å¯èƒ½éå¹³ç¨³çš„ç‰¹å¾æ•°: {non_stationary_features}/10")
        
        if non_stationary_features > 5:
            quality_issues.append("æ•°æ®å¯èƒ½éå¹³ç¨³")
        
        self.results['timesnet_analysis'] = {
            'feature_importance': dict(sorted_features[:10]),
            'avg_outlier_ratio': avg_outlier_ratio,
            'non_stationary_features': non_stationary_features,
            'quality_issues': quality_issues
        }
        
    def propose_solutions(self):
        """æå‡ºè§£å†³æ–¹æ¡ˆ"""
        logger.info("ğŸ’¡ æå‡ºè§£å†³æ–¹æ¡ˆ...")
        
        solutions = []
        
        # åŸºäºåˆ†æç»“æœæå‡ºè§£å†³æ–¹æ¡ˆ
        data_char = self.results['data_characteristics']
        timesnet_analysis = self.results['timesnet_analysis']
        
        # 1. æ•°æ®é¢„å¤„ç†æ”¹è¿›
        if data_char['high_correlation_pairs'] > 50:
            solutions.append("ä½¿ç”¨PCAæˆ–ç‰¹å¾é€‰æ‹©å‡å°‘é«˜ç›¸å…³æ€§ç‰¹å¾")
            
        if data_char['low_variance_features'] > 5:
            solutions.append("ç§»é™¤ä½æ–¹å·®ç‰¹å¾")
            
        if data_char['complexity_ratio'] < 1.2:
            solutions.append("å¼‚å¸¸æ ·æœ¬å¤æ‚åº¦ä¸è¶³ï¼Œè€ƒè™‘é‡æ–°å®šä¹‰å¼‚å¸¸æ ‡å‡†")
            
        # 2. æ¨¡å‹æ”¹è¿›
        if data_char['anomaly_similarity'] > 0.8:
            solutions.append("å¼‚å¸¸æ ·æœ¬ä¸è®­ç»ƒé›†ç›¸ä¼¼åº¦è¿‡é«˜ï¼Œè€ƒè™‘ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹")
            
        # 3. è®­ç»ƒç­–ç•¥æ”¹è¿›
        best_baseline_f1 = max([result['f1'] for result in self.results['baseline_methods'].values()])
        if best_baseline_f1 > 0.2:
            solutions.append(f"ç®€å•åŸºçº¿æ–¹æ³•è¡¨ç°æ›´å¥½(F1={best_baseline_f1:.3f})ï¼ŒTimesNetå¯èƒ½è¿‡äºå¤æ‚")
            
        # 4. æ•°æ®è´¨é‡æ”¹è¿›
        if timesnet_analysis['quality_issues']:
            solutions.extend([f"è§£å†³æ•°æ®è´¨é‡é—®é¢˜: {issue}" for issue in timesnet_analysis['quality_issues']])
            
        # 5. é˜ˆå€¼ç­–ç•¥æ”¹è¿›
        solutions.append("ä½¿ç”¨åŠ¨æ€é˜ˆå€¼æˆ–å¤šé˜ˆå€¼èåˆç­–ç•¥")
        solutions.append("è€ƒè™‘ä½¿ç”¨è®­ç»ƒæ•°æ®çš„é‡æ„è¯¯å·®åˆ†å¸ƒæ¥è®¾å®šé˜ˆå€¼")
        
        logger.info("å»ºè®®çš„è§£å†³æ–¹æ¡ˆ:")
        for i, solution in enumerate(solutions, 1):
            logger.info(f"  {i}. {solution}")
            
        self.results['solutions'] = solutions
        
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        
        report = f"""
# TimesNeté‡æ„å¼‚å¸¸æ£€æµ‹å¤±è´¥åŸå› åˆ†ææŠ¥å‘Š

## ğŸ¯ æ ¸å¿ƒå‘ç°

### æ•°æ®ç‰¹æ€§é—®é¢˜
- é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹: {self.results['data_characteristics']['high_correlation_pairs']}
- ä½æ–¹å·®ç‰¹å¾æ•°: {self.results['data_characteristics']['low_variance_features']}
- å¼‚å¸¸æ ·æœ¬å¤æ‚åº¦æ¯”ä¾‹: {self.results['data_characteristics']['complexity_ratio']:.3f}
- å¼‚å¸¸æ ·æœ¬ç›¸ä¼¼æ€§: {self.results['data_characteristics']['anomaly_similarity']:.3f}

### åŸºçº¿æ–¹æ³•æ€§èƒ½
"""
        
        for method, result in self.results['baseline_methods'].items():
            if 'f1' in result:
                report += f"- {method}: F1={result['f1']:.4f}\n"
        
        report += f"""
### TimesNetç‰¹å®šé—®é¢˜
- å¹³å‡å¼‚å¸¸å€¼æ¯”ä¾‹: {self.results['timesnet_analysis']['avg_outlier_ratio']*100:.2f}%
- éå¹³ç¨³ç‰¹å¾æ•°: {self.results['timesnet_analysis']['non_stationary_features']}/10
- æ•°æ®è´¨é‡é—®é¢˜: {', '.join(self.results['timesnet_analysis']['quality_issues']) if self.results['timesnet_analysis']['quality_issues'] else 'æ— '}

## ğŸ” å¤±è´¥åŸå› åˆ†æ

### 1. æ•°æ®æœ¬èº«çš„é—®é¢˜
- **é«˜ç»´åº¦ä½åŒºåˆ†åº¦**: 27ä¸ªç‰¹å¾ä¸­å¯èƒ½å­˜åœ¨å¤§é‡å†—ä½™ä¿¡æ¯
- **å¼‚å¸¸æ ·æœ¬ä¸å¤Ÿ"å¼‚å¸¸"**: å¼‚å¸¸æ ·æœ¬ä¸æ­£å¸¸æ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´ä¸­è·ç¦»ä¸å¤Ÿè¿œ
- **è®­ç»ƒæ•°æ®è¿‡åº¦å¹³æ»‘**: è¿‡æ»¤å¼‚å¸¸åçš„è®­ç»ƒæ•°æ®å¯èƒ½è¿‡äº"å®Œç¾"

### 2. é‡æ„ä»»åŠ¡çš„å›ºæœ‰å›°éš¾
- **è¿‡åº¦æ³›åŒ–**: æ¨¡å‹å­¦ä¼šé‡æ„æ‰€æœ‰æ ·æœ¬ï¼ŒåŒ…æ‹¬å¼‚å¸¸æ ·æœ¬
- **è¡¨ç¤ºèƒ½åŠ›è¿‡å¼º**: TimesNetçš„è¡¨ç¤ºèƒ½åŠ›å¯èƒ½è¶…è¿‡äº†æ•°æ®çš„å¤æ‚åº¦
- **é˜ˆå€¼è®¾å®šå›°éš¾**: é‡æ„è¯¯å·®åˆ†å¸ƒé‡å ä¸¥é‡

### 3. TimesNetæ¶æ„ä¸åŒ¹é…
- **å‘¨æœŸæ€§å‡è®¾**: TimesNetå‡è®¾å­˜åœ¨å‘¨æœŸæ€§ï¼Œä½†Contactæ•°æ®å¯èƒ½ç¼ºä¹æ˜æ˜¾å‘¨æœŸ
- **æ—¶åºä¾èµ–æ€§**: å¼‚å¸¸å¯èƒ½æ˜¯ç¬æ—¶çš„ï¼Œä¸ä¾èµ–é•¿æœŸæ—¶åºæ¨¡å¼

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ
"""
        
        for i, solution in enumerate(self.results['solutions'], 1):
            report += f"{i}. {solution}\n"
        
        report += """
## ğŸ¯ å…³é”®ç»“è®º

**TimesNetåœ¨Contactæ•°æ®ä¸Šå¤±è´¥çš„æ ¹æœ¬åŸå› å¯èƒ½æ˜¯:**
1. **æ•°æ®ç‰¹æ€§ä¸åŒ¹é…**: Contactæ•°æ®ç¼ºä¹TimesNetæ“…é•¿çš„å‘¨æœŸæ€§æ¨¡å¼
2. **å¼‚å¸¸å®šä¹‰é—®é¢˜**: å½“å‰çš„å¼‚å¸¸æ ‡æ³¨å¯èƒ½ä¸å¤Ÿæ˜ç¡®æˆ–ä¸€è‡´
3. **é‡æ„èŒƒå¼å±€é™**: å¯¹äºè¿™ç±»æ•°æ®ï¼Œé‡æ„å¯èƒ½ä¸æ˜¯æœ€ä½³çš„å¼‚å¸¸æ£€æµ‹æ–¹å¼

**å»ºè®®é‡‡ç”¨çš„æ›¿ä»£æ–¹æ¡ˆ:**
- åŸºäºå¯†åº¦çš„å¼‚å¸¸æ£€æµ‹ (å¦‚Isolation Forest)
- åŸºäºè·ç¦»çš„æ–¹æ³• (å¦‚LOF)
- ç®€åŒ–çš„ç¥ç»ç½‘ç»œæ¶æ„
- é‡æ–°å®¡è§†å¼‚å¸¸æ ‡æ³¨æ ‡å‡†
"""
        
        with open('reconstruction_failure_analysis.md', 'w', encoding='utf-8') as f:
            f.write(report)
            
        logger.info("ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: reconstruction_failure_analysis.md")
        
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        logger.info("ğŸš€ å¼€å§‹é‡æ„å¼‚å¸¸æ£€æµ‹å¤±è´¥åŸå› åˆ†æ...")
        
        # åŠ è½½æ•°æ®
        train_data, test_data, test_labels, scaler = self.load_contact_data()
        
        # å„é¡¹åˆ†æ
        self.analyze_data_characteristics(train_data, test_data, test_labels)
        self.test_simple_reconstruction_baselines(train_data, test_data, test_labels)
        self.analyze_timesnet_specific_issues(train_data, test_data, test_labels)
        
        # æå‡ºè§£å†³æ–¹æ¡ˆ
        self.propose_solutions()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_comprehensive_report()
        
        logger.info("âœ… åˆ†æå®Œæˆï¼")
        
        return self.results

def main():
    diagnostic = ReconstructionAnomalyDiagnostic()
    results = diagnostic.run_full_analysis()
    return results

if __name__ == "__main__":
    main() 