# iTransformerDiffusionDirect æ·±åº¦é‡æ„è®¡åˆ’

> **ä½œè€…**: Claude Code (AI Assistant)
> **æ—¥æœŸ**: 2025-01-20
> **ç‰ˆæœ¬**: v1.0
> **çŠ¶æ€**: è§„åˆ’é˜¶æ®µ

---

## ç›®å½•

1. [æ‰§è¡Œæ‘˜è¦](#æ‰§è¡Œæ‘˜è¦)
2. [å½“å‰æ¨¡å‹é—®é¢˜è¯Šæ–­](#å½“å‰æ¨¡å‹é—®é¢˜è¯Šæ–­)
3. [é‡æ„æ–¹æ¡ˆè¯¦è§£](#é‡æ„æ–¹æ¡ˆè¯¦è§£)
   - [æ¶æ„å±‚é¢ä¼˜åŒ–](#æ¶æ„å±‚é¢ä¼˜åŒ–)
   - [æ‰©æ•£è¿‡ç¨‹ä¼˜åŒ–](#æ‰©æ•£è¿‡ç¨‹ä¼˜åŒ–)
   - [è®­ç»ƒç­–ç•¥ä¼˜åŒ–](#è®­ç»ƒç­–ç•¥ä¼˜åŒ–)
   - [æ•ˆç‡ä¼˜åŒ–](#æ•ˆç‡ä¼˜åŒ–)
4. [å®æ–½è·¯çº¿å›¾](#å®æ–½è·¯çº¿å›¾)
5. [ä¼˜å…ˆçº§æ’åºä¸å»ºè®®](#ä¼˜å…ˆçº§æ’åºä¸å»ºè®®)
6. [é¢„æœŸæ”¶ç›Šåˆ†æ](#é¢„æœŸæ”¶ç›Šåˆ†æ)
7. [é£é™©ä¸ç¼“è§£æªæ–½](#é£é™©ä¸ç¼“è§£æªæ–½)

---

## æ‰§è¡Œæ‘˜è¦

### æ ¸å¿ƒå‘ç°

å½“å‰ `iTransformerDiffusionDirect` æ¨¡å‹å­˜åœ¨ä»¥ä¸‹**æœ¬è´¨é—®é¢˜**ï¼š

1. **æ¶æ„å‰²è£‚**ï¼šiTransformer (Attention) ä¸ UNet1D (CNN) æ¶æ„ä¸ä¸€è‡´ï¼Œæ¡ä»¶ä¿¡æ¯ä¼ é€’å—é™
2. **å‚æ•°åŒ–ä¸ç¨³å®š**ï¼šxâ‚€-prediction åœ¨é«˜å™ªå£°æ—¶é—´æ­¥è¡¨ç°ä¸ç¨³å®šï¼Œå¯¼è‡´é¢„æµ‹ std åä½
3. **é‡‡æ ·æ•ˆç‡ä½**ï¼šDDPM éœ€è¦ 1000 æ­¥é‡‡æ ·ï¼Œæ¨ç†æ—¶é—´è¿‡é•¿
4. **è®­ç»ƒå‰²è£‚**ï¼šä¸¤é˜¶æ®µè®­ç»ƒå¯¼è‡´éª¨å¹²ç½‘ç»œä¸æ‰©æ•£ç½‘ç»œæ— æ³•è”åˆä¼˜åŒ–

### æ¨èé‡æ„è·¯å¾„

```
Phase 1 (åŸºç¡€ä¼˜åŒ–):  v-prediction + ç«¯åˆ°ç«¯è®­ç»ƒ + æ—¶åºæŸå¤±
         â†“
Phase 2 (æ¶æ„å‡çº§):  DiT æ›¿ä»£ UNet + å±‚çº§æ¡ä»¶æ³¨å…¥
         â†“
Phase 3 (å‰æ²¿æŠ€æœ¯):  Flow Matching / Consistency Models
```

### é¢„æœŸæ”¶ç›Š

| æŒ‡æ ‡ | å½“å‰å€¼ | ç›®æ ‡å€¼ | æ”¹è¿›å¹…åº¦ |
|------|--------|--------|----------|
| MSE | 0.5995 | 0.38-0.40 | -35% |
| CRPS | 0.495 | 0.30-0.35 | -35% |
| é‡‡æ ·æ­¥æ•° | 1000 | 50 | -95% |
| è®­ç»ƒæ—¶é—´ | åŸºå‡† | -30% | -30% |

---

## å½“å‰æ¨¡å‹é—®é¢˜è¯Šæ–­

### 1. æ¶æ„å±‚é¢é—®é¢˜

#### 1.1 éª¨å¹²-æ‰©æ•£æ¶æ„æ–­è£‚ ğŸ”´ ä¸¥é‡

**é—®é¢˜æè¿°**ï¼š
- iTransformer ä½¿ç”¨ Self-Attention å¤„ç†å˜é‡é—´å…³ç³»
- UNet1D ä½¿ç”¨ CNN å¤„ç†æ—¶åºç»´åº¦
- ä¸¤è€…æ¶æ„ä¸ä¸€è‡´ï¼Œæ¢¯åº¦æµåŠ¨å’Œç‰¹å¾ä¼ é€’å­˜åœ¨ç“¶é¢ˆ

**ä»£ç ä½ç½®**ï¼š`models/iTransformerDiffusionDirect.py:85-91`

```python
# å½“å‰å®ç°: ä¸¤ä¸ªç‹¬ç«‹ç½‘ç»œ
self.encoder = Encoder(...)        # Transformer
self.denoise_net = UNet1D(...)     # CNN

# æ¡ä»¶ä¼ é€’ç“¶é¢ˆ
cond = self.cond_proj(z, t_emb)    # z.mean() ä¸¢å¤±å˜é‡çº§ä¿¡æ¯
```

**å½±å“**ï¼š
- æ¡ä»¶ä¿¡æ¯ä» `z: [B, N, d_model]` å‹ç¼©ä¸º `cond: [B, cond_dim]`ï¼Œä¸¢å¤±å˜é‡çº§ç»†èŠ‚
- CNN å’Œ Transformer çš„æ„Ÿå—é‡å’Œå½’çº³åç½®ä¸åŒ¹é…
- æ— æ³•å…±äº«æƒé‡æˆ–å¤ç”¨é¢„è®­ç»ƒ

#### 1.2 å˜é‡é€šé“åŒ–è®¾è®¡ç¼ºé™· ğŸŸ  ä¸­ç­‰

**é—®é¢˜æè¿°**ï¼š
- UNet1D å°† N ä¸ªå˜é‡ä½œä¸ºé€šé“ç»´åº¦å¤„ç†
- CNN å·ç§¯æ ¸åœ¨é€šé“é—´ç‹¬ç«‹ä½œç”¨ï¼Œç¼ºä¹æ˜¾å¼çš„å˜é‡é—´äº¤äº’

**ä»£ç ä½ç½®**ï¼š`layers/Diffusion_layers.py:206-208`

```python
# UNet1D è¾“å…¥: [B, N, pred_len] (N ä½œä¸ºé€šé“)
self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, ...)
# Conv1d æ— æ³•æ˜¾å¼å»ºæ¨¡å˜é‡é—´ç›¸å…³æ€§
```

**å½±å“**ï¼š
- å˜é‡ 0, 2 çš„ MSE æ˜¾è‘—é«˜äºå…¶ä»–å˜é‡ï¼ˆå®éªŒè§‚å¯Ÿï¼‰
- å¤šå˜é‡è”åˆåˆ†å¸ƒå»ºæ¨¡èƒ½åŠ›ä¸è¶³

#### 1.3 æ¡ä»¶æ³¨å…¥æ–¹å¼å•ä¸€ ğŸŸ¡ è½»å¾®

**é—®é¢˜æè¿°**ï¼š
- ä»…ä½¿ç”¨ FiLM (å…¨å±€) + CrossAttn (å±€éƒ¨) ä¸¤ç§æ³¨å…¥æ–¹å¼
- FiLM åªæ¥æ”¶ `z.mean()`ï¼Œä¸¢å¤±å˜é‡çº§ä¿¡æ¯

**ä»£ç ä½ç½®**ï¼š`layers/Diffusion_layers.py:72-85`

```python
class ConditionProjector:
    def forward(self, z, t_emb):
        z_global = z.mean(dim=1)  # å‹ç¼©ï¼[B, N, d] â†’ [B, d]
        cond = self.feat_proj(z_global) + self.time_mlp(t_emb)
        return cond
```

---

### 2. æ‰©æ•£è¿‡ç¨‹é—®é¢˜

#### 2.1 xâ‚€-prediction é«˜å™ªå£°ä¸ç¨³å®š ğŸ”´ ä¸¥é‡

**é—®é¢˜æè¿°**ï¼š
- å½“ t â†’ T (é«˜å™ªå£°)ï¼Œx_t â‰ˆ çº¯å™ªå£°ï¼Œä»ä¸­é¢„æµ‹ xâ‚€ æ˜¯ç—…æ€é—®é¢˜
- éœ€è¦ `clamp(-3, 3)` å¼ºåˆ¶ç¨³å®šï¼Œè¯´æ˜é¢„æµ‹å€¼ç»å¸¸è¶Šç•Œ

**ä»£ç ä½ç½®**ï¼š`models/iTransformerDiffusionDirect.py:270`

```python
x0_pred = self.denoise_net(x, t_batch, z)
x0_pred = torch.clamp(x0_pred, -3.0, 3.0)  # å¼ºåˆ¶ç¨³å®š
```

**æ•°å­¦åˆ†æ**ï¼š
```
é«˜å™ªå£°æ—¶ (t â†’ T):
  x_t = âˆšá¾±_t Â· xâ‚€ + âˆš(1-á¾±_t) Â· Îµ
  å½“ á¾±_t â†’ 0 æ—¶ï¼Œx_t â‰ˆ Îµ (çº¯å™ªå£°)

  ç½‘ç»œéœ€è¦ä» Îµ åæ¨ xâ‚€ï¼Œä¿¡å™ªæ¯”æä½ï¼Œè¯¯å·®æ”¾å¤§
```

**å½±å“**ï¼š
- é¢„æµ‹ std = 0.73 < çœŸå® std = 1.05ï¼ˆæ¬ æ‹Ÿåˆï¼‰
- é«˜å™ªå£°æ­¥çš„è¯¯å·®ç´¯ç§¯åˆ°ä½å™ªå£°æ­¥

#### 2.2 é‡‡æ ·æ•ˆç‡ä½ ğŸ”´ ä¸¥é‡

**é—®é¢˜æè¿°**ï¼š
- DDPM éœ€è¦ 1000 æ­¥å®Œæ•´é‡‡æ ·
- æ¯æ­¥éœ€è¦ä¸€æ¬¡å®Œæ•´çš„ UNet å‰å‘æ¨ç†
- æ‰¹é‡é‡‡æ · 100 ä¸ªæ ·æœ¬éœ€è¦ 100,000 æ¬¡å‰å‘

**ä»£ç ä½ç½®**ï¼š`models/iTransformerDiffusionDirect.py:265-294`

```python
for t in reversed(range(self.timesteps)):  # 1000 æ¬¡å¾ªç¯
    x0_pred = self.denoise_net(x, t_batch, z)  # æ¯æ¬¡å®Œæ•´å‰å‘
    ...
```

**å½±å“**ï¼š
- æ¨ç†æ—¶é—´ ~10-30 ç§’/batchï¼ˆå–å†³äº GPUï¼‰
- æ— æ³•ç”¨äºå®æ—¶é¢„æµ‹åœºæ™¯

#### 2.3 å›ºå®š Î² schedule ğŸŸ¡ è½»å¾®

**é—®é¢˜æè¿°**ï¼š
- ä½¿ç”¨é¢„å®šä¹‰çš„ linear/cosine schedule
- ä¸èƒ½æ ¹æ®æ•°æ®ç‰¹æ€§è‡ªé€‚åº”è°ƒæ•´

**ä»£ç ä½ç½®**ï¼š`models/iTransformerDiffusionDirect.py:96-110`

---

### 3. è®­ç»ƒç­–ç•¥é—®é¢˜

#### 3.1 ä¸¤é˜¶æ®µè®­ç»ƒå‰²è£‚ ğŸ”´ ä¸¥é‡

**é—®é¢˜æè¿°**ï¼š
- Stage 1: åªè®­ç»ƒ backbone (MSE loss)
- Stage 2: å†»ç»“ backboneï¼Œåªè®­ç»ƒ diffusion
- æ‰©æ•£ç½‘ç»œçš„æ¢¯åº¦æ— æ³•æµå› backbone

**ä»£ç ä½ç½®**ï¼š`exp/exp_diffusion_forecast.py` (train_stage1, train_stage2)

```python
def train_stage2(self):
    self.model.freeze_encoder()  # å†»ç»“ï¼æ¢¯åº¦æ–­å¼€
    for param in self.model.denoise_net.parameters():
        param.requires_grad = True
```

**å½±å“**ï¼š
- Backbone æ— æ³•å­¦ä¹ å¯¹æ‰©æ•£æœ‰åˆ©çš„ç‰¹å¾è¡¨ç¤º
- ä¸¤é˜¶æ®µä¼˜åŒ–ç›®æ ‡ä¸ä¸€è‡´

#### 3.2 æŸå¤±å‡½æ•°è¿‡äºç®€å• ğŸŸ  ä¸­ç­‰

**é—®é¢˜æè¿°**ï¼š
- ä»…ä½¿ç”¨ MSE æŸå¤±
- æœªè€ƒè™‘æ—¶åºç»“æ„ï¼ˆè¶‹åŠ¿ã€å‘¨æœŸæ€§ï¼‰
- æœªè€ƒè™‘å˜é‡é—´ç›¸å…³æ€§

**ä»£ç ä½ç½®**ï¼š`models/iTransformerDiffusionDirect.py:232`

```python
loss_diff = F.mse_loss(x0_pred, y_norm)  # ç®€å• MSE
```

---

## é‡æ„æ–¹æ¡ˆè¯¦è§£

### æ¶æ„å±‚é¢ä¼˜åŒ–

#### æ–¹æ¡ˆ A: Diffusion Transformer (DiT) ç»Ÿä¸€æ¶æ„ â­â­â­ å¼ºçƒˆæ¨è

**æ ¸å¿ƒæ€æƒ³**ï¼šç”¨ Transformer æ›¿ä»£ UNetï¼Œå®ç°éª¨å¹²ä¸å»å™ªç½‘ç»œçš„æ¶æ„ç»Ÿä¸€

**ç†è®ºä¾æ®**ï¼š
- DiT (Peebles & Xie, 2023) åœ¨å›¾åƒç”Ÿæˆä¸­è¯æ˜ Transformer å¯ä»¥å®Œå…¨æ›¿ä»£ UNet
- Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å¤©ç„¶é€‚åˆå»ºæ¨¡å˜é‡é—´å…³ç³»
- æ¶æ„ç»Ÿä¸€ä¾¿äºæƒé‡å…±äº«å’Œè¿ç§»å­¦ä¹ 

**æ¶æ„è®¾è®¡**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DiT-iTransformer æ¶æ„                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Input: x_hist [B, seq_len, N]                              â”‚
â”‚      â”‚                                                      â”‚
â”‚      â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚   iTransformer Encoder          â”‚                        â”‚
â”‚  â”‚   (ä¿æŒä¸å˜ï¼Œæå–æ¡ä»¶ç‰¹å¾ z)      â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚      â”‚                                                      â”‚
â”‚      â–¼ z: [B, N, d_model]                                   â”‚
â”‚      â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚   Noisy Target: x_t [B, N, T]   â”‚ â† t ~ U(0, T)          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚      â”‚                                                      â”‚
â”‚      â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                DiT Blocks (Ã—L)                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚  1. Patchify: [B, N, T] â†’ [B, P, D]           â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  2. AdaLayerNorm(h, c)  â† c = f(z, t_emb)     â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  3. Self-Attention (across patches)            â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  4. Cross-Attention (to z)                     â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  5. FFN + AdaLayerNorm                         â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚      â”‚                                                      â”‚
â”‚      â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚   Unpatchify â†’ Output [B, N, T] â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ç»„ä»¶å®ç°**ï¼š

```python
class AdaptiveLayerNorm(nn.Module):
    """æ¡ä»¶è‡ªé€‚åº”å±‚å½’ä¸€åŒ– (AdaLN)"""
    def __init__(self, dim, cond_dim):
        super().__init__()
        self.norm = nn.LayerNorm(dim, elementwise_affine=False)
        self.proj = nn.Sequential(
            nn.SiLU(),
            nn.Linear(cond_dim, dim * 2)  # Î³ å’Œ Î²
        )

    def forward(self, x, cond):
        # x: [B, L, D], cond: [B, cond_dim]
        x = self.norm(x)
        gamma, beta = self.proj(cond).chunk(2, dim=-1)
        return x * (1 + gamma.unsqueeze(1)) + beta.unsqueeze(1)


class DiTBlock(nn.Module):
    """å•ä¸ª DiT Block"""
    def __init__(self, dim, n_heads, cond_dim, d_model):
        super().__init__()
        self.adaln1 = AdaptiveLayerNorm(dim, cond_dim)
        self.self_attn = nn.MultiheadAttention(dim, n_heads, batch_first=True)

        self.adaln2 = AdaptiveLayerNorm(dim, cond_dim)
        self.cross_attn = nn.MultiheadAttention(dim, n_heads, batch_first=True)

        self.adaln3 = AdaptiveLayerNorm(dim, cond_dim)
        self.ffn = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Linear(dim * 4, dim)
        )

        # ç”¨äº Cross-Attention çš„ K, V æŠ•å½±
        self.kv_proj = nn.Linear(d_model, dim * 2)

    def forward(self, x, z, cond):
        """
        Args:
            x: [B, P, D] å»å™ªç‰¹å¾
            z: [B, N, d_model] ç¼–ç å™¨ç‰¹å¾
            cond: [B, cond_dim] å…¨å±€æ¡ä»¶ (å«æ—¶é—´æ­¥)
        """
        # Self-Attention
        h = self.adaln1(x, cond)
        h, _ = self.self_attn(h, h, h)
        x = x + h

        # Cross-Attention to encoder features
        h = self.adaln2(x, cond)
        k, v = self.kv_proj(z).chunk(2, dim=-1)
        h, _ = self.cross_attn(h, k, v)
        x = x + h

        # FFN
        h = self.adaln3(x, cond)
        h = self.ffn(h)
        x = x + h

        return x


class DiTDenoiser(nn.Module):
    """DiT å»å™ªç½‘ç»œ (æ›¿ä»£ UNet1D)"""
    def __init__(self, n_vars, pred_len, d_model,
                 dim=256, n_layers=6, n_heads=8, patch_size=4):
        super().__init__()
        self.n_vars = n_vars
        self.pred_len = pred_len
        self.patch_size = patch_size
        self.n_patches = (n_vars * pred_len) // patch_size

        # Patchify: flatten variates and time, then split into patches
        self.patch_embed = nn.Linear(patch_size, dim)
        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches, dim) * 0.02)

        # Time embedding
        self.time_embed = nn.Sequential(
            SinusoidalPosEmb(dim),
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Linear(dim * 4, dim)
        )

        # Condition projection (combine z and t)
        self.cond_proj = nn.Sequential(
            nn.Linear(d_model + dim, dim * 2),
            nn.GELU(),
            nn.Linear(dim * 2, dim)
        )

        # DiT Blocks
        self.blocks = nn.ModuleList([
            DiTBlock(dim, n_heads, dim, d_model)
            for _ in range(n_layers)
        ])

        # Final layer
        self.final_norm = nn.LayerNorm(dim)
        self.final_proj = nn.Linear(dim, patch_size)

    def forward(self, x, t, z):
        """
        Args:
            x: [B, N, T] å™ªå£°ç›®æ ‡
            t: [B] æ—¶é—´æ­¥
            z: [B, N, d_model] ç¼–ç å™¨ç‰¹å¾
        Returns:
            [B, N, T] é¢„æµ‹çš„ xâ‚€ æˆ– v
        """
        B, N, T = x.shape

        # Flatten and patchify: [B, N, T] â†’ [B, N*T] â†’ [B, P, patch_size]
        x_flat = x.reshape(B, -1)
        x_patches = x_flat.reshape(B, self.n_patches, self.patch_size)

        # Patch embedding + positional embedding
        h = self.patch_embed(x_patches) + self.pos_embed

        # Time embedding
        t_emb = self.time_embed(t)  # [B, dim]

        # Global condition: z.mean() + t_emb
        z_global = z.mean(dim=1)  # [B, d_model]
        cond = self.cond_proj(torch.cat([z_global, t_emb], dim=-1))  # [B, dim]

        # DiT Blocks
        for block in self.blocks:
            h = block(h, z, cond)

        # Final projection
        h = self.final_norm(h)
        out = self.final_proj(h)  # [B, P, patch_size]

        # Reshape back: [B, P, patch_size] â†’ [B, N*T] â†’ [B, N, T]
        out = out.reshape(B, -1).reshape(B, N, T)

        return out
```

**å®æ–½æ­¥éª¤**ï¼š

1. **åˆ›å»º `layers/DiT_layers.py`**
   - å®ç° `AdaptiveLayerNorm`
   - å®ç° `DiTBlock`
   - å®ç° `DiTDenoiser`

2. **ä¿®æ”¹ `models/iTransformerDiffusionDirect.py`**
   - å°† `self.denoise_net = UNet1D(...)` æ›¿æ¢ä¸º `self.denoise_net = DiTDenoiser(...)`
   - ç§»é™¤ `ResidualNormalizer` (DiT å†…éƒ¨å¤„ç†å½’ä¸€åŒ–)

3. **è°ƒæ•´è¶…å‚æ•°**
   - `dim=256`, `n_layers=6`, `n_heads=8`, `patch_size=4`
   - å¯æ ¹æ®æ˜¾å­˜è°ƒæ•´

**é¢„æœŸæ”¶ç›Š**ï¼š
- MSE æ”¹è¿› 15-25%
- è®­ç»ƒç¨³å®šæ€§æå‡
- ä¾¿äºåç»­æ‰©å±•

---

#### æ–¹æ¡ˆ B: æ½œåœ¨ç©ºé—´æ‰©æ•£ (Latent Diffusion)

**æ ¸å¿ƒæ€æƒ³**ï¼šå…ˆå°†æ—¶åºå‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œå†åšæ‰©æ•£

**ç†è®ºä¾æ®**ï¼š
- Stable Diffusion (Rombach et al., 2022) è¯æ˜æ½œåœ¨ç©ºé—´æ‰©æ•£æ•ˆç‡æ›´é«˜
- æ—¶åºæ•°æ®é€šå¸¸æœ‰è¾ƒå¼ºçš„æ—¶é—´ç›¸å…³æ€§ï¼Œå¯ä»¥é«˜æ•ˆå‹ç¼©

**æ¶æ„è®¾è®¡**ï¼š

```
åŸå§‹ç©ºé—´æ‰©æ•£ (å½“å‰):
  y âˆˆ R^{B Ã— pred_len Ã— N}  â†’  ç›´æ¥æ‰©æ•£
  è®¡ç®—å¤æ‚åº¦: O(pred_len Ã— N Ã— timesteps)

æ½œåœ¨ç©ºé—´æ‰©æ•£ (æ”¹è¿›):
  y â†’ TemporalEncoder â†’ z âˆˆ R^{B Ã— L Ã— D} â†’ æ‰©æ•£ â†’ TemporalDecoder â†’ Å·
  å…¶ä¸­ L = pred_len / compression_ratio, D ä¸ºæ½œåœ¨ç»´åº¦
  è®¡ç®—å¤æ‚åº¦: O(L Ã— D Ã— timesteps), å‹ç¼©æ¯” 4-8x
```

**å…³é”®ç»„ä»¶å®ç°**ï¼š

```python
class TemporalVAE(nn.Module):
    """æ—¶åºå˜åˆ†è‡ªç¼–ç å™¨"""
    def __init__(self, n_vars, seq_len, latent_dim=64, compression=4):
        super().__init__()
        self.compression = compression
        self.latent_len = seq_len // compression

        # Encoder: ä¸‹é‡‡æ · + å˜åˆ†
        self.encoder = nn.Sequential(
            nn.Conv1d(n_vars, 64, 3, padding=1),
            nn.SiLU(),
            nn.Conv1d(64, 128, 4, stride=2, padding=1),  # /2
            nn.SiLU(),
            nn.Conv1d(128, 256, 4, stride=2, padding=1),  # /4
            nn.SiLU(),
        )
        self.fc_mu = nn.Conv1d(256, latent_dim, 1)
        self.fc_var = nn.Conv1d(256, latent_dim, 1)

        # Decoder: ä¸Šé‡‡æ ·
        self.decoder = nn.Sequential(
            nn.Conv1d(latent_dim, 256, 1),
            nn.SiLU(),
            nn.ConvTranspose1d(256, 128, 4, stride=2, padding=1),  # x2
            nn.SiLU(),
            nn.ConvTranspose1d(128, 64, 4, stride=2, padding=1),   # x4
            nn.SiLU(),
            nn.Conv1d(64, n_vars, 1)
        )

    def encode(self, x):
        """x: [B, N, T] â†’ z: [B, latent_dim, T/compression]"""
        h = self.encoder(x)
        mu = self.fc_mu(h)
        log_var = self.fc_var(h)
        return mu, log_var

    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        """z: [B, latent_dim, T/compression] â†’ x: [B, N, T]"""
        return self.decoder(z)

    def forward(self, x):
        mu, log_var = self.encode(x)
        z = self.reparameterize(mu, log_var)
        x_recon = self.decode(z)
        return x_recon, mu, log_var


class LatentDiffusionModel(nn.Module):
    """æ½œåœ¨ç©ºé—´æ‰©æ•£æ¨¡å‹"""
    def __init__(self, n_vars, pred_len, d_model, latent_dim=64, compression=4):
        super().__init__()

        # é¢„è®­ç»ƒçš„ VAE (æˆ–ç«¯åˆ°ç«¯è®­ç»ƒ)
        self.vae = TemporalVAE(n_vars, pred_len, latent_dim, compression)

        # iTransformer backbone (ä¸å˜)
        self.backbone = iTransformerEncoder(...)

        # æ½œåœ¨ç©ºé—´æ‰©æ•£ (åœ¨å‹ç¼©åçš„ç©ºé—´)
        latent_len = pred_len // compression
        self.diffusion = DiTDenoiser(
            n_vars=latent_dim,
            pred_len=latent_len,
            d_model=d_model
        )

    def forward_loss(self, x_enc, y_true, stage='joint'):
        # ç¼–ç æ¡ä»¶
        z = self.backbone(x_enc)

        # å°†ç›®æ ‡å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´
        y_perm = y_true.permute(0, 2, 1)  # [B, N, T]
        mu, log_var = self.vae.encode(y_perm)
        y_latent = self.vae.reparameterize(mu, log_var)  # [B, D, T/c]

        # åœ¨æ½œåœ¨ç©ºé—´åšæ‰©æ•£
        t = torch.randint(0, self.timesteps, (B,), device=device)
        noise = torch.randn_like(y_latent)
        y_noisy = self.add_noise(y_latent, t, noise)

        pred = self.diffusion(y_noisy, t, z)
        loss_diff = F.mse_loss(pred, y_latent)  # åœ¨æ½œåœ¨ç©ºé—´è®¡ç®—æŸå¤±

        # VAE é‡å»ºæŸå¤± + KL æ•£åº¦
        y_recon = self.vae.decode(y_latent)
        loss_recon = F.mse_loss(y_recon, y_perm)
        loss_kl = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())

        return loss_diff + 0.1 * loss_recon + 0.001 * loss_kl
```

**å®æ–½æ­¥éª¤**ï¼š

1. **é¢„è®­ç»ƒ VAE** (å¯é€‰)
   - åœ¨æ—¶åºæ•°æ®ä¸Šè®­ç»ƒ TemporalVAE
   - ç¡®ä¿é‡å»ºè´¨é‡æ»¡è¶³è¦æ±‚

2. **ä¿®æ”¹æ‰©æ•£ç›®æ ‡**
   - å°†æ‰©æ•£ä»åŸå§‹ç©ºé—´ç§»åˆ°æ½œåœ¨ç©ºé—´
   - è°ƒæ•´ DiT/UNet è¾“å…¥è¾“å‡ºç»´åº¦

3. **ç«¯åˆ°ç«¯å¾®è°ƒ**
   - VAE + Diffusion è”åˆè®­ç»ƒ

**é¢„æœŸæ”¶ç›Š**ï¼š
- è®¡ç®—é‡å‡å°‘ 4-8x
- é‡‡æ ·é€Ÿåº¦æå‡ 4-8x
- ç”Ÿæˆè´¨é‡å¯èƒ½ç•¥æœ‰ä¸‹é™ï¼Œéœ€è¦å¹³è¡¡

---

#### æ–¹æ¡ˆ C: å±‚çº§å¼æ¡ä»¶æ³¨å…¥

**æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨å»å™ªç½‘ç»œæ¯ä¸€å±‚æ³¨å…¥ä¸åŒç²’åº¦çš„æ¡ä»¶ä¿¡æ¯

**å½“å‰é—®é¢˜**ï¼š
```python
# åªä½¿ç”¨å…¨å±€æ¡ä»¶
cond = ConditionProjector(z.mean())  # z: [B, N, d] â†’ cond: [B, c]
# ä¸¢å¤±äº†å˜é‡çº§ä¿¡æ¯ï¼
```

**æ”¹è¿›æ–¹æ¡ˆ**ï¼š

```python
class HierarchicalConditioner(nn.Module):
    """å±‚çº§å¼æ¡ä»¶ç”Ÿæˆå™¨"""
    def __init__(self, d_model, cond_dim, time_emb_dim, n_vars, pred_len):
        super().__init__()

        # 1. å…¨å±€æ¡ä»¶ (ç”¨äº FiLM)
        self.global_proj = nn.Sequential(
            nn.Linear(d_model, cond_dim),
            nn.SiLU(),
            nn.Linear(cond_dim, cond_dim)
        )

        # 2. å˜é‡çº§æ¡ä»¶ (ç”¨äº Cross-Attention)
        self.variate_proj = nn.Sequential(
            nn.Linear(d_model, cond_dim),
            nn.SiLU(),
            nn.Linear(cond_dim, cond_dim)
        )

        # 3. æ—¶é—´çº§æ¡ä»¶ (ç”¨äº Temporal Attention)
        self.temporal_proj = nn.Sequential(
            nn.Linear(d_model, pred_len),
            nn.SiLU(),
            nn.Linear(pred_len, pred_len * cond_dim)
        )

        # æ—¶é—´æ­¥åµŒå…¥
        self.time_mlp = nn.Sequential(
            SinusoidalPosEmb(time_emb_dim),
            nn.Linear(time_emb_dim, cond_dim),
            nn.SiLU(),
            nn.Linear(cond_dim, cond_dim)
        )

        self.n_vars = n_vars
        self.pred_len = pred_len
        self.cond_dim = cond_dim

    def forward(self, z, t):
        """
        Args:
            z: [B, N, d_model] ç¼–ç å™¨ç‰¹å¾
            t: [B] æ—¶é—´æ­¥
        Returns:
            global_cond: [B, cond_dim] å…¨å±€æ¡ä»¶
            variate_cond: [B, N, cond_dim] å˜é‡çº§æ¡ä»¶
            temporal_cond: [B, T, cond_dim] æ—¶é—´çº§æ¡ä»¶
        """
        B = z.shape[0]
        t_emb = self.time_mlp(t)  # [B, cond_dim]

        # å…¨å±€: å˜é‡å¹³å‡ + æ—¶é—´æ­¥
        global_cond = self.global_proj(z.mean(dim=1)) + t_emb

        # å˜é‡çº§: æ¯ä¸ªå˜é‡ç‹¬ç«‹æŠ•å½±
        variate_cond = self.variate_proj(z)  # [B, N, cond_dim]
        # åŠ å…¥æ—¶é—´æ­¥ä¿¡æ¯
        variate_cond = variate_cond + t_emb.unsqueeze(1)

        # æ—¶é—´çº§: å°†å˜é‡èšåˆåå±•å¼€åˆ°æ—¶é—´ç»´åº¦
        temporal_cond = self.temporal_proj(z.mean(dim=1))  # [B, T*cond_dim]
        temporal_cond = temporal_cond.view(B, self.pred_len, self.cond_dim)
        temporal_cond = temporal_cond + t_emb.unsqueeze(1)

        return global_cond, variate_cond, temporal_cond


class HierarchicalUNet1D(nn.Module):
    """å±‚çº§æ¡ä»¶æ³¨å…¥çš„ UNet"""
    def __init__(self, ...):
        super().__init__()
        self.conditioner = HierarchicalConditioner(...)

        # åœ¨ä¸åŒå±‚ä½¿ç”¨ä¸åŒç²’åº¦çš„æ¡ä»¶
        self.down_blocks = nn.ModuleList([
            DownBlockWithHierarchicalCond(use_global=True, use_variate=False),
            DownBlockWithHierarchicalCond(use_global=True, use_variate=True),
            DownBlockWithHierarchicalCond(use_global=True, use_variate=True),
        ])

        self.bottleneck = BottleneckWithAllCond()  # ä½¿ç”¨æ‰€æœ‰ä¸‰ç§æ¡ä»¶

        self.up_blocks = nn.ModuleList([
            UpBlockWithHierarchicalCond(use_temporal=True),
            UpBlockWithHierarchicalCond(use_temporal=True),
            UpBlockWithHierarchicalCond(use_temporal=False),
        ])

    def forward(self, x, t, z):
        global_c, variate_c, temporal_c = self.conditioner(z, t)

        # Encoder
        skips = []
        h = self.init_conv(x)
        for down in self.down_blocks:
            h, skip = down(h, global_c, variate_c)
            skips.append(skip)

        # Bottleneck (ä½¿ç”¨å…¨éƒ¨æ¡ä»¶)
        h = self.bottleneck(h, global_c, variate_c, temporal_c)

        # Decoder
        for up, skip in zip(self.up_blocks, reversed(skips)):
            h = up(h, skip, global_c, temporal_c)

        return self.final_conv(h)
```

**å®æ–½æ­¥éª¤**ï¼š

1. **å®ç° `HierarchicalConditioner`**
2. **ä¿®æ”¹ `ResBlock1D` æ”¯æŒå¤šç§æ¡ä»¶è¾“å…¥**
3. **ä¿®æ”¹ `UNet1D` æ¶æ„ï¼Œåœ¨ä¸åŒå±‚ä½¿ç”¨ä¸åŒæ¡ä»¶**

**é¢„æœŸæ”¶ç›Š**ï¼š
- å˜é‡ 0, 2 çš„ MSE æ”¹å–„
- æ•´ä½“ CRPS æ”¹å–„ 10-15%

---

### æ‰©æ•£è¿‡ç¨‹ä¼˜åŒ–

#### æ–¹æ¡ˆ D: v-prediction å‚æ•°åŒ– â­â­â­ å¼ºçƒˆæ¨è

**æ ¸å¿ƒæ€æƒ³**ï¼šé¢„æµ‹ velocity v è€Œé xâ‚€ æˆ– Îµï¼Œåœ¨æ‰€æœ‰å™ªå£°çº§åˆ«éƒ½ç¨³å®š

**æ•°å­¦å®šä¹‰**ï¼š
```
ç»™å®š: x_t = âˆšá¾±_t Â· xâ‚€ + âˆš(1-á¾±_t) Â· Îµ

å®šä¹‰ velocity:
  v = âˆšá¾±_t Â· Îµ âˆ’ âˆš(1-á¾±_t) Â· xâ‚€

ä» v æ¢å¤:
  xâ‚€ = âˆšá¾±_t Â· x_t âˆ’ âˆš(1-á¾±_t) Â· v
  Îµ  = âˆš(1-á¾±_t) Â· x_t + âˆšá¾±_t Â· v
```

**ä¸ºä»€ä¹ˆ v-prediction æ›´ç¨³å®š**ï¼š

| æ—¶é—´æ­¥ | Îµ-prediction | xâ‚€-prediction | v-prediction |
|--------|--------------|---------------|--------------|
| t â†’ 0 (ä½å™ªå£°) | éš¾ (Îµ å æ¯”å°) | æ˜“ (xâ‚€ ä¸»å¯¼) | ä¸­ç­‰ |
| t â†’ T (é«˜å™ªå£°) | æ˜“ (Îµ ä¸»å¯¼) | éš¾ (xâ‚€ å æ¯”å°) | ä¸­ç­‰ |
| ä¿¡å™ªæ¯”å˜åŒ– | å‰§çƒˆ | å‰§çƒˆ | **å¹³ç¼“** |

**å®ç°ä»£ç **ï¼š

```python
class VPredictionDiffusion(nn.Module):
    """v-prediction å‚æ•°åŒ–çš„æ‰©æ•£æ¨¡å‹"""

    def __init__(self, ...):
        super().__init__()
        # ... å…¶ä»–åˆå§‹åŒ– ...

    def get_v_target(self, x0, noise, t):
        """è®¡ç®— v çš„ç›®æ ‡å€¼"""
        sqrt_alpha = self.sqrt_alpha_cumprods[t][:, None, None]
        sqrt_one_minus_alpha = self.sqrt_one_minus_alpha_cumprods[t][:, None, None]

        # v = âˆšá¾± Â· Îµ âˆ’ âˆš(1-á¾±) Â· xâ‚€
        v_target = sqrt_alpha * noise - sqrt_one_minus_alpha * x0
        return v_target

    def predict_x0_from_v(self, x_t, v_pred, t):
        """ä» v é¢„æµ‹æ¢å¤ xâ‚€"""
        sqrt_alpha = self.sqrt_alpha_cumprods[t][:, None, None]
        sqrt_one_minus_alpha = self.sqrt_one_minus_alpha_cumprods[t][:, None, None]

        # xâ‚€ = âˆšá¾± Â· x_t âˆ’ âˆš(1-á¾±) Â· v
        x0_pred = sqrt_alpha * x_t - sqrt_one_minus_alpha * v_pred
        return x0_pred

    def predict_noise_from_v(self, x_t, v_pred, t):
        """ä» v é¢„æµ‹æ¢å¤ Îµ"""
        sqrt_alpha = self.sqrt_alpha_cumprods[t][:, None, None]
        sqrt_one_minus_alpha = self.sqrt_one_minus_alpha_cumprods[t][:, None, None]

        # Îµ = âˆš(1-á¾±) Â· x_t + âˆšá¾± Â· v
        eps_pred = sqrt_one_minus_alpha * x_t + sqrt_alpha * v_pred
        return eps_pred

    def forward_loss(self, x_enc, y_true, stage='joint'):
        B = x_enc.shape[0]
        device = x_enc.device

        # Backbone
        y_det, z, means, stdev = self.backbone_forward(x_enc)
        loss_mse = F.mse_loss(y_det, y_true)

        if stage == 'warmup':
            return loss_mse, {'loss_mse': loss_mse.item()}

        # å½’ä¸€åŒ–ç›®æ ‡
        y_norm = (y_true - means[:, 0, :].unsqueeze(1)) / stdev[:, 0, :].unsqueeze(1)
        y_norm = y_norm.permute(0, 2, 1)  # [B, N, T]

        # åŠ å™ª
        t = torch.randint(0, self.timesteps, (B,), device=device)
        noise = torch.randn_like(y_norm)
        y_noisy, _ = self.add_noise(y_norm, t, noise)

        # è®¡ç®— v ç›®æ ‡
        v_target = self.get_v_target(y_norm, noise, t)

        # é¢„æµ‹ v
        v_pred = self.denoise_net(y_noisy, t, z)

        # æŸå¤±
        loss_diff = F.mse_loss(v_pred, v_target)

        loss_total = 0.5 * loss_mse + 0.5 * loss_diff
        return loss_total, {
            'loss_total': loss_total.item(),
            'loss_mse': loss_mse.item(),
            'loss_diff': loss_diff.item()
        }

    @torch.no_grad()
    def sample_ddpm_v(self, z, n_samples=1):
        """v-prediction DDPM é‡‡æ ·"""
        B = z.shape[0]
        device = z.device
        N = self.n_vars

        all_samples = []
        for _ in range(n_samples):
            x = torch.randn(B, N, self.pred_len, device=device)

            for t in reversed(range(self.timesteps)):
                t_batch = torch.full((B,), t, device=device, dtype=torch.long)

                # é¢„æµ‹ v
                v_pred = self.denoise_net(x, t_batch, z)

                # ä» v æ¢å¤ xâ‚€ å’Œ Îµ
                x0_pred = self.predict_x0_from_v(x, v_pred, t_batch)
                eps_pred = self.predict_noise_from_v(x, v_pred, t_batch)

                # å¯é€‰: clamp x0 (v-pred é€šå¸¸ä¸éœ€è¦)
                # x0_pred = torch.clamp(x0_pred, -3, 3)

                # DDPM æ›´æ–°
                alpha = self.alphas[t]
                beta = self.betas[t]

                coef1 = 1.0 / torch.sqrt(alpha)
                coef2 = beta / self.sqrt_one_minus_alpha_cumprods[t]
                mean = coef1 * (x - coef2 * eps_pred)

                if t > 0:
                    noise = torch.randn_like(x)
                    sigma = torch.sqrt(beta)
                    x = mean + sigma * noise
                else:
                    x = mean

            all_samples.append(x)

        return torch.stack(all_samples, dim=0)
```

**å®æ–½æ­¥éª¤**ï¼š

1. **ä¿®æ”¹ `forward_loss`**
   - è®¡ç®— v_target è€Œé xâ‚€ æˆ– noise
   - æŸå¤±å‡½æ•°æ”¹ä¸º `MSE(v_pred, v_target)`

2. **ä¿®æ”¹é‡‡æ ·å‡½æ•°**
   - ä» v_pred æ¢å¤ xâ‚€ å’Œ Îµ
   - ä½¿ç”¨æ¢å¤çš„ Îµ è¿›è¡Œ DDPM/DDIM æ›´æ–°

3. **ç§»é™¤ clamp**
   - v-prediction é€šå¸¸ä¸éœ€è¦æ•°å€¼è£å‰ª

**é¢„æœŸæ”¶ç›Š**ï¼š
- é«˜å™ªå£°æ—¶é—´æ­¥ç¨³å®šæ€§æå‡
- é¢„æµ‹ std ä» 0.73 æå‡åˆ°æ¥è¿‘ 1.05
- MSE æ”¹å–„ 10-20%

---

#### æ–¹æ¡ˆ E: Flow Matching â­â­â­ å‰æ²¿æŠ€æœ¯

**æ ¸å¿ƒæ€æƒ³**ï¼šç”¨æœ€ä¼˜ä¼ è¾“æ›¿ä»£æ‰©æ•£è¿‡ç¨‹ï¼Œå­¦ä¹ ä»å™ªå£°åˆ°æ•°æ®çš„ç›´çº¿è·¯å¾„

**ä¸ DDPM çš„æœ¬è´¨åŒºåˆ«**ï¼š

```
DDPM (éšæœºå¾®åˆ†æ–¹ç¨‹ SDE):
  dx = f(x,t)dt + g(t)dW
  è·¯å¾„: æ›²çº¿ï¼Œéœ€è¦ 1000 æ­¥æ‰èƒ½å‡†ç¡®ç§¯åˆ†

Flow Matching (å¸¸å¾®åˆ†æ–¹ç¨‹ ODE):
  dx = v_Î¸(x,t)dt
  è·¯å¾„: ç›´çº¿ï¼Œ50 æ­¥å³å¯ç²¾ç¡®ç§¯åˆ†
```

**æ•°å­¦æ¨å¯¼**ï¼š

```
ç›®æ ‡: å­¦ä¹ ä» pâ‚€ (é«˜æ–¯å™ªå£°) åˆ° pâ‚ (æ•°æ®åˆ†å¸ƒ) çš„æ˜ å°„

æœ€ä¼˜ä¼ è¾“è·¯å¾„ (ç›´çº¿):
  x_t = (1-t) Â· xâ‚€ + t Â· xâ‚
  å…¶ä¸­ xâ‚€ ~ N(0, I), xâ‚ ~ p_data

velocity (è·¯å¾„å¯¼æ•°):
  v*(x_t, t) = dx_t/dt = xâ‚ - xâ‚€

è®­ç»ƒç›®æ ‡:
  L = E_{t, xâ‚€, xâ‚} [ ||v_Î¸(x_t, t) - (xâ‚ - xâ‚€)||Â² ]

é‡‡æ · (ODE ç§¯åˆ†):
  xâ‚ = xâ‚€ + âˆ«â‚€Â¹ v_Î¸(x_t, t) dt
  â‰ˆ xâ‚€ + Î£áµ¢ v_Î¸(x_táµ¢, táµ¢) Â· Î”t  (Euler æ–¹æ³•)
```

**å®Œæ•´å®ç°**ï¼š

```python
class FlowMatchingModel(nn.Module):
    """Flow Matching æ—¶åºé¢„æµ‹æ¨¡å‹"""

    def __init__(self, n_vars, seq_len, pred_len, d_model,
                 dim=256, n_layers=6, sigma_min=0.001):
        super().__init__()
        self.n_vars = n_vars
        self.pred_len = pred_len
        self.sigma_min = sigma_min

        # iTransformer backbone
        self.backbone = iTransformerEncoder(seq_len, n_vars, d_model)

        # Velocity network (DiT æˆ– UNet)
        self.velocity_net = DiTDenoiser(n_vars, pred_len, d_model, dim, n_layers)

    def get_interpolation(self, x0, x1, t):
        """
        è®¡ç®—æœ€ä¼˜ä¼ è¾“æ’å€¼è·¯å¾„

        Args:
            x0: [B, N, T] å™ªå£°æ ·æœ¬
            x1: [B, N, T] æ•°æ®æ ·æœ¬
            t: [B] æ—¶é—´ (0=å™ªå£°, 1=æ•°æ®)
        Returns:
            x_t: [B, N, T] æ’å€¼ç‚¹
            target_v: [B, N, T] ç›®æ ‡ velocity
        """
        t = t[:, None, None]  # [B, 1, 1]

        # çº¿æ€§æ’å€¼ (æœ€ä¼˜ä¼ è¾“è·¯å¾„)
        x_t = (1 - t) * x0 + t * x1

        # ç›®æ ‡ velocity (ç›´çº¿æ–¹å‘)
        target_v = x1 - x0

        return x_t, target_v

    def forward_loss(self, x_enc, y_true):
        """
        è®­ç»ƒæŸå¤±è®¡ç®—

        Args:
            x_enc: [B, seq_len, N] å†å²è¾“å…¥
            y_true: [B, pred_len, N] ç›®æ ‡
        """
        B = x_enc.shape[0]
        device = x_enc.device

        # Backbone: æå–æ¡ä»¶ç‰¹å¾
        z = self.backbone(x_enc)  # [B, N, d_model]

        # å½’ä¸€åŒ–ç›®æ ‡
        y_norm = self.normalize(y_true).permute(0, 2, 1)  # [B, N, T]

        # é‡‡æ ·å™ªå£°
        x0 = torch.randn_like(y_norm)

        # é‡‡æ ·æ—¶é—´ t âˆˆ (0, 1)
        t = torch.rand(B, device=device)

        # è®¡ç®—æ’å€¼å’Œç›®æ ‡ velocity
        x_t, target_v = self.get_interpolation(x0, y_norm, t)

        # é¢„æµ‹ velocity
        pred_v = self.velocity_net(x_t, t, z)

        # æŸå¤±: åŒ¹é… velocity
        loss = F.mse_loss(pred_v, target_v)

        return loss, {'loss_flow': loss.item()}

    @torch.no_grad()
    def sample(self, x_enc, n_samples=1, steps=50, method='euler'):
        """
        ODE é‡‡æ ·

        Args:
            x_enc: [B, seq_len, N] å†å²è¾“å…¥
            n_samples: é‡‡æ ·æ•°é‡
            steps: ODE ç§¯åˆ†æ­¥æ•°
            method: 'euler' æˆ– 'heun' (2é˜¶)
        """
        B = x_enc.shape[0]
        device = x_enc.device

        # Backbone
        z = self.backbone(x_enc)

        # æ‰©å±• z ç”¨äºå¤šæ ·æœ¬
        z_exp = z.unsqueeze(0).expand(n_samples, -1, -1, -1)
        z_exp = z_exp.reshape(n_samples * B, *z.shape[1:])

        # ä»å™ªå£°å¼€å§‹ (t=0)
        x = torch.randn(n_samples * B, self.n_vars, self.pred_len, device=device)

        # ODE ç§¯åˆ†: ä» t=0 ç§¯åˆ†åˆ° t=1
        dt = 1.0 / steps
        for i in range(steps):
            t = torch.full((n_samples * B,), i * dt, device=device)

            if method == 'euler':
                # Euler æ–¹æ³•
                v = self.velocity_net(x, t, z_exp)
                x = x + v * dt

            elif method == 'heun':
                # Heun æ–¹æ³• (2é˜¶ Runge-Kutta)
                v1 = self.velocity_net(x, t, z_exp)
                x_mid = x + v1 * dt
                t_next = torch.full((n_samples * B,), (i + 1) * dt, device=device)
                v2 = self.velocity_net(x_mid, t_next, z_exp)
                x = x + 0.5 * (v1 + v2) * dt

        # åå½’ä¸€åŒ–
        x = x.reshape(n_samples, B, self.n_vars, self.pred_len)
        x = x.permute(0, 1, 3, 2)  # [n_samples, B, T, N]
        x = self.denormalize(x)

        return x.mean(dim=0), x.std(dim=0), x

    def normalize(self, y):
        """Instance normalization"""
        mean = y.mean(dim=1, keepdim=True)
        std = y.std(dim=1, keepdim=True) + 1e-5
        return (y - mean) / std

    def denormalize(self, y, mean, std):
        return y * std + mean
```

**æ¡ä»¶ Flow Matching (Conditional FM)**:

```python
class ConditionalFlowMatching(FlowMatchingModel):
    """æ¡ä»¶ Flow Matching: å°†ç¡®å®šæ€§é¢„æµ‹ä½œä¸º xâ‚ çš„å…ˆéªŒ"""

    def get_interpolation(self, x0, x1, t, y_det=None):
        """
        æ¡ä»¶æ’å€¼: åœ¨ç¡®å®šæ€§é¢„æµ‹é™„è¿‘åŠ å™ª

        Args:
            x0: [B, N, T] å™ªå£°
            x1: [B, N, T] çœŸå®ç›®æ ‡
            t: [B] æ—¶é—´
            y_det: [B, N, T] ç¡®å®šæ€§é¢„æµ‹ (å…ˆéªŒ)
        """
        t = t[:, None, None]

        if y_det is not None:
            # æ¡ä»¶ FM: ä»¥ç¡®å®šæ€§é¢„æµ‹ä¸ºä¸­å¿ƒ
            # x_t = (1-t) * N(y_det, Ïƒ) + t * x1
            sigma = 0.1 * (1 - t)  # å™ªå£°éš t å¢å¤§å‡å°
            x0_cond = y_det + sigma * torch.randn_like(y_det)
            x_t = (1 - t) * x0_cond + t * x1
            target_v = x1 - x0_cond
        else:
            x_t = (1 - t) * x0 + t * x1
            target_v = x1 - x0

        return x_t, target_v
```

**å®æ–½æ­¥éª¤**ï¼š

1. **åˆ›å»º `models/FlowMatching.py`**
   - å®ç°åŸºç¡€ Flow Matching
   - å®ç°æ¡ä»¶ Flow Matching

2. **åˆ›å»º `exp/exp_flow_matching.py`**
   - å®ç° Flow Matching è®­ç»ƒå¾ªç¯
   - å®ç° ODE é‡‡æ ·è¯„ä¼°

3. **æ·»åŠ å‘½ä»¤è¡Œå‚æ•°**
   - `--flow_steps`: ODE ç§¯åˆ†æ­¥æ•°
   - `--flow_method`: euler / heun

**é¢„æœŸæ”¶ç›Š**ï¼š
- é‡‡æ ·æ­¥æ•°: 1000 â†’ 50 (-95%)
- ç”Ÿæˆè´¨é‡: ä¸ DDPM ç›¸å½“æˆ–æ›´å¥½
- è®­ç»ƒæ›´ç¨³å®š

---

#### æ–¹æ¡ˆ F: Consistency Models

**æ ¸å¿ƒæ€æƒ³**ï¼šå­¦ä¹ ä»ä»»æ„å™ªå£°ç‚¹ä¸€æ­¥åˆ°è¾¾æ•°æ®ç‚¹çš„æ˜ å°„

**è‡ªæ´½æ€§çº¦æŸ**ï¼š
```
å¯¹äºåŒä¸€æ¡æ‰©æ•£è½¨è¿¹ä¸Šçš„ä»»æ„ä¸¤ç‚¹ x_t å’Œ x_s (t â‰  s):
  f_Î¸(x_t, t) = f_Î¸(x_s, s) = xâ‚€

å³: æ— è®ºä»è½¨è¿¹å“ªä¸ªç‚¹å‡ºå‘ï¼Œéƒ½åº”è¯¥æ˜ å°„åˆ°åŒä¸€ç»ˆç‚¹
```

**è®­ç»ƒæ–¹å¼**ï¼š

```python
class ConsistencyModel(nn.Module):
    """Consistency Model for Time Series"""

    def __init__(self, ...):
        super().__init__()
        self.backbone = iTransformerEncoder(...)
        self.consistency_net = DiTDenoiser(...)

        # EMA ç½‘ç»œ (ç”¨äºè‡ªæ´½æ€§ç›®æ ‡)
        self.ema_net = copy.deepcopy(self.consistency_net)
        for p in self.ema_net.parameters():
            p.requires_grad = False

    def consistency_function(self, x_t, t, z):
        """
        Consistency function: ä» x_t é¢„æµ‹ xâ‚€

        åœ¨ t=0 æ—¶ï¼Œåº”è¿”å›è¾“å…¥æœ¬èº« (è¾¹ç•Œæ¡ä»¶)
        """
        if t.min() < 0.01:
            # è¾¹ç•Œæ¡ä»¶: f(x_0, 0) = x_0
            return x_t

        # å¦åˆ™ï¼Œç”¨ç½‘ç»œé¢„æµ‹
        return self.consistency_net(x_t, t, z)

    def forward_loss(self, x_enc, y_true):
        """Consistency Training Loss"""
        B = x_enc.shape[0]
        device = x_enc.device

        z = self.backbone(x_enc)
        y_norm = self.normalize(y_true).permute(0, 2, 1)

        # é‡‡æ ·ç›¸é‚»æ—¶é—´æ­¥ t å’Œ t + Î”t
        t = torch.rand(B, device=device) * 0.99 + 0.01  # t âˆˆ (0.01, 1)
        delta_t = 0.01  # å°æ­¥é•¿
        t_next = torch.clamp(t + delta_t, max=1.0)

        # ä» y_true åŠ å™ªåˆ° t å’Œ t_next
        noise = torch.randn_like(y_norm)
        x_t = self.add_noise(y_norm, t, noise)
        x_t_next = self.add_noise(y_norm, t_next, noise)

        # å½“å‰ç½‘ç»œé¢„æµ‹
        pred_t = self.consistency_function(x_t, t, z)

        # EMA ç½‘ç»œé¢„æµ‹ (ä½œä¸ºç›®æ ‡ï¼Œstop gradient)
        with torch.no_grad():
            target = self.ema_consistency_function(x_t_next, t_next, z)

        # è‡ªæ´½æ€§æŸå¤±: åŒä¸€è½¨è¿¹ä¸Šçš„ç‚¹åº”æ˜ å°„åˆ°åŒä¸€ç»ˆç‚¹
        loss = F.mse_loss(pred_t, target)

        # æ›´æ–° EMA
        self.update_ema()

        return loss, {'loss_consistency': loss.item()}

    @torch.no_grad()
    def sample_one_step(self, x_enc, n_samples=1):
        """ä¸€æ­¥ç”Ÿæˆï¼"""
        z = self.backbone(x_enc)

        # ä»çº¯å™ªå£°å¼€å§‹ (t=1)
        x = torch.randn(n_samples, *y_shape, device=device)
        t = torch.ones(n_samples * B, device=device)

        # ä¸€æ­¥åˆ°ä½
        x0_pred = self.consistency_function(x, t, z)

        return x0_pred

    @torch.no_grad()
    def sample_multi_step(self, x_enc, n_samples=1, steps=4):
        """å¤šæ­¥ç²¾åŒ– (å¯é€‰)"""
        z = self.backbone(x_enc)

        x = torch.randn(n_samples, *y_shape, device=device)

        timesteps = torch.linspace(1, 0.01, steps + 1)
        for i in range(steps):
            t = torch.full((n_samples * B,), timesteps[i], device=device)

            # é¢„æµ‹ xâ‚€
            x0_pred = self.consistency_function(x, t, z)

            # å¦‚æœä¸æ˜¯æœ€åä¸€æ­¥ï¼ŒåŠ å›éƒ¨åˆ†å™ªå£°åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥
            if i < steps - 1:
                t_next = timesteps[i + 1]
                x = self.add_noise(x0_pred, t_next, torch.randn_like(x))

        return x0_pred

    def update_ema(self, decay=0.999):
        with torch.no_grad():
            for p_ema, p in zip(self.ema_net.parameters(),
                                self.consistency_net.parameters()):
                p_ema.data.mul_(decay).add_(p.data, alpha=1 - decay)
```

**å®æ–½æ­¥éª¤**ï¼š

1. **å®ç° Consistency Model åŸºç¡€æ¡†æ¶**
2. **å®ç° Consistency Training**
3. **å®ç°ä¸€æ­¥/å¤šæ­¥é‡‡æ ·**

**é¢„æœŸæ”¶ç›Š**ï¼š
- ä¸€æ­¥ç”Ÿæˆ (1000x åŠ é€Ÿ!)
- 4 æ­¥ç²¾åŒ–å¯è¾¾æ¥è¿‘ DDPM è´¨é‡
- æ¨ç†å»¶è¿Ÿé™åˆ°æ¯«ç§’çº§

---

### è®­ç»ƒç­–ç•¥ä¼˜åŒ–

#### æ–¹æ¡ˆ G: ç«¯åˆ°ç«¯è”åˆè®­ç»ƒ â­â­â­ å¼ºçƒˆæ¨è

**æ ¸å¿ƒæ€æƒ³**ï¼šå–æ¶ˆä¸¤é˜¶æ®µåˆ†ç¦»ï¼Œä»ä¸€å¼€å§‹å°±è”åˆä¼˜åŒ– backbone å’Œ diffusion

**å½“å‰é—®é¢˜**ï¼š

```python
# Stage 1: åªè®­ç»ƒ backbone
train_stage1():
    loss = MSE(y_det, y_true)  # backbone åªå­¦ç¡®å®šæ€§é¢„æµ‹

# Stage 2: å†»ç»“ backboneï¼Œåªè®­ç»ƒ diffusion
train_stage2():
    model.freeze_encoder()  # æ¢¯åº¦æ–­å¼€ï¼
    loss = diffusion_loss   # diffusion æ— æ³•ä¼˜åŒ– backbone çš„ç‰¹å¾è¡¨ç¤º
```

**æ”¹è¿›æ–¹æ¡ˆ**ï¼š

```python
class EndToEndTrainer:
    """ç«¯åˆ°ç«¯è”åˆè®­ç»ƒå™¨"""

    def __init__(self, model, warmup_epochs=10, total_epochs=50):
        self.model = model
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs

    def get_loss_weights(self, epoch):
        """è¯¾ç¨‹å­¦ä¹ : é€æ¸ä»ç¡®å®šæ€§è¿‡æ¸¡åˆ°æ‰©æ•£"""
        if epoch < self.warmup_epochs:
            # å‰æœŸ: ä»¥ç¡®å®šæ€§é¢„æµ‹ä¸ºä¸»
            alpha = 1.0 - epoch / self.warmup_epochs * 0.3  # 1.0 â†’ 0.7
        else:
            # åæœŸ: ä»¥æ‰©æ•£ä¸ºä¸»
            alpha = 0.3  # å›ºå®š 30% MSE + 70% Diffusion
        return alpha, 1 - alpha

    def train_step(self, x_enc, y_true, epoch):
        # å‰å‘ä¼ æ’­ (backbone + diffusionï¼Œæ¢¯åº¦è¿é€š)
        y_det, z, means, stdev = self.model.backbone_forward(x_enc)

        # ç¡®å®šæ€§æŸå¤±
        loss_det = F.mse_loss(y_det, y_true)

        # æ‰©æ•£æŸå¤± (z å‚ä¸ï¼Œæ¢¯åº¦å¯ä»¥å›ä¼ åˆ° backbone!)
        loss_diff = self.model.diffusion_loss(y_true, z, means, stdev)

        # è”åˆæŸå¤± (è¯¾ç¨‹å­¦ä¹ )
        alpha, beta = self.get_loss_weights(epoch)
        loss = alpha * loss_det + beta * loss_diff

        return loss, {
            'loss_total': loss.item(),
            'loss_det': loss_det.item(),
            'loss_diff': loss_diff.item(),
            'alpha': alpha
        }

    def train_epoch(self, dataloader, epoch):
        self.model.train()

        # åŠ¨æ€å­¦ä¹ ç‡
        if epoch < self.warmup_epochs:
            # Warmup: backbone æ­£å¸¸ lrï¼Œdiffusion å° lr
            lr_backbone = 1e-4
            lr_diffusion = 1e-5
        else:
            # è”åˆ: éƒ½ç”¨è¾ƒå° lr
            lr_backbone = 1e-5
            lr_diffusion = 1e-4

        self.set_learning_rates(lr_backbone, lr_diffusion)

        for batch in dataloader:
            loss, log = self.train_step(*batch, epoch)
            loss.backward()
            self.optimizer.step()
            self.optimizer.zero_grad()
```

**ä¼˜åŒ–å™¨é…ç½®**ï¼š

```python
def configure_optimizers(self):
    """åˆ†ç»„å­¦ä¹ ç‡"""
    backbone_params = list(self.model.enc_embedding.parameters()) + \
                      list(self.model.encoder.parameters()) + \
                      list(self.model.projection.parameters())

    diffusion_params = list(self.model.denoise_net.parameters())

    optimizer = torch.optim.AdamW([
        {'params': backbone_params, 'lr': 1e-4, 'weight_decay': 0.01},
        {'params': diffusion_params, 'lr': 1e-4, 'weight_decay': 0.01}
    ])

    # Cosine annealing scheduler
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=self.total_epochs, eta_min=1e-6
    )

    return optimizer, scheduler
```

**å®æ–½æ­¥éª¤**ï¼š

1. **ä¿®æ”¹ `exp/exp_diffusion_forecast.py`**
   - ç§»é™¤ `train_stage1` å’Œ `train_stage2` åˆ†ç¦»
   - å®ç° `EndToEndTrainer`

2. **ä¿®æ”¹æŸå¤±è®¡ç®—**
   - ç¡®ä¿ `z` å‚ä¸æ‰©æ•£æŸå¤±è®¡ç®—ä¸”ä¿ç•™æ¢¯åº¦
   - å®ç°è¯¾ç¨‹å­¦ä¹ æƒé‡è°ƒåº¦

3. **é…ç½®åˆ†ç»„ä¼˜åŒ–å™¨**

**é¢„æœŸæ”¶ç›Š**ï¼š
- Backbone å­¦ä¹ å¯¹æ‰©æ•£æœ‰åˆ©çš„ç‰¹å¾
- æ•´ä½“æ€§èƒ½æå‡ 15-25%
- è®­ç»ƒæ›´ç¨³å®š

---

#### æ–¹æ¡ˆ H: æ—¶åºæ„ŸçŸ¥æŸå¤±å‡½æ•°

**æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨æ—¶åºæ•°æ®çš„ç»“æ„ç‰¹æ€§è®¾è®¡æŸå¤±å‡½æ•°

**ç»„ä»¶**ï¼š

```python
class TimeSeriesAwareLoss(nn.Module):
    """æ—¶åºæ„ŸçŸ¥å¤åˆæŸå¤±å‡½æ•°"""

    def __init__(self, lambda_point=1.0, lambda_trend=0.1,
                 lambda_freq=0.1, lambda_corr=0.05):
        super().__init__()
        self.lambda_point = lambda_point
        self.lambda_trend = lambda_trend
        self.lambda_freq = lambda_freq
        self.lambda_corr = lambda_corr

    def point_loss(self, pred, target):
        """ç‚¹çº§ MSE æŸå¤±"""
        return F.mse_loss(pred, target)

    def trend_loss(self, pred, target):
        """
        è¶‹åŠ¿æŸå¤±: ä¸€é˜¶å·®åˆ†çš„ MSE
        æ•æ‰æ—¶åºçš„å±€éƒ¨å˜åŒ–è¶‹åŠ¿
        """
        pred_diff = pred[:, 1:] - pred[:, :-1]
        target_diff = target[:, 1:] - target[:, :-1]
        return F.mse_loss(pred_diff, target_diff)

    def frequency_loss(self, pred, target):
        """
        é¢‘åŸŸæŸå¤±: FFT å¹…åº¦è°±çš„ MSE
        æ•æ‰å‘¨æœŸæ€§æ¨¡å¼
        """
        pred_fft = torch.fft.rfft(pred, dim=1)
        target_fft = torch.fft.rfft(target, dim=1)

        # å¹…åº¦è°±
        pred_mag = pred_fft.abs()
        target_mag = target_fft.abs()

        return F.mse_loss(pred_mag, target_mag)

    def correlation_loss(self, pred, target):
        """
        ç›¸å…³æ€§æŸå¤±: å˜é‡é—´ç›¸å…³çŸ©é˜µçš„ MSE
        ä¿æŒå¤šå˜é‡é—´çš„ç›¸å…³ç»“æ„
        """
        # è®¡ç®—ç›¸å…³çŸ©é˜µ
        def compute_corr(x):
            # x: [B, T, N]
            B, T, N = x.shape
            x_centered = x - x.mean(dim=1, keepdim=True)
            x_std = x.std(dim=1, keepdim=True) + 1e-5
            x_norm = x_centered / x_std
            corr = torch.bmm(x_norm.transpose(1, 2), x_norm) / T
            return corr

        pred_corr = compute_corr(pred)
        target_corr = compute_corr(target)

        return F.mse_loss(pred_corr, target_corr)

    def forward(self, pred, target):
        """
        è®¡ç®—æ€»æŸå¤±

        Args:
            pred: [B, pred_len, N] é¢„æµ‹
            target: [B, pred_len, N] çœŸå®å€¼
        """
        loss_point = self.point_loss(pred, target)
        loss_trend = self.trend_loss(pred, target)
        loss_freq = self.frequency_loss(pred, target)
        loss_corr = self.correlation_loss(pred, target)

        total = (self.lambda_point * loss_point +
                 self.lambda_trend * loss_trend +
                 self.lambda_freq * loss_freq +
                 self.lambda_corr * loss_corr)

        return total, {
            'loss_point': loss_point.item(),
            'loss_trend': loss_trend.item(),
            'loss_freq': loss_freq.item(),
            'loss_corr': loss_corr.item()
        }
```

**æ‰©å±•: æ¦‚ç‡æŸå¤± (ç”¨äºæ‰©æ•£)**

```python
class ProbabilisticLoss(nn.Module):
    """æ¦‚ç‡é¢„æµ‹æŸå¤±"""

    def crps_loss(self, samples, target):
        """
        CRPS æŸå¤±çš„å¯å¾®è¿‘ä¼¼

        Args:
            samples: [n_samples, B, T, N] é‡‡æ ·
            target: [B, T, N] çœŸå®å€¼
        """
        n_samples = samples.shape[0]

        # é¢„æµ‹å‡å€¼
        mean_pred = samples.mean(dim=0)

        # |y - Å·| é¡¹
        term1 = torch.abs(target - mean_pred).mean()

        # E[|y' - y''|] / 2 é¡¹ (æ ·æœ¬é—´å·®å¼‚)
        # ä½¿ç”¨é‡‡æ ·è¿‘ä¼¼
        idx1 = torch.randperm(n_samples)[:n_samples//2]
        idx2 = torch.randperm(n_samples)[:n_samples//2]
        term2 = torch.abs(samples[idx1] - samples[idx2]).mean() / 2

        return term1 - term2

    def calibration_loss(self, samples, target, quantiles=[0.1, 0.5, 0.9]):
        """
        æ ¡å‡†æŸå¤±: é¢„æµ‹åˆ†ä½æ•°åº”åŒ…å«æ­£ç¡®æ¯”ä¾‹çš„çœŸå®å€¼
        """
        loss = 0
        for q in quantiles:
            q_pred = torch.quantile(samples, q, dim=0)
            # çœŸå®å€¼åº”æœ‰ q æ¯”ä¾‹å°äº q_pred
            actual_below = (target < q_pred).float().mean()
            loss += (actual_below - q) ** 2

        return loss / len(quantiles)
```

**å®æ–½æ­¥éª¤**ï¼š

1. **åˆ›å»º `utils/losses.py`**
2. **åœ¨è®­ç»ƒä¸­ä½¿ç”¨å¤åˆæŸå¤±**
3. **è°ƒæ•´æŸå¤±æƒé‡è¶…å‚æ•°**

**é¢„æœŸæ”¶ç›Š**ï¼š
- è¶‹åŠ¿é¢„æµ‹æ”¹å–„
- å˜é‡ç›¸å…³æ€§ä¿æŒ
- CRPS æŒ‡æ ‡æ”¹å–„

---

#### æ–¹æ¡ˆ I: æ¸è¿›å¼æ‰©æ•£è®­ç»ƒ

**æ ¸å¿ƒæ€æƒ³**ï¼šä»ç®€å•ï¼ˆä½å™ªå£°ï¼‰åˆ°å›°éš¾ï¼ˆé«˜å™ªå£°ï¼‰æ¸è¿›å­¦ä¹ 

```python
class ProgressiveTrainer:
    """æ¸è¿›å¼æ‰©æ•£è®­ç»ƒ"""

    def __init__(self, max_timesteps=1000, initial_timesteps=100,
                 increase_per_epoch=50):
        self.max_T = max_timesteps
        self.current_T = initial_timesteps
        self.increase_per_epoch = increase_per_epoch

    def update_curriculum(self, epoch):
        """æ¯ä¸ª epoch å¢åŠ éš¾åº¦"""
        self.current_T = min(
            self.max_T,
            self.initial_timesteps + epoch * self.increase_per_epoch
        )

    def sample_timestep(self, batch_size, device):
        """åªåœ¨å½“å‰éš¾åº¦èŒƒå›´å†…é‡‡æ ·"""
        return torch.randint(0, self.current_T, (batch_size,), device=device)

    def get_snr_weights(self, t):
        """
        ä¿¡å™ªæ¯”åŠ æƒ: ç»™å›°éš¾æ ·æœ¬æ›´é«˜æƒé‡

        SNR(t) = á¾±_t / (1 - á¾±_t)
        æƒé‡ = 1 / (SNR + 1)  (ä½ SNR = é«˜å™ªå£° = é«˜æƒé‡)
        """
        alpha_t = self.alpha_cumprods[t]
        snr = alpha_t / (1 - alpha_t + 1e-8)
        weights = 1.0 / (snr + 1.0)
        return weights
```

**å®æ–½æ­¥éª¤**ï¼š

1. å®ç°è¯¾ç¨‹å­¦ä¹ è°ƒåº¦å™¨
2. å®ç° SNR åŠ æƒæŸå¤±
3. é›†æˆåˆ°è®­ç»ƒå¾ªç¯

---

### æ•ˆç‡ä¼˜åŒ–

#### æ–¹æ¡ˆ J: æ··åˆç²¾åº¦ + æ¢¯åº¦æ£€æŸ¥ç‚¹

```python
from torch.cuda.amp import autocast, GradScaler
from torch.utils.checkpoint import checkpoint

class EfficientModel(nn.Module):
    def forward_with_efficiency(self, x, t, z):
        # æ¢¯åº¦æ£€æŸ¥ç‚¹: ç”¨è®¡ç®—æ¢æ˜¾å­˜
        def dit_block_fn(h, z, cond):
            for block in self.dit_blocks:
                h = block(h, z, cond)
            return h

        with autocast(dtype=torch.float16):  # FP16 æ··åˆç²¾åº¦
            h = self.init_proj(x)
            cond = self.cond_proj(z, t)
            h = checkpoint(dit_block_fn, h, z, cond, use_reentrant=False)
            out = self.final_proj(h)

        return out

# è®­ç»ƒå¾ªç¯
scaler = GradScaler()
for batch in dataloader:
    with autocast():
        loss = model.forward_loss(*batch)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
```

#### æ–¹æ¡ˆ K: æ¨¡å‹è’¸é¦

```python
class DiffusionDistillation:
    """å°†å¤šæ­¥æ¨¡å‹è’¸é¦åˆ°å°‘æ­¥æ¨¡å‹"""

    def __init__(self, teacher, student):
        self.teacher = teacher  # 1000 æ­¥
        self.student = student  # 50 æ­¥

    def distill_step(self, x_enc, y_true):
        # Teacher: é«˜è´¨é‡ä½†æ…¢
        with torch.no_grad():
            teacher_samples = self.teacher.sample(x_enc, steps=1000)

        # Student: å¿«ä½†éœ€è¦å­¦ä¹ 
        student_samples = self.student.sample(x_enc, steps=50)

        # è’¸é¦æŸå¤±: åŒ¹é… Teacher è¾“å‡º
        loss = F.mse_loss(student_samples.mean(0), teacher_samples.mean(0))

        return loss
```

---

## å®æ–½è·¯çº¿å›¾

### Phase 1: åŸºç¡€ä¼˜åŒ– (å»ºè®®é¦–å…ˆå®æ–½)

| æ­¥éª¤ | ä»»åŠ¡ | ä¿®æ”¹æ–‡ä»¶ | é¢„æœŸæ•ˆæœ |
|------|------|----------|----------|
| 1.1 | v-prediction å‚æ•°åŒ– | `models/iTransformerDiffusionDirect.py` | MSE -15%, ç¨³å®šæ€§â†‘ |
| 1.2 | ç«¯åˆ°ç«¯è”åˆè®­ç»ƒ | `exp/exp_diffusion_forecast.py` | MSE -10%, è®­ç»ƒæ•ˆç‡â†‘ |
| 1.3 | æ—¶åºæ„ŸçŸ¥æŸå¤± | `utils/losses.py` (æ–°å»º) | CRPS -10% |
| 1.4 | æ··åˆç²¾åº¦è®­ç»ƒ | `exp/exp_diffusion_forecast.py` | æ˜¾å­˜ -30%, é€Ÿåº¦â†‘ |

**æ€»é¢„æœŸæ•ˆæœ**: MSE: 0.60 â†’ 0.42-0.45, CRPS: 0.50 â†’ 0.35-0.40

### Phase 2: æ¶æ„å‡çº§

| æ­¥éª¤ | ä»»åŠ¡ | ä¿®æ”¹æ–‡ä»¶ | é¢„æœŸæ•ˆæœ |
|------|------|----------|----------|
| 2.1 | DiT æ›¿ä»£ UNet | `layers/DiT_layers.py` (æ–°å»º) | MSE -10%, æ¶æ„ç»Ÿä¸€ |
| 2.2 | å±‚çº§æ¡ä»¶æ³¨å…¥ | `layers/Diffusion_layers.py` | å˜é‡ MSE å¹³è¡¡ |
| 2.3 | AdaLayerNorm | `layers/DiT_layers.py` | æ¡ä»¶æ³¨å…¥æ•ˆç‡â†‘ |

**æ€»é¢„æœŸæ•ˆæœ**: MSE: 0.42 â†’ 0.38-0.40

### Phase 3: å‰æ²¿æŠ€æœ¯

| æ­¥éª¤ | ä»»åŠ¡ | ä¿®æ”¹æ–‡ä»¶ | é¢„æœŸæ•ˆæœ |
|------|------|----------|----------|
| 3.1 | Flow Matching | `models/FlowMatching.py` (æ–°å»º) | é‡‡æ ·æ­¥æ•° 1000â†’50 |
| 3.2 | Consistency Model | `models/ConsistencyModel.py` (æ–°å»º) | ä¸€æ­¥ç”Ÿæˆ |
| 3.3 | æ½œåœ¨ç©ºé—´æ‰©æ•£ | `models/LatentDiffusion.py` (æ–°å»º) | è®¡ç®—é‡ -75% |

**æ€»é¢„æœŸæ•ˆæœ**: æ¨ç†é€Ÿåº¦ 20-1000x æå‡

---

## ä¼˜å…ˆçº§æ’åºä¸å»ºè®®

### æœ€é«˜ä¼˜å…ˆçº§ (ç«‹å³å®æ–½) â­â­â­

1. **v-prediction å‚æ•°åŒ–** (æ–¹æ¡ˆ D)
   - å®æ–½éš¾åº¦: ä½
   - ä»£ç æ”¹åŠ¨: ~50 è¡Œ
   - æ”¶ç›Š: æ˜¾è‘—æå‡ç¨³å®šæ€§å’Œç²¾åº¦

2. **ç«¯åˆ°ç«¯è”åˆè®­ç»ƒ** (æ–¹æ¡ˆ G)
   - å®æ–½éš¾åº¦: ä¸­
   - ä»£ç æ”¹åŠ¨: ~100 è¡Œ
   - æ”¶ç›Š: æ ¹æœ¬æ€§è§£å†³ä¸¤é˜¶æ®µå‰²è£‚é—®é¢˜

### é«˜ä¼˜å…ˆçº§ (ç¬¬äºŒè½®å®æ–½) â­â­

3. **DiT æ¶æ„** (æ–¹æ¡ˆ A)
   - å®æ–½éš¾åº¦: ä¸­é«˜
   - ä»£ç æ”¹åŠ¨: æ–°å»ºæ–‡ä»¶ ~300 è¡Œ
   - æ”¶ç›Š: æ¶æ„ç»Ÿä¸€ï¼Œä¾¿äºåç»­æ‰©å±•

4. **Flow Matching** (æ–¹æ¡ˆ E)
   - å®æ–½éš¾åº¦: ä¸­
   - ä»£ç æ”¹åŠ¨: æ–°å»ºæ–‡ä»¶ ~200 è¡Œ
   - æ”¶ç›Š: é‡‡æ ·é€Ÿåº¦æå‡ 20x

### ä¸­ä¼˜å…ˆçº§ (å¯é€‰) â­

5. **æ—¶åºæ„ŸçŸ¥æŸå¤±** (æ–¹æ¡ˆ H)
6. **å±‚çº§æ¡ä»¶æ³¨å…¥** (æ–¹æ¡ˆ C)
7. **æ¸è¿›å¼è®­ç»ƒ** (æ–¹æ¡ˆ I)

### ä½ä¼˜å…ˆçº§ (ç ”ç©¶æ€§)

8. **Consistency Models** (æ–¹æ¡ˆ F) - éœ€è¦æ›´å¤šç ”ç©¶
9. **æ½œåœ¨ç©ºé—´æ‰©æ•£** (æ–¹æ¡ˆ B) - éœ€è¦é¢„è®­ç»ƒ VAE

---

## é¢„æœŸæ”¶ç›Šåˆ†æ

### æ€§èƒ½æ”¹è¿›é¢„æµ‹

| æŒ‡æ ‡ | å½“å‰ | Phase 1 å | Phase 2 å | Phase 3 å |
|------|------|------------|------------|------------|
| MSE | 0.5995 | 0.42-0.45 | 0.38-0.40 | 0.38-0.40 |
| MAE | ~0.50 | ~0.40 | ~0.38 | ~0.38 |
| CRPS | 0.495 | 0.35-0.40 | 0.30-0.35 | 0.30-0.35 |
| Calib-50% | 0.49 | 0.48-0.52 | 0.48-0.52 | 0.48-0.52 |
| Calib-90% | 0.88 | 0.88-0.92 | 0.88-0.92 | 0.88-0.92 |

### æ•ˆç‡æ”¹è¿›é¢„æµ‹

| æŒ‡æ ‡ | å½“å‰ | Phase 1 å | Phase 2 å | Phase 3 å |
|------|------|------------|------------|------------|
| é‡‡æ ·æ­¥æ•° | 1000 | 1000 | 1000 | **50** |
| æ¨ç†æ—¶é—´/batch | ~10s | ~7s | ~7s | **0.5s** |
| è®­ç»ƒæ˜¾å­˜ | 8GB | 5.5GB | 6GB | 4GB |
| è®­ç»ƒæ—¶é—´ | åŸºå‡† | -20% | -10% | -30% |

---

## é£é™©ä¸ç¼“è§£æªæ–½

### é£é™© 1: v-prediction å¯èƒ½ä¸é€‚åˆæ—¶åºæ•°æ®

**ç¼“è§£**:
- å…ˆåœ¨å°æ•°æ®é›†éªŒè¯
- ä¿ç•™ xâ‚€-prediction ä½œä¸º fallback
- å¯ä»¥å°è¯•æ··åˆå‚æ•°åŒ–

### é£é™© 2: DiT æ¶æ„è®¡ç®—é‡å¯èƒ½æ›´å¤§

**ç¼“è§£**:
- ä½¿ç”¨è¾ƒå°çš„ patch_size
- å‡å°‘ DiT å±‚æ•°
- ä½¿ç”¨ Flash Attention

### é£é™© 3: Flow Matching è®­ç»ƒä¸ç¨³å®š

**ç¼“è§£**:
- ä½¿ç”¨æ¡ä»¶ Flow Matching (ä»¥ç¡®å®šæ€§é¢„æµ‹ä¸ºå…ˆéªŒ)
- å®ç° Ïƒ_min æ­£åˆ™åŒ–
- æ¸è¿›å¼å¢åŠ  ODE ç§¯åˆ†æ­¥æ•°

### é£é™© 4: ç«¯åˆ°ç«¯è®­ç»ƒæ¢¯åº¦çˆ†ç‚¸

**ç¼“è§£**:
- ä½¿ç”¨æ¢¯åº¦è£å‰ª (max_norm=1.0)
- å­¦ä¹ ç‡ warmup
- å±‚çº§å­¦ä¹ ç‡è¡°å‡

---

## é™„å½•

### A. å®éªŒé…ç½®æ¨¡æ¿

```bash
# Phase 1 å®éªŒ
python run.py \
  --task_name diffusion_forecast \
  --model iTransformerDiffusionV \
  --parameterization v \
  --training_mode end_to_end \
  --loss_type timeseries_aware \
  --use_amp \
  --data ETTh1 \
  --seq_len 96 --pred_len 96 \
  --d_model 128 --d_ff 128 \
  --diffusion_steps 1000 \
  --train_epochs 50 \
  --batch_size 32 \
  --learning_rate 1e-4

# Phase 3 Flow Matching å®éªŒ
python run.py \
  --task_name flow_matching_forecast \
  --model FlowMatchingTS \
  --flow_steps 50 \
  --flow_method heun \
  --data ETTh1 \
  --seq_len 96 --pred_len 96
```

### B. å‚è€ƒæ–‡çŒ®

1. DiT: Peebles & Xie (2023). "Scalable Diffusion Models with Transformers"
2. Flow Matching: Lipman et al. (2023). "Flow Matching for Generative Modeling"
3. Consistency Models: Song et al. (2023). "Consistency Models"
4. v-prediction: Salimans & Ho (2022). "Progressive Distillation for Fast Sampling"
5. iTransformer: Liu et al. (2024). "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting"

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-01-20
**ä½œè€…**: Claude Code (AI Assistant)
