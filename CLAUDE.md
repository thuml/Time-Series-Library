# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## äº¤äº’è¦æ±‚

- **Thinking æ€è€ƒè¿‡ç¨‹ç”¨ä¸­æ–‡è¡¨è¿°**
- **Reply å›ç­”ä¹Ÿè¦ç”¨ä¸­æ–‡å›å¤**
- **ä»£ç æ³¨é‡Šç”¨ä¸­æ–‡ç¼–å†™**

## Overview

Time-Series-Library (TSLib) is an open-source deep learning library from THU-ML supporting 40+ models for time series forecasting, imputation, anomaly detection, and classification. All experiments run through a unified CLI interface.

**æœ¬é¡¹ç›®é‡ç‚¹ç ”ç©¶æ¨¡å‹**:
- **iTransformerDiffusion** - æ¡ä»¶æ®‹å·®æ‰©æ•£æ¨¡å‹ (CRD-Net)
- **iTransformerDiffusionDirect** - ç›´æ¥é¢„æµ‹æ‰©æ•£æ¨¡å‹ï¼ˆæ”¯æŒ xâ‚€/Îµ/v å¤šç§å‚æ•°åŒ–ï¼‰

## ç¯å¢ƒé…ç½®

### ä¾èµ–å®‰è£…

```bash
conda create -n tslib python=3.9
conda activate tslib
pip install -r requirements.txt
```

**æ ¸å¿ƒä¾èµ–**:
- torch==2.5.1 (éœ€è¦ CUDA æ”¯æŒ)
- einops==0.8.1
- scikit-learn==1.2.2
- scipy==1.10.1
- tqdm==4.64.1

**å¯é€‰ä¾èµ–**:
- mamba_ssm (ç”¨äº Mamba æ¨¡å‹)
- transformers (ç”¨äºé¢„è®­ç»ƒæ¨¡å‹)
- datasets (ç”¨äºæ•°æ®åŠ è½½)

## Running Experiments

```bash
conda activate tslib

# === iTransformerDiffusion (æ¦‚ç‡é¢„æµ‹, ä¸¤é˜¶æ®µè®­ç»ƒ) ===
python run.py \
  --task_name diffusion_forecast \
  --is_training 1 \
  --model iTransformerDiffusion \
  --data ETTh1 \
  --root_path ./dataset/ETT-small/ \
  --data_path ETTh1.csv \
  --seq_len 96 --pred_len 96 \
  --enc_in 7 --dec_in 7 --c_out 7 \
  --d_model 128 --d_ff 128 \
  --diffusion_steps 1000 --beta_schedule cosine \
  --stage1_epochs 30 --stage2_epochs 20 \
  --n_samples 100 --use_amp

# ä½æ˜¾å­˜è¿è¡Œ (8GB GPU)
bash scripts/diffusion_forecast/ETT_script/iTransformerDiffusion_ETTh1_8GB.sh

# æ ‡å‡† iTransformer (ç¡®å®šæ€§é¢„æµ‹)
python run.py \
  --task_name long_term_forecast \
  --is_training 1 \
  --model iTransformer \
  --data ETTh1 ...

# Test only (no training)
python run.py --is_training 0 [same args...]
```

**Task types:** `long_term_forecast`, `short_term_forecast`, `imputation`, `anomaly_detection`, `classification`, `zero_shot_forecast`, `diffusion_forecast`

### æ¨¡å‹é€‰æ‹©æŒ‡å—

**ä½•æ—¶ä½¿ç”¨ iTransformerDiffusion**:
- éœ€è¦é«˜è´¨é‡çš„ç¡®å®šæ€§é¢„æµ‹ + ä¸ç¡®å®šæ€§é‡åŒ–
- å·²æœ‰å¼ºå¤§çš„ç¡®å®šæ€§ backbone
- æ®‹å·®å»ºæ¨¡æ›´åˆé€‚çš„åœºæ™¯
- ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥

**ä½•æ—¶ä½¿ç”¨ iTransformerDiffusionDirect**:
- ç«¯åˆ°ç«¯æ¦‚ç‡å»ºæ¨¡
- éœ€è¦æ›´ç¨³å®šçš„è®­ç»ƒï¼ˆv-predictionï¼‰
- æ¢ç´¢ä¸åŒå‚æ•°åŒ–ç­–ç•¥ï¼ˆxâ‚€/Îµ/vï¼‰
- æ›´ç®€æ´çš„æ¶æ„

**ä½•æ—¶ä½¿ç”¨åŸºç¡€ iTransformer**:
- åªéœ€è¦ç‚¹é¢„æµ‹ï¼Œä¸éœ€è¦ä¸ç¡®å®šæ€§é‡åŒ–
- è®­ç»ƒ/æ¨ç†é€Ÿåº¦ä¼˜å…ˆ
- èµ„æºå—é™ç¯å¢ƒ

**Key parameters:**
- `--seq_len`: Input sequence length (default 96)
- `--pred_len`: Prediction horizon (default 96)
- `--label_len`: Decoder start token length (default 48)
- `--enc_in/dec_in/c_out`: Number of variates (channels)
- `--d_model/d_ff`: Model dimensions
- `--e_layers/d_layers`: Encoder/decoder layers
- `--features`: M (multivariateâ†’multivariate), S (univariate), MS (multivariateâ†’univariate)

## Architecture

```
run.py                    # Entry point - parses args, routes to Exp classes
â”œâ”€â”€ exp/
â”‚   â”œâ”€â”€ exp_basic.py      # Model registry (model_dict) and base class
â”‚   â”œâ”€â”€ exp_long_term_forecasting.py
â”‚   â”œâ”€â”€ exp_diffusion_forecast.py   # â˜… æ‰©æ•£æ¨¡å‹ä¸¤é˜¶æ®µè®­ç»ƒ
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ iTransformer.py             # åŸºç¡€ iTransformer
â”‚   â”œâ”€â”€ iTransformerDiffusion.py    # â˜… iTransformer + CRD-Net æ··åˆæ¶æ„
â”‚   â”œâ”€â”€ GaussianDiffusion.py        # åŸºç¡€é«˜æ–¯æ‰©æ•£å·¥å…·ç±»
â”‚   â””â”€â”€ ...
â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ Embed.py                    # DataEmbedding_inverted
â”‚   â”œâ”€â”€ Diffusion_layers.py         # â˜… UNet1D, FiLM, VariateCrossAttention
â”‚   â”œâ”€â”€ SelfAttention_Family.py
â”‚   â””â”€â”€ Transformer_EncDec.py
â”œâ”€â”€ data_provider/
â””â”€â”€ scripts/
    â””â”€â”€ diffusion_forecast/         # â˜… æ‰©æ•£é¢„æµ‹è„šæœ¬
```

---

## iTransformerDiffusion Architecture (æ ¸å¿ƒç ”ç©¶æ¨¡å‹)

**è®¾è®¡ç†å¿µ**: ç»“åˆ iTransformer çš„å˜é‡çº§æ³¨æ„åŠ›æœºåˆ¶ä¸æ¡ä»¶æ®‹å·®æ‰©æ•£ (CRD-Net)ï¼Œå®ç°æ¦‚ç‡æ—¶åºé¢„æµ‹ã€‚

### æ•´ä½“æ•°æ®æµ

```
Input x_hist [B, seq_len, N]
    â”‚
    â–¼ iTransformer Backbone
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Instance Norm â†’ DataEmbedding_inverted â”‚
â”‚  â†’ Encoder (attention across variates)  â”‚
â”‚  â†’ Projection                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                    â”‚
    â–¼                    â–¼
y_det [B, pred_len, N]   z [B, N, d_model]  (encoder features)
    â”‚                    â”‚
    â–¼                    â”‚
Residual = y_true - y_detâ”‚    (è®­ç»ƒæ—¶)
    â”‚                    â”‚
    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CRD-Net (1D U-Net)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Time Embedding (SinusoidalPosEmb) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  ConditionProjector: z + t_emb â†’ cond   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Encoder: DownBlocks + FiLM     â”‚  â”‚
â”‚  â”‚   Bottleneck: ResBlock + CrossAttnâ”‚  â”‚
â”‚  â”‚   Decoder: UpBlocks + FiLM + XAttnâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
y_final = y_det + sampled_residual  (æ¨ç†æ—¶)
```

### æ ¸å¿ƒç»„ä»¶ (`layers/Diffusion_layers.py`)

| ç»„ä»¶ | ä½œç”¨ |
|------|------|
| `SinusoidalPosEmb` | æ‰©æ•£æ—¶é—´æ­¥çš„æ­£å¼¦ä½ç½®ç¼–ç  |
| `ConditionProjector` | èåˆ iTransformer ç‰¹å¾ z ä¸æ—¶é—´åµŒå…¥ t_emb â†’ å…¨å±€æ¡ä»¶å‘é‡ |
| `FiLMLayer` | Feature-wise Linear Modulation: Î³*h + Î² |
| `VariateCrossAttention` | å˜é‡çº§äº¤å‰æ³¨æ„åŠ›ï¼Œå»å™ªç‰¹å¾ attend to ç¼–ç å™¨ç‰¹å¾ |
| `ResBlock1D` | 1D æ®‹å·®å—ï¼Œå¸¦æ‰©å¼ å·ç§¯ + FiLM è°ƒåˆ¶ |
| `DownBlock1D` / `UpBlock1D` | U-Net çš„ä¸‹/ä¸Šé‡‡æ ·å— |
| `UNet1D` | å®Œæ•´çš„ 1D U-Net å»å™ªç½‘ç»œ |
| `ResidualNormalizer` | æ®‹å·®å½’ä¸€åŒ–ï¼ŒEMA è·Ÿè¸ªç»Ÿè®¡é‡ |

### ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ (`exp/exp_diffusion_forecast.py`)

```
Stage 1 (Warmup): 30 epochs
  â”œâ”€â”€ è®­ç»ƒ: enc_embedding + encoder + projection
  â”œâ”€â”€ æŸå¤±: MSE(y_det, y_true)
  â””â”€â”€ å­¦ä¹ ç‡: 1e-4

Stage 2 (Joint): 20 epochs
  â”œâ”€â”€ å†»ç»“: enc_embedding + encoder
  â”œâ”€â”€ è®­ç»ƒ: projection + denoise_net + residual_normalizer
  â”œâ”€â”€ æŸå¤±: Î»*MSE + (1-Î»)*Diffusion (Î»=0.5)
  â””â”€â”€ å­¦ä¹ ç‡: projection 1e-5, diffusion 1e-4
```

### æ‰©æ•£è¿‡ç¨‹

**å‰å‘æ‰©æ•£ (è®­ç»ƒ)**:
```python
# è®¡ç®—æ®‹å·®å¹¶å½’ä¸€åŒ–
residual = y_true - y_det.detach()
residual_norm = residual_normalizer.normalize(residual)

# åŠ å™ª
t = random(0, timesteps)
xt = sqrt(á¾±t)*x0 + sqrt(1-á¾±t)*Îµ

# é¢„æµ‹å™ªå£°
noise_pred = denoise_net(xt, t, z)
loss_diff = MSE(noise_pred, noise)
```

**é€†å‘é‡‡æ · (æ¨ç†)**:
- DDPM: 1000 æ­¥å®Œæ•´é‡‡æ ·
- DDIM: 50 æ­¥åŠ é€Ÿé‡‡æ · (Î·=0 ç¡®å®šæ€§, Î·>0 éšæœºæ€§)
- æ‰¹é‡é‡‡æ ·: `sample_ddpm_batch()` / `sample_ddim_batch()` å¹¶è¡Œå¤„ç†å¤šæ ·æœ¬
- åˆ†å—é‡‡æ ·: `sample_chunked()` æ§åˆ¶æ˜¾å­˜ä½¿ç”¨

### å…³é”®é…ç½®å‚æ•°

```bash
# æ‰©æ•£æ¨¡å‹å‚æ•°
--diffusion_steps 1000     # æ‰©æ•£æ­¥æ•°
--beta_schedule cosine     # beta è°ƒåº¦: linear/cosine
--cond_dim 256             # FiLM æ¡ä»¶ç»´åº¦

# è®­ç»ƒå‚æ•°
--stage1_epochs 30         # Stage 1 è½®æ•°
--stage2_epochs 20         # Stage 2 è½®æ•°
--stage1_lr 1e-4           # Stage 1 å­¦ä¹ ç‡
--stage2_lr 1e-5           # Stage 2 å­¦ä¹ ç‡
--loss_lambda 0.5          # MSE æŸå¤±æƒé‡

# é‡‡æ ·å‚æ•°
--n_samples 100            # æ¦‚ç‡é¢„æµ‹é‡‡æ ·æ•°
--use_ddim                 # ä½¿ç”¨ DDIM åŠ é€Ÿé‡‡æ ·
--ddim_steps 50            # DDIM æ­¥æ•°
--chunk_size 10            # åˆ†å—é‡‡æ ·å¤§å° (æ§åˆ¶æ˜¾å­˜)
--use_amp                  # å¯ç”¨æ··åˆç²¾åº¦ (èŠ‚çœ 30-50% æ˜¾å­˜)
```

### è¯„ä¼°æŒ‡æ ‡

**ç‚¹é¢„æµ‹**: MSE, MAE, RMSE
**æ¦‚ç‡é¢„æµ‹**: CRPS (Continuous Ranked Probability Score), Calibration (50%/90% è¦†ç›–ç‡), Sharpness

---

## iTransformerDiffusionDirect Architecture (ç›´æ¥é¢„æµ‹å˜ä½“)

**è®¾è®¡ç†å¿µ**: ç›´æ¥é¢„æµ‹ç›®æ ‡è€Œéæ®‹å·®ï¼Œæ”¯æŒå¤šç§å‚æ•°åŒ–ç­–ç•¥ï¼ˆxâ‚€/Îµ/vï¼‰ï¼Œè®­ç»ƒæ›´ç¨³å®šã€‚

### ä¸ iTransformerDiffusion çš„å¯¹æ¯”

| ç‰¹æ€§ | iTransformerDiffusion | iTransformerDiffusionDirect |
|------|----------------------|----------------------------|
| **é¢„æµ‹ç›®æ ‡** | æ®‹å·® (y_true - y_det) | ç›´æ¥é¢„æµ‹ y_true |
| **å‚æ•°åŒ–** | å•ä¸€ (å™ªå£°é¢„æµ‹) | å¤šç§ (xâ‚€/Îµ/v) |
| **è®­ç»ƒç¨³å®šæ€§** | éœ€è¦æ®‹å·®å½’ä¸€åŒ– | v-prediction æœ€ç¨³å®š |
| **è®­ç»ƒæ¨¡å¼** | ä¸¤é˜¶æ®µåˆ†ç¦»è®­ç»ƒ | ç«¯åˆ°ç«¯æˆ–ä¸¤é˜¶æ®µ |
| **é€‚ç”¨åœºæ™¯** | ç¡®å®šæ€§ backbone å¼º | ç«¯åˆ°ç«¯æ¦‚ç‡å»ºæ¨¡ |

### æ•´ä½“æ•°æ®æµ

```
Input x_hist [B, seq_len, N]
    â”‚
    â–¼ iTransformer Backbone
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Instance Norm â†’ DataEmbedding_inverted â”‚
â”‚  â†’ Encoder (attention across variates)  â”‚
â”‚  â†’ Projection                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                    â”‚
    â–¼                    â–¼
y_det [B, pred_len, N]   z [B, N, d_model]  (æ¡ä»¶ç‰¹å¾)
    â”‚                    â”‚
    â–¼                    â–¼
ç›®æ ‡ y_true              1D U-Net Denoiser
    â”‚                   (FiLM + CrossAttention)
    â”‚                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    ç›´æ¥é¢„æµ‹ y_true (è®­ç»ƒ)
    æ¦‚ç‡é‡‡æ · (æ¨ç†)
```

### å‚æ•°åŒ–ç­–ç•¥

**v-prediction (æ¨è)** âœ…
- æ‰€æœ‰æ—¶é—´æ­¥ä¿¡å™ªæ¯”å¹³è¡¡
- æ— éœ€ clamp() ç¨³å®šé¢„æµ‹
- æ›´å¥½çš„æ¢¯åº¦æµåŠ¨
- æ•°å­¦å®šä¹‰: v = âˆšá¾±_t Â· Îµ âˆ’ âˆš(1-á¾±_t) Â· xâ‚€

**xâ‚€-prediction** ğŸŸ¡
- ç›´æ¥é¢„æµ‹ç›®æ ‡ï¼Œç›´è§‚æ˜“æ‡‚
- éœ€è¦ clamp() é˜²æ­¢æ•°å€¼ä¸ç¨³å®š
- æ—©æœŸæ—¶é—´æ­¥ä¿¡å™ªæ¯”ä½

**Îµ-prediction** ğŸ”´
- DDPM æ ‡å‡†æ–¹æ³•
- åæœŸæ—¶é—´æ­¥ä¿¡å™ªæ¯”ä½
- è®­ç»ƒä¸å¤Ÿç¨³å®š

### å¿«é€Ÿè¿è¡Œ

```bash
# æ¨èé…ç½® (v-prediction, ç«¯åˆ°ç«¯è®­ç»ƒ)
python run.py \
  --task_name diffusion_forecast \
  --is_training 1 \
  --model iTransformerDiffusionDirect \
  --data ETTh1 \
  --root_path ./dataset/ETT-small/ \
  --data_path ETTh1.csv \
  --seq_len 96 --pred_len 96 \
  --enc_in 7 --dec_in 7 --c_out 7 \
  --d_model 64 --d_ff 64 \
  --parameterization v \
  --training_mode end_to_end \
  --train_epochs 50 \
  --n_samples 100 \
  --use_amp

# ä½æ˜¾å­˜ç‰ˆæœ¬ (8GB GPU)
bash scripts/diffusion_forecast/ETT_script/iTransformerDiffusionDirect_ETTh1_v1.sh

# ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼
python run.py \
  --model iTransformerDiffusionDirect \
  --training_mode two_stage \
  --stage1_epochs 30 \
  --stage2_epochs 20 \
  --parameterization v \
  [å…¶ä»–å‚æ•°...]
```

### å…³é”®é…ç½®å‚æ•°

```bash
# å‚æ•°åŒ–é€‰æ‹©
--parameterization v           # v/x0/epsilon (æ¨è v)

# è®­ç»ƒæ¨¡å¼
--training_mode end_to_end     # end_to_end/two_stage
--train_epochs 50              # ç«¯åˆ°ç«¯è®­ç»ƒè½®æ•°
--warmup_epochs 10             # é¢„çƒ­è½®æ•°

# æ‰©æ•£å‚æ•°ï¼ˆä¸ iTransformerDiffusion ç›¸åŒï¼‰
--diffusion_steps 1000
--beta_schedule cosine
--cond_dim 256
```

---

## åŸºç¡€ iTransformer Architecture

**Paper:** https://arxiv.org/abs/2310.06625 (ICLR 2024)

iTransformer inverts the standard Transformer by applying self-attention across **variates (channels)** instead of the temporal dimension. This is a lightweight, encoder-only architecture.

**Data flow:**
```
Input [B, seq_len, variates]
  â†’ Normalize per variate
  â†’ Permute to [B, variates, seq_len]
  â†’ Linear(seq_len â†’ d_model) â†’ [B, variates, d_model]
  â†’ Encoder (attention across variates)
  â†’ Linear(d_model â†’ pred_len) â†’ [B, variates, pred_len]
  â†’ Permute back, denormalize
Output [B, pred_len, variates]
```

**iTransformer-specific settings:**
- Uses smaller `d_model=128, d_ff=128` (vs default 512/2048)
- Typically 2 encoder layers
- No decoder needed (encoder-only)

---

## Testing

```bash
# è¿è¡Œ iTransformerDiffusion å•å…ƒæµ‹è¯•
cd ~/projects/Time-Series-Library
python -m pytest tests/test_iTransformerDiffusion.py -v

# è¾¹ç•Œæƒ…å†µæµ‹è¯•
python -m pytest tests/test_iTransformerDiffusion_edge_cases.py -v

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python -m pytest tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
python -m pytest tests/test_iTransformerDiffusion.py::test_forward -v
```

## ç»“æœåˆ†æä¸è°ƒè¯•

### æŸ¥çœ‹è®­ç»ƒç»“æœ

```bash
# æŸ¥çœ‹æ‰€æœ‰å®éªŒç»“æœ
ls results/diffusion_forecast/

# æŸ¥çœ‹ç‰¹å®šå®éªŒçš„æ—¥å¿—
tail -f checkpoints/<experiment_name>/log.txt

# æŸ¥çœ‹æœ€æ–°å®éªŒæ—¥å¿—
tail -f checkpoints/$(ls -t checkpoints/ | head -1)/log.txt

# æŸ¥çœ‹æµ‹è¯•ç»“æœ
ls test_results/diffusion_forecast/

# æŸ¥çœ‹ç‰¹å®šç»“æœæ–‡ä»¶
cat results/diffusion_forecast/result_<model>_<data>_<seq>_<pred>.txt
```

### æ€§èƒ½ç›‘æ§

```bash
# ç›‘æ§ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# ç›‘æ§è®­ç»ƒè¿›åº¦ï¼ˆä½¿ç”¨é¡¹ç›®å†…ç½®è„šæœ¬ï¼‰
bash scripts/phase2_monitor.sh

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f logs/*.log
```

### è°ƒè¯•æ¨¡å¼

```bash
# å¿«é€Ÿè°ƒè¯•ï¼ˆå°æ•°æ®é›†ï¼Œå°‘è½®æ•°ï¼‰
python run.py \
  --task_name diffusion_forecast \
  --model iTransformerDiffusion \
  --data ETTh1 \
  --train_epochs 2 \
  --stage1_epochs 1 \
  --stage2_epochs 1 \
  --n_samples 10 \
  --batch_size 4

# è¿‡æ‹Ÿåˆå•ä¸ª batchï¼ˆéªŒè¯æ¨¡å‹å®ç°æ­£ç¡®æ€§ï¼‰
python run.py \
  --task_name diffusion_forecast \
  --model iTransformerDiffusion \
  --data ETTh1 \
  --train_epochs 100 \
  --batch_size 1 \
  --num_workers 0
```

### å¸¸è§é—®é¢˜æ’æŸ¥

**æ˜¾å­˜ä¸è¶³ (OOM)**:
```bash
# è§£å†³æ–¹æ¡ˆ 1: å¯ç”¨æ··åˆç²¾åº¦ + å‡å° batch size
--use_amp --batch_size 8

# è§£å†³æ–¹æ¡ˆ 2: åˆ†å—é‡‡æ ·
--chunk_size 5 --n_samples 50

# è§£å†³æ–¹æ¡ˆ 3: å‡å°æ¨¡å‹å°ºå¯¸
--d_model 64 --d_ff 64 --unet_channels [32,64,128,256]
```

**è®­ç»ƒä¸ç¨³å®š**:
```bash
# è§£å†³æ–¹æ¡ˆ 1: ä½¿ç”¨ v-prediction (ä»… Direct æ¨¡å‹)
--parameterization v

# è§£å†³æ–¹æ¡ˆ 2: é™ä½å­¦ä¹ ç‡
--learning_rate 5e-5 --stage2_lr 5e-6

# è§£å†³æ–¹æ¡ˆ 3: å¢åŠ é¢„çƒ­è½®æ•°
--warmup_epochs 20
```

**æ¨ç†é€Ÿåº¦æ…¢**:
```bash
# è§£å†³æ–¹æ¡ˆ: DDIM åŠ é€Ÿé‡‡æ ·
--use_ddim --ddim_steps 20 --n_samples 50
```

## Adding a New Model

1. Create `models/YourModel.py` with `class Model(nn.Module)` taking `configs` arg
2. Import and add to `model_dict` in `exp/exp_basic.py`
3. Create run scripts in `scripts/<task>/<dataset>/YourModel.sh`

## Data

Datasets go in `./dataset/`. Common ones: ETTh1, ETTh2, ETTm1, ETTm2, Weather, ECL, Traffic.

ETT datasets have 7 variates. Set `--enc_in 7 --dec_in 7 --c_out 7`.

## Key Files for iTransformerDiffusion Development

| æ–‡ä»¶ | æè¿° |
|------|------|
| `models/iTransformerDiffusion.py` | â˜… ä¸»æ¨¡å‹å®ç° (backbone + CRD-Net) |
| `models/iTransformerDiffusionDirect.py` | â˜… ç›´æ¥é¢„æµ‹å˜ä½“ (æ”¯æŒ xâ‚€/Îµ/v) |
| `layers/Diffusion_layers.py` | â˜… æ‰©æ•£ç»„ä»¶ (UNet1D, FiLM, CrossAttn) |
| `exp/exp_diffusion_forecast.py` | â˜… ä¸¤é˜¶æ®µè®­ç»ƒé€»è¾‘ |
| `models/GaussianDiffusion.py` | åŸºç¡€é«˜æ–¯æ‰©æ•£å·¥å…·ç±» |
| `models/iTransformer.py` | åŸºç¡€ iTransformer å‚è€ƒ |
| `layers/Embed.py:129-143` | `DataEmbedding_inverted` |
| `scripts/diffusion_forecast/` | æ‰©æ•£é¢„æµ‹è„šæœ¬ |
| `tests/test_iTransformerDiffusion.py` | å•å…ƒæµ‹è¯• |

---

## å¸¸è§å¼€å‘å·¥ä½œæµ

### 1. å®éªŒæ–°æ¨¡å‹å˜ä½“

```bash
# æ­¥éª¤ 1: å¤åˆ¶åŸºç¡€æ¨¡å‹
cp models/iTransformer.py models/MyModel.py

# æ­¥éª¤ 2: ä¿®æ”¹æ¨¡å‹ï¼ˆç¡®ä¿ç±»åä¸º Modelï¼‰
# ç¼–è¾‘ models/MyModel.py

# æ­¥éª¤ 3: æ³¨å†Œæ¨¡å‹
# åœ¨ exp/exp_basic.py çš„ model_dict ä¸­æ·»åŠ :
# 'MyModel': MyModel,

# æ­¥éª¤ 4: åˆ›å»ºè¿è¡Œè„šæœ¬
cp scripts/long_term_forecast/ETT_script/iTransformer.sh \
   scripts/long_term_forecast/ETT_script/MyModel.sh

# æ­¥éª¤ 5: è¿è¡Œæµ‹è¯•
bash scripts/long_term_forecast/ETT_script/MyModel.sh
```

### 2. æ‰¹é‡å®éªŒ

```bash
# å¯åŠ¨å¤šä¸ªå®éªŒï¼ˆåå°è¿è¡Œï¼‰
bash scripts/phase2_launch_all.sh

# ç›‘æ§å®éªŒè¿›åº¦
bash scripts/phase2_monitor.sh

# æ”¶é›†æ‰€æœ‰ç»“æœ
bash scripts/phase2_collect_results.sh

# åˆ†æç»“æœ
python scripts/analyze_prediction_gap.py
```

### 3. æ¨¡å‹å¯¹æ¯”è¯„ä¼°

```bash
# è¿è¡ŒåŸºçº¿æ¨¡å‹ï¼ˆç¡®å®šæ€§é¢„æµ‹ï¼‰
python run.py \
  --task_name long_term_forecast \
  --model iTransformer \
  --data ETTh1 \
  --seq_len 96 --pred_len 96

# è¿è¡Œæ‰©æ•£æ¨¡å‹ï¼ˆæ¦‚ç‡é¢„æµ‹ï¼‰
python run.py \
  --task_name diffusion_forecast \
  --model iTransformerDiffusion \
  --data ETTh1 \
  --seq_len 96 --pred_len 96 \
  --n_samples 100

# å¯¹æ¯”ç»“æœ
# æŸ¥çœ‹ results/ å’Œ test_results/ ç›®å½•ä¸‹çš„ .txt æ–‡ä»¶
```

### 4. è¶…å‚æ•°è°ƒä¼˜

```bash
# å­¦ä¹ ç‡è°ƒä¼˜
for lr in 1e-3 5e-4 1e-4 5e-5; do
  python run.py --learning_rate $lr [å…¶ä»–å‚æ•°...]
done

# æ¨¡å‹å°ºå¯¸è°ƒä¼˜
for dim in 64 128 256; do
  python run.py --d_model $dim --d_ff $dim [å…¶ä»–å‚æ•°...]
done

# æ‰©æ•£æ­¥æ•°è°ƒä¼˜
for steps in 100 500 1000; do
  python run.py --diffusion_steps $steps [å…¶ä»–å‚æ•°...]
done
```

### 5. è·¨æ•°æ®é›†è¯„ä¼°

```bash
# ETT ç³»åˆ—
for data in ETTh1 ETTh2 ETTm1 ETTm2; do
  python run.py --data $data --root_path ./dataset/ETT-small/ \
    --data_path ${data}.csv --enc_in 7 --dec_in 7 --c_out 7 \
    [å…¶ä»–å‚æ•°...]
done

# å¤§è§„æ¨¡æ•°æ®é›†
for data in Weather ECL Traffic; do
  python run.py --data $data --root_path ./dataset/ \
    [è°ƒæ•´ enc_in/dec_in/c_out...] [å…¶ä»–å‚æ•°...]
done
```

---

## é¡¹ç›®æ–‡æ¡£ç´¢å¼•

| æ–‡æ¡£ | æè¿° |
|------|------|
| `README.md` | iTransformerDiffusionDirect æ¨¡å‹è¯´æ˜ï¼ˆä¸»READMEï¼‰ |
| `CLAUDE.md` | Claude Code æ“ä½œæŒ‡å—ï¼ˆæœ¬æ–‡ä»¶ï¼‰ |
| `docs/iTransformerDiffusionDirect_Technical_Document.md` | æŠ€æœ¯æ–‡æ¡£è¯¦è§£ |
| `docs/iTransformerDiffusionDirect_Technical_Doc.md` | æŠ€æœ¯æ–‡æ¡£ï¼ˆç®€ç‰ˆï¼‰ |
| `docs/iTransformerDiffusionDirect_Refactoring_Plan.md` | é‡æ„è®¡åˆ’ |
| `docs/FR2_INTEGRATION_GUIDE.md` | ç‰¹å¾é‡æ„é›†æˆæŒ‡å— |
| `tests/TEST_SUMMARY.md` | æµ‹è¯•æ€»ç»“ |
| `HOW_TO_USE_BEST_MODEL.md` | æœ€ä½³æ¨¡å‹ä½¿ç”¨æŒ‡å— |
| `IMPLEMENTATION_SUMMARY.md` | å®ç°æ€»ç»“ |
| `CONTRIBUTING.md` | è´¡çŒ®æŒ‡å— |

---

## ç›¸å…³è®ºæ–‡

- **iTransformer**: [iTransformer: Inverted Transformers Are Effective for Time Series Forecasting](https://arxiv.org/abs/2310.06625) (ICLR 2024)
- **DDPM**: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (NeurIPS 2020)
- **DDIM**: [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502) (ICLR 2021)
- **v-Prediction**: [Progressive Distillation for Fast Sampling](https://arxiv.org/abs/2202.00512) (ICLR 2022)
- **Diffusion for Time Series**: Multiple recent works on probabilistic forecasting
