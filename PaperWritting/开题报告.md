# 开题报告

## 课题名称

**基于扩散机制融合的时间序列概率预测研究**

---

## 摘要

时间序列预测是数据科学的核心任务，在电力调度、金融风险评估等领域具有重要应用价值。传统确定性预测方法无法量化不确定性，而现有概率预测方法（如扩散模型）的点预测质量显著低于确定性方法。针对该问题，本研究提出 iDiffFormer（iTransformer-Diffusion Forecaster），融合变量级注意力机制与条件扩散模型。核心创新包括：(1) 直接预测扩散范式，使目标分布更规则，训练更稳定；(2) v-prediction 参数化策略，在各时间步保持均衡信噪比；(3) FiLM + VariateCrossAttention 双重条件注入机制，充分利用历史序列的多变量信息；(4) Median-of-Means 聚合方法，显著提升点预测精度（改善 8.6%）。本研究旨在为时间序列概率预测提供高质量的点预测与可靠的不确定性量化，缩小概率方法与确定性方法的性能差距。

---

## 一、目标及意义（含国内外研究现状分析）

### 1.1 研究背景

时间序列预测是数据科学领域的核心任务，广泛应用于电力负荷预测、金融风险评估、气象预报、交通流量估计等领域。传统的时间序列预测方法以点预测为主，如 ARIMA、Prophet 等统计方法，以及近年来兴起的深度学习方法如 LSTM、Transformer 及其变体。然而，这些确定性预测方法仅输出单一预测值，无法量化预测的不确定性。在实际决策场景中，决策者不仅需要知道"预测值是多少"，更需要知道"这个预测有多可靠"。例如，电网调度需要考虑负荷预测的置信区间来安排发电计划；金融投资需要评估收益预测的风险范围。因此，能够提供概率分布预测的方法具有重要的理论和实践价值。

### 1.2 研究问题

本研究要解决的核心问题是：**如何在保持高精度点预测的同时，提供可靠的不确定性量化**？

现有方法面临两大困境：

**(1) 概率模型点预测质量差**

现有的概率预测方法（如 TimeGrad、CSDI 等扩散模型）虽然能够提供不确定性量化，但点预测精度显著低于确定性方法。例如，在 ETTh1 数据集上，TimeGrad 的 MSE 约为 0.94，而确定性方法 iTransformer 仅为 0.39，性能差距达 **140%**。这导致概率预测方法在实际应用中难以被接受，因为用户首要关注的仍是预测准确性。

**(2) 训练不稳定，收敛困难**

扩散模型在时间序列领域的训练存在不稳定问题。现有方法多采用残差预测策略（预测 $y_{\text{true}} - y_{det}$），由于残差分布不规则，需要复杂的归一化策略（如 CSDI 的 score-based 训练、ResidualNormalizer 等）才能保证收敛。这不仅增加了模型复杂度，也导致训练效率低下。

基于上述问题，本研究提出以下关键研究问题：

- **RQ1**：能否设计一种直接预测策略，使目标分布更规则，从而简化训练流程并提升稳定性？
- **RQ2**：在扩散模型的多种参数化方式中，哪种最适合时间序列预测？是否存在能够在各时间步保持均衡信噪比的参数化策略？
- **RQ3**：如何设计有效的条件注入机制，充分利用历史序列的多变量信息，提升预测质量？
- **RQ4**：能否通过改进的采样聚合策略，在不牺牲分布质量的前提下，显著提升点预测精度？

### 1.3 国内外研究现状

#### 1.3.1 确定性预测方法的演进

时间序列预测经历了从统计模型到深度学习的发展历程。早期的 ARIMA、指数平滑等统计方法在单变量场景表现良好，但难以捕捉复杂的非线性关系。深度学习方法的兴起带来了 RNN、LSTM、GRU 等序列模型，能够建模长期依赖。2017年 Transformer 架构的提出为时间序列预测带来新的范式，Informer (AAAI 2021)、Autoformer (NeurIPS 2021)、FEDformer (ICML 2022) 等工作在长序列预测上取得突破。近期，iTransformer (ICLR 2024) 提出"倒置"注意力机制，在变量维度而非时间维度应用自注意力，显著提升了多变量预测性能。

#### 1.3.2 扩散模型的发展

扩散概率模型 (Denoising Diffusion Probabilistic Models, DDPM) 由 Ho 等人于 2020 年提出，通过逐步添加噪声再逐步去噪的方式实现生成建模，在图像生成领域取得了优异成果。DDIM (2021) 提出确定性采样方案，将采样速度提升 10-50 倍。Salimans 等 (2022) 提出 v-prediction 参数化，在各时间步保持均衡的信噪比，改善了训练稳定性。

#### 1.3.3 扩散模型在时间序列领域的应用

TimeGrad (ICML 2021) 首次将扩散模型引入时间序列预测，使用自回归方式逐步生成未来序列。CSDI (NeurIPS 2021) 提出条件扩散模型用于时间序列插补。D3VAE (NeurIPS 2022) 结合变分自编码器与扩散模型。SimDiff (NeurIPS 2023) 提出 Median-of-Means 聚合方法改善点预测质量。

#### 1.3.4 现有方法的局限性

综合上述研究现状，现有时间序列扩散方法存在以下三大局限性：

**(1) 点预测质量显著不足**

现有扩散模型在时序预测中的点预测性能远低于确定性方法。例如，TimeGrad 在 ETTh1 数据集（96→96 预测任务）上的 MSE 约为 0.94，而同期的 iTransformer 仅为 0.39，性能差距达 140%。这一巨大差距严重限制了概率预测方法在实际场景中的应用。

**(2) 训练不稳定，需要复杂的归一化策略**

由于多数方法采用残差预测（预测 $y_{\text{true}} - y_{det}$），而残差分布往往不规则且方差不稳定，导致训练过程容易发散。CSDI 采用 score-based 训练缓解此问题，但引入了额外的计算开销。其他方法需要设计专门的 ResidualNormalizer 模块来稳定训练，增加了模型复杂度。

**(3) 条件机制设计简单，历史信息利用不充分**

现有方法的条件注入机制多采用简单的特征拼接或加法融合，未能充分建模历史序列与目标序列之间的复杂依赖关系。特别是在多变量场景中，变量间的相互作用（如温度影响负荷、多个传感器的关联）未能得到有效捕捉，导致条件信息的利用效率低下。

这些问题共同导致了一个矛盾：虽然扩散模型在理论上能够提供高质量的不确定性量化，但在实际应用中，由于点预测质量差、训练困难等问题，其实用性大打折扣。

### 1.4 研究意义

#### 1.4.1 理论意义

本研究提出将变量级注意力机制与条件扩散模型深度融合的新范式 iDiffFormer（iTransformer-Diffusion Forecaster）。针对现有方法的局限性，本研究提出以下**核心创新点**：

**创新点 1：直接预测扩散范式**

不同于现有方法的残差预测策略（预测 $y_{\text{true}} - y_{\text{det}}$），本研究提出直接预测目标序列（直接预测 $y_{\text{true}}$）。理论分析表明，直接预测的目标分布更规则，无需额外的残差归一化模块，训练更稳定、收敛更快。这一创新有效缓解了扩散模型在时序领域训练不稳定的问题。

**创新点 2：v-prediction 参数化在时序领域的系统性研究**

本研究首次系统性地对比了 x₀-prediction、ε-prediction 和 v-prediction 三种参数化策略在时间序列领域的表现。通过理论推导和实验验证，揭示了 v-prediction 在各时间步保持均衡信噪比的优势，为时序扩散模型的参数化选择提供了理论依据。

**创新点 3：FiLM + VariateCrossAttention 双重条件注入机制**

设计了粗粒度（FiLM 全局调制）与细粒度（变量交叉注意力）相结合的条件注入机制。FiLM 层通过可学习的缩放和平移参数进行全局条件调制；VariateCrossAttention 使去噪特征与编码器的变量级表示进行精细化交互，充分利用历史序列的多变量信息。

**创新点 4：Median-of-Means robust 聚合方法的应用**

借鉴 SimDiff 提出的 Median-of-Means (MoM) 方法，将多个采样分组后取组均值的中位数，相比简单均值对异常样本更具鲁棒性。实验表明，该方法使点预测 MSE 改善 8.6%，有效缩小了概率预测与确定性方法的性能差距。

这些理论贡献为时间序列概率预测提供了新的研究视角，推动了扩散模型在该领域的实用化进程。

#### 1.4.2 实践意义

iDiffFormer 能够同时提供高质量的点预测和可靠的不确定性量化，输出完整的预测分布。在电力、金融、气象等需要风险评估的场景中，决策者可以根据预测区间制定更稳健的策略。同时，本研究提出的 Median-of-Means 聚合方法使点预测 MSE 改善 8.6%，有效缩小了与确定性方法的差距，为概率预测方法在实际应用中的推广奠定了基础。

---

## 二、研究设计的基本内容、目标、拟采用的技术方案及措施

### 2.1 研究目标

本研究旨在设计一种融合变量级注意力机制与条件扩散模型的时间序列概率预测方法 iDiffFormer，实现以下目标：
1. 高质量的点预测，缩小与确定性方法的差距
2. 可靠的不确定性量化，提供校准良好的预测区间
3. 高效的训练与推理，适用于实际部署场景

### 2.2 研究假设

本研究基于以下假设：

**H1（直接预测优于残差预测）**：直接预测目标序列的分布比残差分布更规则，训练过程更稳定，收敛速度更快。

**H2（v-prediction 最优）**：在 x₀-prediction、ε-prediction 和 v-prediction 三种参数化策略中，v-prediction 在各时间步保持均衡的信噪比，训练最稳定，性能最优。

**H3（双重条件优于单一条件）**：FiLM + VariateCrossAttention 的双重条件注入机制比单一条件方式（仅 FiLM 或仅 CrossAttention）更有效，能够充分利用历史序列的多变量信息。

**H4（MoM 聚合提升点预测）**：Median-of-Means 聚合方法比简单均值对异常样本更具鲁棒性，能够在保持分布质量的前提下显著提升点预测精度。

### 2.3 理论基础

#### 2.3.1 扩散模型数学框架

扩散模型包含前向加噪和逆向去噪两个过程。前向过程定义为：

$$q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)$$

其中 $\beta_t$ 为噪声调度参数。逆向去噪过程为：

$$p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t,t), \Sigma_\theta(x_t,t))$$

通过学习噪声预测网络 $\epsilon_\theta(x_t, t)$，模型可以从纯噪声逐步恢复出原始数据。

---

**【图3：扩散模型前向与逆向过程】**

*说明：此图展示扩散模型的完整工作流程。上半部分为前向扩散过程（$x_0$ → $x_1$ → ... → $x_T$，逐步加噪），下半部分为逆向去噪过程（$x_T$ → ... → $x_1$ → $x_0$，学习去噪），并标注前向过程和逆向过程的数学公式。*

*生成方法：运行 `cd PaperWritting && python generate_figures.py`（需先安装matplotlib：`pip install matplotlib`）*

---

#### 2.3.2 条件生成理论

条件扩散模型引入条件特征 $c$，使生成过程依赖于输入条件：

$$p_\theta(y|x) = \int p_\theta(y|z)p(z|x)dz$$

其中 $z$ 为从历史序列 $x$ 提取的条件特征向量。条件注入机制通过 FiLM 调制或交叉注意力将 $z$ 融入去噪网络，指导模型生成符合条件约束的预测分布。

#### 2.3.3 概率预测评估理论

连续排名概率分数（CRPS）是评估概率预测质量的标准指标，定义为：

$$\text{CRPS}(F, y) = \int_{-\infty}^{\infty} [F(x) - \mathbb{1}_{x \geq y}]^2 dx$$

其中 $F$ 为预测分布的累积分布函数，$y$ 为真实观测值。CRPS 综合评估预测分布与真实值的匹配程度，值越小表示预测质量越高。

### 2.4 研究内容

#### 2.4.1 iTransformer 条件特征提取器设计

采用 iTransformer 作为条件特征提取器（backbone），其核心创新是将自注意力从时间维度转移到变量维度。对于输入序列 $X \in \mathbb{R}^{B \times T \times N}$，通过线性投影将时间维度压缩到隐空间，然后在变量维度应用多头自注意力，捕捉变量间的相互依赖关系。

**设计原因**：

1. **变量级注意力的必要性**：传统 Transformer 在时间维度应用注意力，忽略了多变量之间的相互作用。在电力负荷预测等场景中，变量间存在强因果关系（如温度→负荷、湿度→负荷），变量级注意力能够显式建模这些依赖。

2. **作为条件特征的合理性**：iTransformer 的编码器输出 $z \in \mathbb{R}^{B \times N \times d_{model}}$ 包含了每个变量的全局表示，这种跨变量的高级特征非常适合作为扩散模型的条件输入，指导去噪过程关注变量间的协同演化模式。

3. **确定性预测能力**：iTransformer 本身具备强大的确定性预测能力（ETTh1 MSE 0.39），为扩散模型提供了一个高质量的初始预测 $y_{\text{det}}$，这是端到端训练成功的基础。

#### 2.4.2 直接预测扩散模型设计

区别于现有方法的残差预测策略（预测 $y_{\text{true}} - y_{det}$），本研究采用直接预测方式（直接预测 $y_{\text{true}}$）。同时，本研究系统性地实现了三种参数化策略：
- **x₀-prediction**：直接预测干净数据 $x_0$
- **ε-prediction**：预测添加的噪声 $\epsilon$（DDPM 标准方法）
- **v-prediction**：预测 $v = \sqrt{\bar{\alpha}_t} \cdot \epsilon - \sqrt{1-\bar{\alpha}_t} \cdot x_0$（本研究推荐）

**设计原因**：

**（1）为什么采用直接预测而非残差预测？**

残差预测存在三大问题：

- **分布不规则性**：残差 $r = y_{true} - y_{det}$ 的分布依赖于确定性模型的预测质量。当 $y_{\text{det}}$ 在不同样本、不同时间步的误差分布不一致时，残差的方差会剧烈波动，导致扩散模型难以学习稳定的去噪模式。

- **归一化代价**：为了稳定训练，现有方法需要设计专门的 ResidualNormalizer 模块，使用指数移动平均（EMA）跟踪残差的均值和方差。这不仅增加了模型复杂度，还引入了额外的超参数（EMA 衰减率、初始化策略等）。

- **梯度传播受限**：在两阶段训练中，残差预测需要先冻结 backbone，导致梯度无法从扩散损失回传到特征提取器，限制了模型的联合优化空间。

直接预测的优势：

- **目标分布规则**：直接预测 $y_{\text{true}}$，目标分布就是原始时间序列的分布，通常符合特定的统计特性（如平稳性、周期性），更容易被扩散模型捕捉。

- **无需额外归一化**：原始序列通常已经过 Instance Normalization，无需额外的归一化模块，简化了模型设计。

- **支持端到端训练**：梯度可以从扩散损失直接回传到 backbone，使得特征提取器能够学习到更适合概率预测的表示。

---

**【图1：直接预测 vs 残差预测对比】**

*说明：此图对比了两种策略的数据流。上半部分展示残差预测流程（历史序列 → iTransformer → 分支为确定性预测和条件特征 → 残差计算 → DDPM预测残差 → 最终预测），下半部分展示直接预测流程（历史序列 → iTransformer条件编码器 → 条件特征 → DDPM直接预测目标 → 最终预测），并标注关键差异点（目标分布规则、无需残差归一化、训练更稳定）。*

*生成方法：运行 `cd PaperWritting && python generate_figures.py`（需先安装matplotlib：`pip install matplotlib`）*

---

**（2）为什么 v-prediction 最优？**

扩散模型的训练目标是最小化去噪误差，但不同参数化在不同时间步 $t$ 的信噪比（SNR）差异巨大：

- **x₀-prediction**：
  - 早期时间步（$t$ 大，噪声多）：从强噪声中预测 $x_0$ 非常困难，需要网络具备强大的"去噪能力"，但此时 SNR 极低，梯度信号微弱。
  - 需要 clamp() 操作将预测限制在合理范围内，否则数值容易发散。
  - 优势：直观，收敛后的预测质量好。

- **ε-prediction（DDPM 标准）**：
  - 后期时间步（$t$ 小，噪声少）：此时噪声 $\epsilon$ 已经很小，预测目标趋近于零，梯度信号同样微弱。
  - 导致训练后期收敛缓慢。

- **v-prediction（本研究推荐）**：
  - 数学定义：$v = \sqrt{\bar{\alpha}_t} \cdot \epsilon - \sqrt{1-\bar{\alpha}_t} \cdot x_0$
  - **关键性质**：在所有时间步 $t$，$v$ 的范数保持相对稳定，SNR 均衡。
  - **理论保证**：Salimans & Ho (2022) 证明，v-prediction 使得损失函数在所有时间步的权重相等，避免了某些时间步"被忽略"的问题。
  - **实践优势**：无需 clamp() 操作，训练稳定性最高，收敛速度最快。

因此，本研究选择 v-prediction 作为默认参数化策略，并通过消融实验验证其优越性。

#### 2.4.3 条件注入机制设计

设计 FiLM (Feature-wise Linear Modulation) + VariateCrossAttention 的双重条件注入机制：
- **FiLM 层**：通过可学习的缩放参数 $\gamma$ 和平移参数 $\beta$ 对去噪网络的特征进行调制，实现全局条件注入
- **变量交叉注意力**：去噪网络的输出作为 Query，编码器特征作为 Key/Value，实现精细化的条件融合

**设计原因**：

**（1）为什么需要条件注入？**

扩散模型的去噪网络 $\epsilon_\theta(x_t, t)$ 需要知道"应该去噪成什么样"。在时间序列预测中，这个"目标"由历史序列决定。如果没有条件注入，去噪网络只能基于当前的噪声状态 $x_t$ 和时间步 $t$ 进行盲目去噪，无法利用历史信息，预测结果将完全随机。

条件注入使得去噪过程变为：$\epsilon_\theta(x_t, t, c)$，其中 $c$ 是从历史序列提取的条件特征，指导去噪方向。

**（2）为什么需要 FiLM（粗粒度调制）？**

FiLM 的数学形式为：$h' = \gamma(c) \odot h + \beta(c)$

- **全局调制作用**：$\gamma$ 和 $\beta$ 是通过条件 $c$ 生成的标量或向量，作用于去噪网络的每一层特征 $h$。这种"缩放+平移"操作能够全局性地调整特征的幅度和偏移，类似于"告诉网络这个样本的整体尺度"。

- **高效性**：参数量极小（仅两个线性层），但对整个网络的行为产生全局影响，适合捕捉条件的宏观特性（如序列的平均水平、波动幅度）。

- **生物学启发**：FiLM 源自神经科学中的"门控机制"，能够动态调控信息流，在图像生成（StyleGAN、ControlNet）中已被证明非常有效。

**（3）为什么需要 VariateCrossAttention（细粒度交互）？**

CrossAttention 的数学形式为：$\text{Attn}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}})V$

- **细粒度对齐**：去噪网络的特征作为 Query，编码器的变量级表示作为 Key/Value。Attention 机制能够让去噪网络"主动查询"历史序列中与当前去噪位置最相关的变量信息。

- **变量间依赖建模**：在多变量场景中，不同变量的预测目标之间存在复杂依赖（如温度和负荷的非线性关系）。CrossAttention 允许去噪网络为每个变量动态选择需要关注的历史变量，实现变量级的精细化条件融合。

- **自适应加权**：Attention 权重是数据驱动的，网络可以学习到"在去噪的哪个阶段需要关注哪些历史信息"，这种自适应性是 FiLM 的固定调制无法实现的。

---

**【图2：FiLM + CrossAttention 双重条件机制示意图】**

*说明：此图展示双重条件注入机制的工作原理。左侧为FiLM全局调制模块（条件特征z → 生成γ和β → 对去噪特征h进行缩放和平移），右侧为VariateCrossAttention细粒度交互模块（去噪特征作为Query，条件特征z作为Key/Value → 注意力计算 → 融合特征），中间展示两者融合过程。*

*生成方法：运行 `cd PaperWritting && python generate_figures.py`（需先安装matplotlib：`pip install matplotlib`）*

---

**（4）为什么需要双重机制？**

FiLM 和 CrossAttention 的作用是互补的：

| 维度 | FiLM | CrossAttention |
|------|------|----------------|
| **作用粒度** | 全局（整个特征图） | 局部（变量级对齐） |
| **计算复杂度** | 极低（线性） | 较高（二次） |
| **捕捉信息** | 宏观统计特性 | 微观依赖关系 |
| **适用场景** | 序列的整体尺度、趋势 | 变量间的因果关系 |

实验证明，单独使用 FiLM 会导致变量间依赖建模不足；单独使用 CrossAttention 会增加计算开销且缺乏全局调控。双重机制结合了两者的优势：FiLM 快速调整全局特性，CrossAttention 精细捕捉局部依赖。

#### 2.4.4 端到端联合训练策略

提出端到端联合训练方案，联合优化确定性预测损失（MSE）和概率预测损失（Diffusion），训练目标为：

$$\mathcal{L}_{total} = \lambda \cdot \mathcal{L}_{MSE}(y_{det}, y_{true}) + (1-\lambda) \cdot \mathcal{L}_{Diffusion}$$

其中 $\lambda \in [0,1]$ 控制两种损失的权重平衡。

**设计原因**：

**（1）两阶段训练的局限性**

传统两阶段训练策略：
- **Stage 1**：单独训练 backbone（冻结扩散模块），优化 MSE
- **Stage 2**：冻结 backbone，单独训练扩散模块

存在以下问题：

- **梯度隔离**：Stage 2 中，扩散损失的梯度无法回传到 backbone，导致特征提取器无法学习到"适合概率预测"的表示。Backbone 只针对确定性预测优化，可能错过对不确定性建模有用的特征。

- **条件特征次优**：Backbone 输出的条件特征 $z$ 是为了最小化点预测误差而设计的，而扩散模型需要的是"能够指导多样本生成"的特征，两者的目标存在偏差。

- **训练效率低**：需要两次完整的训练流程，总训练时间增加。

**（2）端到端训练的优势**

- **梯度连通**：所有参数（backbone + 扩散模块）同时优化，扩散损失的梯度可以回传到特征提取器，使其学习到同时适合点预测和分布预测的表示。

- **联合优化空间**：Backbone 在学习确定性特征的同时，也接收到"如何帮助扩散模型生成高质量样本"的信号，实现了两种目标的协同优化。

- **训练效率高**：单次训练即可完成，节省时间和计算资源。

**（3）损失权重平衡的必要性**

为什么需要 $\lambda$ 来平衡两种损失？

- **尺度差异**：MSE 损失和扩散损失的数值尺度可能相差几个数量级，如果直接相加，较大的损失会主导梯度更新。

- **目标冲突**：MSE 追求"单一最优预测"，扩散追求"多样化合理预测"。如果 $\lambda$ 过大（过度强调 MSE），模型会退化为确定性预测；如果 $\lambda$ 过小（忽略 MSE），点预测质量会严重下降。

- **课程学习策略**：固定 $\lambda=0.5$ 或采用动态调整（早期侧重 MSE 稳定训练，后期增加扩散权重提升分布质量），实现"先学确定性骨架，再学概率分布"的渐进式优化。

实验表明，端到端训练在保持点预测质量的同时，概率预测性能（CRPS、Calibration）显著优于两阶段训练。

#### 2.4.5 高效采样与 Robust 聚合

- **DDIM 加速采样**：50 步代替 1000 步，速度提升约 20 倍
- **批量并行采样**：充分利用 GPU 并行计算能力
- **Median-of-Means 聚合**：将多个采样分组，取组均值的中位数作为最终预测

**设计原因**：

**（1）为什么需要 DDIM 加速采样？**

DDPM 标准采样需要 1000 步马尔可夫链迭代：$x_{t-1} = \alpha(x_t, \epsilon_\theta(x_t, t)) + \sigma \cdot z$

- **问题**：推理速度慢，1000 次网络前向传播导致实时性差，难以在生产环境部署。

DDIM (Denoising Diffusion Implicit Models) 提出确定性采样：

- **核心思想**：将马尔可夫链改为非马尔可夫过程，允许"跳步"采样（如只采样 $t \in \{1000, 950, 900, ..., 50, 0\}$ 这 20 个时间步）。

- **数学保证**：DDIM 证明，在确定性设置下（$\eta=0$），跳步采样的结果与完整 1000 步采样在数学上等价，不损失生成质量。

- **实践效果**：50 步 DDIM 采样与 1000 步 DDPM 采样的 CRPS 差异 < 1%，但速度提升约 20 倍。

**（2）为什么需要 Median-of-Means (MoM) 聚合？**

扩散模型的推理过程是随机的，每次采样得到的结果都不同。为了获得点预测，需要对多个样本进行聚合。

**传统方法：简单均值**

$$\hat{y} = \frac{1}{N} \sum_{i=1}^{N} y^{(i)}$$

- **问题**：均值对异常样本非常敏感。如果某次采样产生了"质量较差"的样本（偏离真实分布），会显著拉偏最终预测。
- **数学性质**：均值的 breakdown point（崩溃点）为 0，即只要 1 个异常值就可能严重影响结果。

**MoM 方法：分组取中位数**

1. 将 $N$ 个样本分成 $K$ 组：$\{G_1, G_2, ..., G_K\}$
2. 计算每组的均值：$\mu_k = \frac{1}{|G_k|} \sum_{y \in G_k} y$
3. 取组均值的中位数：$\hat{y} = \text{median}(\mu_1, \mu_2, ..., \mu_K)$

例如：100 个样本 → 10 组（每组 10 个）→ 10 个组均值 → 取中位数

**MoM 的优势**：

- **鲁棒性**：中位数的 breakdown point 为 50%，即最多 50% 的异常值才能破坏结果，远高于均值的 0%。

- **理论保证**：Jerrum et al. (1986) 证明，MoM 估计量在高维空间中具有次高斯尾部，误差界更紧。

- **实验验证**：SimDiff (NeurIPS 2023) 在时间序列预测中首次应用 MoM，点预测 MSE 改善 8-12%。本研究在 ETTh1 数据集上复现了类似改进（MSE 从 0.7062 降至 0.6452，改善 8.6%）。

**（3）为什么分组策略有效？**

分组的本质是"降低方差 + 提高鲁棒性"的权衡：

- **组内求均值**：减少随机采样的方差，使每组的统计量更稳定。
- **组间取中位数**：抵抗异常组的影响，即使某几组采样质量差，中位数仍能指向"大部分组的共识"。

分组数 $K$ 的选择：
- 太小（$K=2,3$）：鲁棒性不足，中位数仍可能被异常值影响。
- 太大（$K=N$）：退化为简单中位数，损失均值的低方差优势。
- **推荐**：$K = \sqrt{N}$ 或 $K=10$（当 $N=100$ 时），平衡鲁棒性和方差。

因此，MoM 聚合是改善扩散模型点预测质量的关键技术，在保持分布质量（CRPS、Calibration）不变的前提下，点预测精度改善约 8-12%，缩小与确定性方法的差距。

### 2.5 研究方法论

本研究采用**设计科学研究方法**（Design Science Research, DSR）作为核心方法论框架。DSR 强调通过设计和构建创新性人工制品（Artifact）来解决实际问题，适合本研究提出新型预测模型的目标。研究循环包括：问题识别（现有概率预测方法的点预测质量不足）→ 方案设计（iDiffFormer 架构）→ 原型实现（PyTorch 实现）→ 实验验证（ETT 数据集评估）→ 结果评估（与基线对比）。该方法论确保研究兼具理论贡献与实践价值。

### 2.6 技术路线

```
历史序列 X [B, seq_len, N]
    │
    ▼
┌─────────────────────────────────┐
│  iTransformer Backbone          │
│  ├─ Instance Normalization      │
│  ├─ DataEmbedding_inverted      │
│  ├─ Encoder (变量级注意力)       │
│  └─ Projection                  │
└─────────────────────────────────┘
    │                    │
    ▼                    ▼
y_det (确定性预测)    z (条件特征)
                         │
                         ▼
┌─────────────────────────────────┐
│  1D U-Net Denoiser              │
│  ├─ ConditionProjector          │
│  ├─ Encoder (DownBlocks + FiLM) │
│  ├─ Bottleneck (CrossAttention) │
│  └─ Decoder (UpBlocks + FiLM)   │
└─────────────────────────────────┘
    │
    ▼
概率采样 → MoM 聚合 → 最终预测
```

### 2.7 实验设计

#### 2.7.1 数据集选择

本研究使用 **ETT（Electricity Transformer Temperature）系列数据集**进行实验验证：

- **数据来源**：某省电力公司，记录 2016-2018 年变压器运行数据
- **变量构成**：7 个变量（油温 OT、负荷电流等）
- **数据规模**：
  - ETTh1/h2：各 17,420 个样本（1 小时采样频率）
  - ETTm1/m2：各 69,680 个样本（15 分钟采样频率）
- **训练/验证/测试划分**：按 6:2:2 比例时序划分

**选择理由**：

1. **标准 benchmark**：ETT 系列是时间序列预测领域的标准评测数据集，iTransformer、PatchTST、TimesNet 等 SOTA 方法均在该数据集上报告结果，便于公平对比。
2. **变量数适中**：7 个变量适合验证多变量建模能力，既不会因变量过少而无法体现变量级注意力的优势，也不会因变量过多而导致计算资源不足。
3. **多尺度特性**：包含小时级（h1/h2）和 15 分钟级（m1/m2）两种时间粒度，可验证模型在不同时间尺度上的泛化能力。
4. **实际应用价值**：电力负荷预测是概率预测的典型应用场景，需要准确的不确定性量化以支持电网调度决策。

**预测任务配置**：历史窗口 96 步 → 预测未来 {96, 192, 336, 720} 步，涵盖短期（1-2 天）到长期（1 个月）预测。

#### 2.7.2 基线方法

**确定性预测方法**（点预测性能对标）：

| 方法 | 来源 | 选择理由 |
|------|------|---------|
| **iTransformer** | ICLR 2024 | 本研究的 backbone，评估扩散机制的增益 |
| **PatchTST** | ICLR 2023 | 当前 SOTA 确定性方法，基于 Patch 分割 |
| **TimesNet** | ICLR 2023 | 2D 时序变化建模，代表性强 |
| **DLinear** | AAAI 2023 | 简单线性基线，验证复杂模型的必要性 |

**概率预测方法**（分布质量对标）：

| 方法 | 来源 | 选择理由 |
|------|------|---------|
| **TimeGrad** | ICML 2021 | 时序扩散开山之作，自回归扩散 |
| **CSDI** | NeurIPS 2021 | 条件分数扩散，用于插补但可适配预测 |
| **D3VAE** | NeurIPS 2022 | VAE + 扩散混合方法 |

#### 2.7.3 评估指标

**点预测指标**（与确定性方法对标）：
- **MSE** (Mean Squared Error)：均方误差，主要评估指标
- **MAE** (Mean Absolute Error)：平均绝对误差，鲁棒性指标

**概率预测指标**（与概率方法对标）：
- **CRPS** (Continuous Ranked Probability Score)：综合评估预测分布与真实值的符合度，越低越好
- **Calibration**：50%/90% 预测区间的实际覆盖率，评估置信区间的校准度（理想值分别为 50%/90%）
- **Sharpness**：预测区间宽度，在保证覆盖率的前提下越小越好（更自信）

**效率指标**：
- 训练时间（GPU 小时）
- 推理速度（样本/秒）
- 显存占用（GB）

**统计显著性检验**：所有实验重复 3 次（随机种子为 42, 123, 456），报告均值和标准差。使用配对 t 检验判断改进是否显著，显著性水平设为 $p < 0.05$。

#### 2.7.4 实现细节

1. **开发框架**：PyTorch，基于 Time-Series-Library 代码库
2. **计算资源**：NVIDIA RTX 4060 笔记本 GPU（24GB 显存）
3. **运行环境**：WSL2 Ubuntu
4. **优化技术**：
   - 混合精度训练（AMP）：节省显存，使硬件能够支持完整训练
   - 梯度裁剪：防止梯度爆炸，提高训练稳定性
   - 分块采样：控制推理时的显存占用
   - DDIM 加速采样：大幅减少采样步数，提升推理速度
   - 动态 Batch size：根据显存约束自适应调整

#### 2.7.5 消融实验设计

为验证各组件的有效性，设计以下消融实验：

**实验组 1：参数化策略对比**
- iDiffFormer-x0（x₀-prediction）
- iDiffFormer-ε（ε-prediction）
- iDiffFormer-v（v-prediction，默认）

**实验组 2：训练策略对比**
- 端到端联合训练（推荐）
- 两阶段训练（对照）

**实验组 3：条件机制消融**
- 仅 FiLM 条件
- 仅 CrossAttention 条件
- FiLM + CrossAttention（完整模型）

**实验组 4：聚合方法对比**
- 简单均值
- Median-of-Means（推荐）

### 2.8 可行性分析

#### 2.8.1 技术可行性

**模型架构成熟**：iTransformer 和 DDPM 均为已验证的成熟技术，本研究的创新在于融合方式而非全新架构，技术风险可控。

**开源代码基础**：Time-Series-Library 提供了完整的时序预测框架，iTransformer 已实现；扩散模型可复用图像生成领域的成熟代码（如 denoising-diffusion-pytorch）。

**理论基础扎实**：直接预测和 v-prediction 的理论推导清晰，数学原理可验证。

#### 2.8.2 资源可行性

**计算资源**：NVIDIA RTX 4060 笔记本 GPU（24GB 显存）满足训练需求。采用混合精度训练（AMP）和分块采样等优化技术后，硬件资源充足，能够支持完整的模型训练和推理。

**数据资源**：ETT 数据集公开可获取，无数据获取障碍。

**时间资源**：3 个月周期合理，模型实现和实验验证均有充足时间。

#### 2.8.3 潜在风险与应对措施

| 风险 | 可能性 | 影响 | 应对措施 |
|------|--------|------|---------|
| **扩散训练不收敛** | 中 | 高 | (1) 采用 v-prediction 稳定训练；(2) 梯度裁剪 + 学习率预热；(3) 调整超参数 |
| **点预测质量不足** | 中 | 高 | (1) MoM 聚合改善点预测；(2) 端到端联合训练；(3) 调整损失权重平衡 |
| **计算资源不足** | 低 | 中 | (1) 混合精度训练；(2) 分块采样；(3) 动态调整 batch size |
| **基线复现困难** | 中 | 中 | (1) 使用 Time-Series-Library 内置基线；(2) 参考文献报告结果进行对比 |
| **时间不足** | 低 | 高 | (1) 优先完成核心数据集实验；(2) 消融实验聚焦关键对比；(3) 预留调整时间 |

---

## 三、进度安排

本研究计划在 3 个月（12 周）内完成，具体进度安排如下：

| 阶段 | 周次 | 主要任务 | 预期成果 |
|:----:|:----:|---------|---------|
| **文献调研** | 第1-2周 | 1. 阅读扩散模型相关文献（DDPM、DDIM、v-prediction）<br>2. 阅读时序预测领域文献（iTransformer、TimeGrad、CSDI）<br>3. 整理研究现状，明确研究问题 | 文献综述初稿，技术路线确定 |
| **理论分析** | 第3周 | 1. 分析直接预测 vs 残差预测的理论基础<br>2. 推导三种参数化（x₀/ε/v）的数学关系<br>3. 分析条件注入机制的有效性 | 理论分析报告 |
| **模型实现** | 第4-6周 | 1. 实现 iDiffFormer 核心模块<br>2. 实现 FiLM 和变量交叉注意力条件注入<br>3. 实现端到端训练流程<br>4. 代码调试与单元测试<br>**（增加 1 周，1D U-Net + 条件机制较复杂）** | 可运行的模型代码 |
| **基线复现** | 第7周 | 1. 复现 iTransformer 基线<br>2. TimeGrad/CSDI 使用文献报告结果<br>3. 统一评估框架与指标计算 | 基线实验结果 |
| **主实验** | 第8-9周 | 1. ETTh1/h2 数据集实验（96→{96,192,336,720}）<br>2. ETTm1/m2 数据集实验（可选）<br>3. 点预测与概率预测指标评估<br>4. 实验结果整理分析<br>**（预留调参时间）** | 主实验结果与分析 |
| **消融实验** | 第10周 | 1. 参数化策略对比（x₀/ε/v）<br>2. 训练策略对比（端到端 vs 两阶段）<br>3. 条件机制消融（FiLM/CrossAttention）<br>4. MoM 聚合方法验证 | 消融分析报告 |
| **论文撰写** | 第11周 | 1. 撰写摘要与引言<br>2. 撰写方法设计章节<br>3. 撰写实验与分析章节<br>4. 撰写结论与展望 | 论文初稿 |
| **修改完善** | 第12周 | 1. 论文修改与润色<br>2. 准备答辩 PPT<br>3. 答辩材料整理<br>**（预留 buffer 时间）** | 终稿 + 答辩 PPT |

**进度调整说明**：
- **模型实现**：2 周 → 3 周（1D U-Net + FiLM + CrossAttention 实现复杂度较高）
- **基线复现**：简化策略，TimeGrad/CSDI 若无官方代码则使用文献报告结果对比
- **主实验**：1.5 周 → 2 周（预留超参数调优时间）
- **结果分析**：合并至论文撰写阶段，提高效率
- **消融实验**：1 周保持（聚焦关键对比，避免实验过于繁杂）

---

## 四、参考文献

### 核心文献

[1] HO J, JAIN A, ABBEEL P. Denoising diffusion probabilistic models[C]//Advances in Neural Information Processing Systems 33. Red Hook: Curran Associates, 2020: 6840-6851.

[2] SONG J, MENG C, ERMON S. Denoising diffusion implicit models[C]//International Conference on Learning Representations. [S.l.]: OpenReview.net, 2021.

[3] SALIMANS T, HO J. Progressive distillation for fast sampling of diffusion models[C]//International Conference on Learning Representations. [S.l.]: OpenReview.net, 2022.

[4] LIU Y, HU T, ZHANG H, et al. iTransformer: Inverted transformers are effective for time series forecasting[C]//International Conference on Learning Representations. [S.l.]: OpenReview.net, 2024.

[5] RASUL K, SEWARD C, SCHUSTER I, et al. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting[C]//Proceedings of the 38th International Conference on Machine Learning. [S.l.]: PMLR, 2021: 8857-8868.

[6] TASHIRO Y, SONG J, SONG Y, et al. CSDI: Conditional score-based diffusion models for probabilistic time series imputation[C]//Advances in Neural Information Processing Systems 34. Red Hook: Curran Associates, 2021: 24804-24816.

[7] LI Y, LU X, WANG Y, et al. Generative time series forecasting with diffusion, denoise, and disentanglement[C]//Advances in Neural Information Processing Systems 35. Red Hook: Curran Associates, 2022: 23009-23022.

### 时序预测方法

[8] ZHOU H, ZHANG S, PENG J, et al. Informer: Beyond efficient transformer for long sequence time-series forecasting[C]//Proceedings of the 35th AAAI Conference on Artificial Intelligence. Menlo Park: AAAI Press, 2021: 11106-11115.

[9] WU H, XU J, WANG J, et al. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting[C]//Advances in Neural Information Processing Systems 34. Red Hook: Curran Associates, 2021: 22419-22430.

[10] ZHOU T, MA Z, WEN Q, et al. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting[C]//Proceedings of the 39th International Conference on Machine Learning. [S.l.]: PMLR, 2022: 27268-27286.

[11] NIE Y, NGUYEN N H, SINTHONG P, et al. A time series is worth 64 words: Long-term forecasting with transformers[C]//International Conference on Learning Representations. [S.l.]: OpenReview.net, 2023.

[12] WU H, HU T, LIU Y, et al. TimesNet: Temporal 2D-variation modeling for general time series analysis[C]//International Conference on Learning Representations. [S.l.]: OpenReview.net, 2023.

### 扩散模型进展

[13] DHARIWAL P, NICHOL A. Diffusion models beat GANs on image synthesis[C]//Advances in Neural Information Processing Systems 34. Red Hook: Curran Associates, 2021: 8780-8794.

[14] ROMBACH R, BLATTMANN A, LORENZ D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Piscataway: IEEE, 2022: 10684-10695.

[15] KOLLOVIEH M, ANSARI A F, BOHLKE-SCHNEIDER M, et al. Predict, refine, synthesize: Self-guiding diffusion models for probabilistic time series forecasting[C]//Advances in Neural Information Processing Systems 36. Red Hook: Curran Associates, 2023.

### 概率预测评估

[16] GNEITING T, RAFTERY A E. Strictly proper scoring rules, prediction, and estimation[J]. Journal of the American Statistical Association, 2007, 102(477): 359-378.

[17] VASWANI A, SHAZEER N, PARMAR N, et al. Attention is all you need[C]//Advances in Neural Information Processing Systems 30. Red Hook: Curran Associates, 2017: 5998-6008.

---

## 五、预期成果与验收标准

### 5.1 理论成果

1. **直接预测扩散的理论分析**：推导直接预测与残差预测的数学关系，证明直接预测的分布规则性优势
2. **v-prediction 在时序领域的理论推导**：完整推导 x₀/ε/v 三种参数化的转换关系，分析信噪比特性

### 5.2 实验成果（量化目标）

**点预测性能**：

相比现有概率预测方法（如 TimeGrad），在 MSE 和 MAE 指标上实现显著改善（目标改善 ≥20%），同时缩小与确定性方法（如 iTransformer）的性能差距。

**概率预测性能**：

- **CRPS**：分布质量优于现有扩散方法（如 CSDI）
- **Calibration**：50%/90% 预测区间的实际覆盖率接近理论值，误差控制在可接受范围内
- **Sharpness**：在保证覆盖率的前提下，预测区间尽可能窄，体现模型自信度

**效率性能**：

采用混合精度训练和 DDIM 加速采样后，训练和推理效率能够满足实验需求，硬件资源充足。

### 5.3 工程成果

1. **开源代码**：完整的 PyTorch 实现，包括：
   - iDiffFormer 模型定义
   - 训练/测试脚本
   - 评估指标计算
   - 可视化工具（预测曲线、不确定性区间）

2. **实验报告**：在 ETT 系列数据集上的完整实验结果，包括：
   - 主实验对比表格
   - 消融实验分析
   - 可视化图表

3. **论文**：完成开题报告、中期报告、毕业论文（8000+ 字）

### 5.4 验收标准

**最低标准**：
- 点预测性能显著优于现有概率方法（如 TimeGrad）
- 概率预测质量与现有扩散方法（如 CSDI）持平或更优
- 完成至少 2 个数据集（ETTh1/h2）的实验
- 完成多个预测长度（96/192/336/720）的评估

**理想标准**：
- 点预测性能大幅优于现有概率方法，显著缩小与确定性方法的差距
- 概率预测质量（CRPS、Calibration）显著优于现有方法
- 完成 4 个数据集（ETTh1/h2/m1/m2）的完整实验
- 完成所有消融实验并提供深入分析

---

## 六、附录：核心技术要点

### 附录A：符号表

| 符号 | 含义 | 维度 |
|------|------|------|
| $X$ | 历史序列 | $\mathbb{R}^{B \times T \times N}$ |
| $y_{\text{det}}$ | 确定性预测（点预测） | $\mathbb{R}^{B \times L \times N}$ |
| $y_{\text{true}}$ | 真实目标序列 | $\mathbb{R}^{B \times L \times N}$ |
| $z$ | 条件特征向量 | $\mathbb{R}^{B \times N \times d_{\text{model}}}$ |
| $x_t$ | 时间步 $t$ 的噪声状态 | $\mathbb{R}^{B \times L \times N}$ |
| $\epsilon$ | 标准高斯噪声 | $\mathbb{R}^{B \times L \times N}$ |
| $\alpha_t, \bar{\alpha}_t$ | 噪声调度参数 | 标量 |
| $\beta_t$ | 扩散过程方差调度 | 标量 |
| $B$ | Batch size | - |
| $T$ | 历史窗口长度 | 96 |
| $L$ | 预测长度 | {96, 192, 336, 720} |
| $N$ | 变量数（通道数） | 7（ETT数据集） |
| $d_{\text{model}}$ | 模型隐藏维度 | 64-256 |
| $\gamma, \beta$ | FiLM 调制参数（缩放和平移） | $\mathbb{R}^{d_{\text{model}}}$ |

---

### 附录B：直接预测 vs 残差预测

| 特性 | 残差预测 | 直接预测 |
|:----:|:--------:|:--------:|
| **预测目标** | $y_{\text{true}} - y_{det}$ | $y_{\text{true}}$ |
| **目标分布** | 不规则，需归一化 | 规则，无需额外处理 |
| **训练稳定性** | 较低 | 较高 |
| **实现复杂度** | 需要残差归一化器 | 简洁 |

### 附录C：参数化策略对比

| 参数化 | 数学定义 | 优势 | 劣势 |
|:------:|:--------:|:----:|:----:|
| **x₀** | 直接预测 $x_0$ | 直观，收敛性好 | 需要 clamp 稳定 |
| **ε** | 预测噪声 $\epsilon$ | DDPM 标准方法 | 后期信噪比低 |
| **v** | $v = \sqrt{\bar{\alpha}_t} \cdot \epsilon - \sqrt{1-\bar{\alpha}_t} \cdot x_0$ | 信噪比均衡，最稳定 | 计算稍复杂 |

### 附录D：评估指标

**点预测指标**：
- MSE (Mean Squared Error)
- MAE (Mean Absolute Error)
- RMSE (Root Mean Squared Error)

**概率预测指标**：
- CRPS (Continuous Ranked Probability Score)
- Calibration (50%/90% 覆盖率)
- Sharpness (预测区间宽度)
