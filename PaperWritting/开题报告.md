# 开题报告

## 课题名称

**基于扩散机制融合的时间序列概率预测研究**

---

## 一、目标及意义（含国内外研究现状分析）

### 1.1 研究背景

时间序列预测是数据科学领域的核心任务，广泛应用于电力负荷预测、金融风险评估、气象预报、交通流量估计等领域。传统的时间序列预测方法以点预测为主，如 ARIMA、Prophet 等统计方法，以及近年来兴起的深度学习方法如 LSTM、Transformer 及其变体。然而，这些确定性预测方法仅输出单一预测值，无法量化预测的不确定性。在实际决策场景中，决策者不仅需要知道"预测值是多少"，更需要知道"这个预测有多可靠"。例如，电网调度需要考虑负荷预测的置信区间来安排发电计划；金融投资需要评估收益预测的风险范围。因此，能够提供概率分布预测的方法具有重要的理论和实践价值。

### 1.2 国内外研究现状

#### 1.2.1 确定性预测方法的演进

时间序列预测经历了从统计模型到深度学习的发展历程。早期的 ARIMA、指数平滑等统计方法在单变量场景表现良好，但难以捕捉复杂的非线性关系。深度学习方法的兴起带来了 RNN、LSTM、GRU 等序列模型，能够建模长期依赖。2017年 Transformer 架构的提出为时间序列预测带来新的范式，Informer (AAAI 2021)、Autoformer (NeurIPS 2021)、FEDformer (ICML 2022) 等工作在长序列预测上取得突破。近期，iTransformer (ICLR 2024) 提出"倒置"注意力机制，在变量维度而非时间维度应用自注意力，显著提升了多变量预测性能。

#### 1.2.2 扩散模型的发展

扩散概率模型 (Denoising Diffusion Probabilistic Models, DDPM) 由 Ho 等人于 2020 年提出，通过逐步添加噪声再逐步去噪的方式实现生成建模，在图像生成领域取得了卓越成果。DDIM (2021) 提出确定性采样方案，将采样速度提升 10-50 倍。Salimans 等 (2022) 提出 v-prediction 参数化，在各时间步保持均衡的信噪比，显著提升训练稳定性。

#### 1.2.3 扩散模型在时间序列领域的应用

TimeGrad (ICML 2021) 首次将扩散模型引入时间序列预测，使用自回归方式逐步生成未来序列。CSDI (NeurIPS 2021) 提出条件扩散模型用于时间序列插补。D3VAE (NeurIPS 2022) 结合变分自编码器与扩散模型。然而，现有方法存在以下不足：(1) 条件机制设计简单，难以充分利用历史信息；(2) 训练不稳定，收敛困难；(3) 点预测质量牺牲过大，与确定性方法差距明显。

### 1.3 研究意义

#### 1.3.1 理论意义

本研究提出将变量级注意力机制与条件扩散模型深度融合的新范式 iDiffFormer（iTransformer-Diffusion Forecaster）。不同于现有方法的残差预测策略，本研究采用直接预测方式，理论分析表明其分布更规则、收敛更快。同时，本研究系统性地对比了 x₀-prediction、ε-prediction 和 v-prediction 三种参数化策略在时间序列领域的表现，揭示了 v-prediction 在该领域的优越性。这些理论贡献为时间序列概率预测提供了新的研究视角。

#### 1.3.2 实践意义

iDiffFormer 能够同时提供高质量的点预测和可靠的不确定性量化，输出完整的预测分布。在电力、金融、气象等需要风险评估的场景中，决策者可以根据预测区间制定更稳健的策略。同时，本研究提出的 Median-of-Means 聚合方法使点预测质量得到显著提升，有效缩小了与确定性方法的差距，为概率预测方法在实际应用中的推广奠定了基础。

---

## 二、研究设计的基本内容、目标、拟采用的技术方案及措施

### 2.1 研究目标

本研究旨在设计一种融合变量级注意力机制与条件扩散模型的时间序列概率预测方法 iDiffFormer，实现以下目标：
1. 高质量的点预测，缩小与确定性方法的差距
2. 可靠的不确定性量化，提供校准良好的预测区间
3. 高效的训练与推理，适用于实际部署场景

### 2.2 研究内容

#### （1）iTransformer 条件特征提取器设计

采用 iTransformer 作为条件特征提取器（backbone），其核心创新是将自注意力从时间维度转移到变量维度。对于输入序列 $X \in \mathbb{R}^{B \times T \times N}$，通过线性投影将时间维度压缩到隐空间，然后在变量维度应用多头自注意力，捕捉变量间的相互依赖关系。这种设计特别适合多变量时间序列，能够学习如"温度影响负荷"这类变量间的因果关系。

#### （2）直接预测扩散模型设计

区别于现有方法的残差预测策略（预测 $y_{true} - y_{det}$），本研究采用直接预测方式（直接预测 $y_{true}$）。理论分析表明，直接预测的目标分布更规则，无需额外的残差归一化模块，训练更稳定。同时，本研究系统性地实现了三种参数化策略：
- **x₀-prediction**：直接预测干净数据，直观但需要数值截断
- **ε-prediction**：DDPM 标准方法，后期时间步信噪比低
- **v-prediction**：均衡信噪比，本研究推荐的方案

#### （3）条件注入机制设计

设计 FiLM (Feature-wise Linear Modulation) + VariateCrossAttention 的双重条件注入机制：
- **FiLM 层**：通过可学习的缩放参数 $\gamma$ 和平移参数 $\beta$ 对去噪网络的特征进行调制，实现全局条件注入
- **变量交叉注意力**：去噪网络的输出作为 Query，编码器特征作为 Key/Value，实现精细化的条件融合

#### （4）端到端联合训练策略

提出端到端联合训练方案，相比传统两阶段训练具有以下优势：所有参数同时优化，梯度连通；采用固定权重课程学习（MSE 损失 + 扩散损失联合优化）；实验证明收敛更快、性能更稳定。

#### （5）高效采样与 Robust 聚合

- **DDIM 加速采样**：50 步代替 1000 步，速度提升 20 倍
- **批量并行采样**：充分利用 GPU 并行计算能力
- **Median-of-Means 聚合**：将多个采样分组，取组均值的中位数作为最终预测，有效提升点预测质量

### 2.3 技术路线

```
历史序列 X [B, seq_len, N]
    │
    ▼
┌─────────────────────────────────┐
│  iTransformer Backbone          │
│  ├─ Instance Normalization      │
│  ├─ DataEmbedding_inverted      │
│  ├─ Encoder (变量级注意力)       │
│  └─ Projection                  │
└─────────────────────────────────┘
    │                    │
    ▼                    ▼
y_det (确定性预测)    z (条件特征)
                         │
                         ▼
┌─────────────────────────────────┐
│  1D U-Net Denoiser              │
│  ├─ ConditionProjector          │
│  ├─ Encoder (DownBlocks + FiLM) │
│  ├─ Bottleneck (CrossAttention) │
│  └─ Decoder (UpBlocks + FiLM)   │
└─────────────────────────────────┘
    │
    ▼
概率采样 → MoM 聚合 → 最终预测
```

### 2.4 拟采取的措施

1. **模型实现**：基于 PyTorch 深度学习框架实现 iDiffFormer 模型，包括 iTransformer backbone、1D U-Net 去噪网络、条件注入模块等核心组件。

2. **实验验证**：在 ETT 系列数据集（ETTh1、ETTh2、ETTm1、ETTm2）上进行充分的实验验证，与现有确定性预测方法和概率预测方法进行全面对比。

3. **消融分析**：对模型的关键设计进行消融实验，包括参数化策略对比、训练策略对比、条件机制消融等，验证各组件的有效性。

4. **性能优化**：采用混合精度训练、分块采样、DDIM 加速采样等技术手段，确保模型在实际应用中的可行性。

---

## 三、进度安排

本研究计划在 3 个月（12 周）内完成，具体进度安排如下：

| 阶段 | 周次 | 主要任务 | 预期成果 |
|:----:|:----:|---------|---------|
| **文献调研** | 第1-2周 | 1. 阅读扩散模型相关文献（DDPM、DDIM、v-prediction）<br>2. 阅读时序预测领域文献（iTransformer、TimeGrad、CSDI）<br>3. 整理研究现状，明确研究问题 | 文献综述初稿，技术路线确定 |
| **理论分析** | 第3周 | 1. 分析直接预测 vs 残差预测的理论基础<br>2. 推导三种参数化（x₀/ε/v）的数学关系<br>3. 分析条件注入机制的有效性 | 理论分析报告 |
| **模型实现** | 第4-5周 | 1. 实现 iDiffFormer 核心模块<br>2. 实现 FiLM 和变量交叉注意力条件注入<br>3. 实现端到端训练流程<br>4. 代码调试与单元测试 | 可运行的模型代码 |
| **基线复现** | 第6周 | 1. 复现 iTransformer 基线<br>2. 复现 TimeGrad、CSDI 等概率预测方法<br>3. 统一评估框架与指标计算 | 基线实验结果 |
| **主实验** | 第7-8周 | 1. ETTh1/h2 数据集实验（96→{96,192,336,720}）<br>2. ETTm1/m2 数据集实验<br>3. 点预测与概率预测指标评估<br>4. 实验结果整理分析 | 主实验结果与分析 |
| **消融实验** | 第9周 | 1. 参数化策略对比（x₀/ε/v）<br>2. 训练策略对比（端到端 vs 两阶段）<br>3. 条件机制消融（FiLM/CrossAttention）<br>4. MoM 聚合方法验证 | 消融分析报告 |
| **结果分析** | 第10周 | 1. 整理所有实验数据<br>2. 绘制对比图表与可视化<br>3. 分析实验结论，总结技术贡献 | 完整实验图表 |
| **论文撰写** | 第11周 | 1. 撰写摘要与引言<br>2. 撰写方法设计章节<br>3. 撰写实验与分析章节<br>4. 撰写结论与展望 | 论文初稿 |
| **修改完善** | 第12周 | 1. 论文修改与润色<br>2. 准备答辩 PPT<br>3. 答辩材料整理 | 终稿 + 答辩 PPT |

---

## 四、参考文献

### 核心文献

[1] Ho J, Jain A, Abbeel P. Denoising Diffusion Probabilistic Models[C]. Advances in Neural Information Processing Systems (NeurIPS), 2020.

[2] Song J, Meng C, Ermon S. Denoising Diffusion Implicit Models[C]. International Conference on Learning Representations (ICLR), 2021.

[3] Salimans T, Ho J. Progressive Distillation for Fast Sampling of Diffusion Models[C]. International Conference on Learning Representations (ICLR), 2022.

[4] Liu Y, Hu T, Zhang H, et al. iTransformer: Inverted Transformers Are Effective for Time Series Forecasting[C]. International Conference on Learning Representations (ICLR), 2024.

[5] Rasul K, Seward C, Schuster I, et al. Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting[C]. International Conference on Machine Learning (ICML), 2021.

[6] Tashiro Y, Song J, Song Y, et al. CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation[C]. Advances in Neural Information Processing Systems (NeurIPS), 2021.

[7] Li Y, Lu X, Wang Y, et al. Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement[C]. Advances in Neural Information Processing Systems (NeurIPS), 2022.

### 时序预测方法

[8] Zhou H, Zhang S, Peng J, et al. Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting[C]. AAAI Conference on Artificial Intelligence (AAAI), 2021.

[9] Wu H, Xu J, Wang J, et al. Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting[C]. Advances in Neural Information Processing Systems (NeurIPS), 2021.

[10] Zhou T, Ma Z, Wen Q, et al. FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting[C]. International Conference on Machine Learning (ICML), 2022.

[11] Nie Y, Nguyen N H, Sinthong P, et al. A Time Series is Worth 64 Words: Long-term Forecasting with Transformers[C]. International Conference on Learning Representations (ICLR), 2023.

[12] Wu H, Hu T, Liu Y, et al. TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis[C]. International Conference on Learning Representations (ICLR), 2023.

### 扩散模型进展

[13] Dhariwal P, Nichol A. Diffusion Models Beat GANs on Image Synthesis[C]. Advances in Neural Information Processing Systems (NeurIPS), 2021.

[14] Rombach R, Blattmann A, Lorenz D, et al. High-Resolution Image Synthesis with Latent Diffusion Models[C]. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.

[15] Kollovieh M, Ansari A F, Bohlke-Schneider M, et al. Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting[C]. Advances in Neural Information Processing Systems (NeurIPS), 2023.

### 概率预测评估

[16] Gneiting T, Raftery A E. Strictly Proper Scoring Rules, Prediction, and Estimation[J]. Journal of the American Statistical Association (JASA), 2007, 102(477): 359-378.

[17] Vaswani A, Shazeer N, Parmar N, et al. Attention Is All You Need[C]. Advances in Neural Information Processing Systems (NeurIPS), 2017.

---

## 五、预期成果

1. **模型**：iDiffFormer，一种融合变量级注意力与条件扩散的时间序列概率预测模型
2. **论文**：完成开题报告、中期报告、毕业论文
3. **代码**：开源实现，包含完整的训练、测试、可视化代码
4. **实验**：在 ETT 系列数据集上的对比实验和消融分析结果

---

## 六、附录：核心技术要点

### A. 直接预测 vs 残差预测

| 特性 | 残差预测 | 直接预测 |
|:----:|:--------:|:--------:|
| **预测目标** | $y_{true} - y_{det}$ | $y_{true}$ |
| **目标分布** | 不规则，需归一化 | 规则，无需额外处理 |
| **训练稳定性** | 较低 | 较高 |
| **实现复杂度** | 需要残差归一化器 | 简洁 |

### B. 参数化策略对比

| 参数化 | 数学定义 | 优势 | 劣势 |
|:------:|:--------:|:----:|:----:|
| **x₀** | 直接预测 $x_0$ | 直观，收敛性好 | 需要 clamp 稳定 |
| **ε** | 预测噪声 $\epsilon$ | DDPM 标准方法 | 后期信噪比低 |
| **v** | $v = \sqrt{\bar{\alpha}_t} \cdot \epsilon - \sqrt{1-\bar{\alpha}_t} \cdot x_0$ | 信噪比均衡，最稳定 | 计算稍复杂 |

### C. 评估指标

**点预测指标**：
- MSE (Mean Squared Error)
- MAE (Mean Absolute Error)
- RMSE (Root Mean Squared Error)

**概率预测指标**：
- CRPS (Continuous Ranked Probability Score)
- Calibration (50%/90% 覆盖率)
- Sharpness (预测区间宽度)
