# iTransformerDiffusionDirect: ç›´æ¥é¢„æµ‹æ¡ä»¶æ‰©æ•£æ¨¡å‹

## ğŸ“‹ Overview

iTransformerDiffusionDirect æ˜¯ä¸€ä¸ªå…ˆè¿›çš„æ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œç»“åˆäº† **iTransformer** çš„å˜é‡çº§æ³¨æ„åŠ›æœºåˆ¶ä¸ **ç›´æ¥é¢„æµ‹æ‰©æ•£æ¨¡å‹** çš„ä¼˜åŠ¿ã€‚è¯¥æ¨¡å‹æ”¯æŒå¤šç§å‚æ•°åŒ–ç­–ç•¥ï¼ˆxâ‚€/Îµ/vï¼‰ï¼Œé€šè¿‡ç«¯åˆ°ç«¯è”åˆè®­ç»ƒå®ç°é«˜ç²¾åº¦çš„ç‚¹é¢„æµ‹å’Œé«˜è´¨é‡çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹ç‚¹

- **ğŸ”„ å¤šå‚æ•°åŒ–æ”¯æŒ**: xâ‚€é¢„æµ‹ï¼ˆé»˜è®¤ï¼‰ã€Îµé¢„æµ‹ï¼ˆDDPMæ ‡å‡†ï¼‰ã€vé¢„æµ‹ï¼ˆæ¨èï¼‰
- **âš¡ ç«¯åˆ°ç«¯è®­ç»ƒ**: è”åˆä¼˜åŒ–backboneå’Œæ‰©æ•£ç½‘ç»œï¼Œæ¢¯åº¦è¿é€š
- **ğŸ¯ ä¸¤é˜¶æ®µç­–ç•¥**: æ”¯æŒç»å…¸ä¸¤é˜¶æ®µè®­ç»ƒå’Œç«¯åˆ°ç«¯è”åˆè®­ç»ƒ
- **ğŸš€ é«˜æ•ˆé‡‡æ ·**: DDPM/DDIMé‡‡æ ·ï¼Œæ‰¹é‡å¹¶è¡Œå¤„ç†ï¼Œåˆ†å—å†…å­˜ç®¡ç†
- **ğŸ“Š æ¦‚ç‡é¢„æµ‹**: CRPSã€æ ¡å‡†åº¦ã€Sharpnesså…¨æ–¹ä½è¯„ä¼°
- **ğŸ”§ å·¥ç¨‹ä¼˜åŒ–**: AMPæ··åˆç²¾åº¦ã€è¯¾ç¨‹å­¦ä¹ ã€æ—©åœæœºåˆ¶

## ğŸ—ï¸ Architecture

### æ•´ä½“æ¶æ„
```
Input x_hist [B, seq_len, N]
    â”‚
    â–¼ iTransformer Backbone
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Instance Norm â†’ DataEmbedding_inverted â”‚
â”‚  â†’ Encoder (attention across variates)  â”‚
â”‚  â†’ Projection                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                    â”‚
    â–¼                    â–¼
y_det [B, pred_len, N]   z [B, N, d_model]  (æ¡ä»¶ç‰¹å¾)
    â”‚                    â”‚
    â–¼                    â”‚
ç›®æ ‡ y_true (ç”¨äºæ‰©æ•£è®­ç»ƒ) â”‚
    â”‚                    â–¼
    â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚ 1D U-Net Denoiser â”‚
    â”‚              â”‚ - FiLM è°ƒåˆ¶      â”‚
    â”‚              â”‚ - CrossAttention â”‚
    â”‚              â”‚ - æ®‹å·®è¿æ¥        â”‚
    â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
        æ¦‚ç‡é¢„æµ‹é‡‡æ ·
```

### æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | æè¿° | ä½œç”¨ |
|------|------|------|
| **iTransformer Backbone** | å˜é‡çº§æ³¨æ„åŠ›ç¼–ç å™¨ | æå–æ—¶åºç‰¹å¾ï¼Œç”Ÿæˆæ¡ä»¶è¡¨ç¤º |
| **1D U-Net Denoiser** | æ—¶åºå»å™ªç½‘ç»œ | é¢„æµ‹xâ‚€/Îµ/vï¼Œå®ç°æ¦‚ç‡å»ºæ¨¡ |
| **ConditionProjector** | æ¡ä»¶æŠ•å½±å™¨ | èåˆbackboneç‰¹å¾ä¸æ—¶é—´åµŒå…¥ |
| **FiLMLayer** | ç‰¹å¾çº¿æ€§è°ƒåˆ¶ | æ¡ä»¶æ³¨å…¥çš„æ ¸å¿ƒæœºåˆ¶ |
| **VariateCrossAttention** | å˜é‡äº¤å‰æ³¨æ„åŠ› | ç²¾ç»†åŒ–çš„å˜é‡çº§æ¡ä»¶èåˆ |
| **ResidualNormalizer** | æ®‹å·®å½’ä¸€åŒ–å™¨ | ç¨³å®šæ‰©æ•£è®­ç»ƒçš„æ•°å€¼å°ºåº¦ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```bash
conda create -n tslib python=3.8
conda activate tslib
pip install -r requirements.txt
```

### åŸºç¡€è®­ç»ƒ

```bash
# æ ‡å‡†è®­ç»ƒï¼ˆæ¨èv-predictionï¼‰
python run.py \
  --task_name diffusion_forecast \
  --is_training 1 \
  --model iTransformerDiffusionDirect \
  --data ETTh1 \
  --root_path ./dataset/ETT-small/ \
  --data_path ETTh1.csv \
  --seq_len 96 --pred_len 96 \
  --enc_in 7 --dec_in 7 --c_out 7 \
  --d_model 64 --d_ff 64 \
  --parameterization v \
  --training_mode end_to_end \
  --train_epochs 50 \
  --diffusion_steps 1000 \
  --n_samples 100 \
  --use_amp
```

### ä½æ˜¾å­˜è®­ç»ƒï¼ˆ8GB GPUï¼‰

```bash
bash scripts/diffusion_forecast/ETT_script/iTransformerDiffusionDirect_ETTh1_v1.sh
```

### æµ‹è¯•è¯„ä¼°

```bash
python run.py \
  --task_name diffusion_forecast \
  --is_training 0 \
  --model iTransformerDiffusionDirect \
  --data ETTh1 \
  # ... å…¶ä»–å‚æ•°ä¸è®­ç»ƒä¸€è‡´
  --n_samples 100 \
  --use_ddim \
  --ddim_steps 50
```

## âš™ï¸ é…ç½®å‚æ•°

### æ¨¡å‹å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--parameterization` | `v` | å‚æ•°åŒ–ç±»å‹: `x0`/`epsilon`/`v` |
| `--diffusion_steps` | `1000` | æ‰©æ•£æ­¥æ•° |
| `--beta_schedule` | `cosine` | Î²è°ƒåº¦: `linear`/`cosine` |
| `--cond_dim` | `256` | FiLMæ¡ä»¶ç»´åº¦ |
| `--unet_channels` | `[64,128,256,512]` | U-Neté€šé“é…ç½® |
| `--n_samples` | `100` | æ¦‚ç‡é¢„æµ‹é‡‡æ ·æ•° |

### è®­ç»ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--training_mode` | `end_to_end` | è®­ç»ƒæ¨¡å¼: `end_to_end`/`two_stage` |
| `--train_epochs` | `50` | ç«¯åˆ°ç«¯è®­ç»ƒè½®æ•° |
| `--warmup_epochs` | `10` | é¢„çƒ­è½®æ•° |
| `--stage1_epochs` | `30` | Stage1è½®æ•°ï¼ˆtwo_stageæ¨¡å¼ï¼‰ |
| `--stage2_epochs` | `20` | Stage2è½®æ•°ï¼ˆtwo_stageæ¨¡å¼ï¼‰ |
| `--learning_rate` | `1e-4` | åŸºç¡€å­¦ä¹ ç‡ |
| `--use_amp` | `False` | å¯ç”¨æ··åˆç²¾åº¦ï¼ˆçœ30-50%æ˜¾å­˜ï¼‰ |

### é‡‡æ ·å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--use_ddim` | `False` | ä½¿ç”¨DDIMåŠ é€Ÿé‡‡æ · |
| `--ddim_steps` | `50` | DDIMé‡‡æ ·æ­¥æ•° |
| `--chunk_size` | `10` | åˆ†å—é‡‡æ ·å¤§å°ï¼ˆæ˜¾å­˜æ§åˆ¶ï¼‰ |
| `--use_mom` | `True` | Median-of-Meansï¼ˆMSEé™8.3%ï¼‰ |
| `--mom_k` | `10` | MoMåˆ†ç»„æ•° |

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

### ç‚¹é¢„æµ‹æŒ‡æ ‡
- **MSE**: å‡æ–¹è¯¯å·®
- **MAE**: å¹³å‡ç»å¯¹è¯¯å·®  
- **RMSE**: å‡æ–¹æ ¹è¯¯å·®

### æ¦‚ç‡é¢„æµ‹æŒ‡æ ‡
- **CRPS**: è¿ç»­æ’åæ¦‚ç‡åˆ†æ•°
- **Calibration**: 50%/90%è¦†ç›–ç‡æ ¡å‡†
- **Sharpness**: é¢„æµ‹åŒºé—´é”åº¦

## ğŸ¯ å‚æ•°åŒ–é€‰æ‹©æŒ‡å—

### v-Prediction (æ¨è) âœ…
**ä¼˜åŠ¿**: 
- æ‰€æœ‰æ—¶é—´æ­¥ä¿¡å™ªæ¯”å¹³è¡¡
- æ— éœ€clamp()ç¨³å®šé¢„æµ‹
- æ›´å¥½çš„æ¢¯åº¦æµ

**é€‚ç”¨**: å¤§å¤šæ•°åœºæ™¯ï¼Œç‰¹åˆ«æ˜¯è®­ç»ƒç¨³å®šæ€§è¦æ±‚é«˜çš„æƒ…å†µ

### xâ‚€-Prediction (ç¨³å®š) ğŸŸ¡
**ä¼˜åŠ¿**: 
- ç›´æ¥é¢„æµ‹ç›®æ ‡ï¼Œç›´è§‚ç†è§£
- æ”¶æ•›æ€§è´¨è‰¯å¥½

**ç¼ºç‚¹**: 
- éœ€è¦clamp()é˜²æ­¢æ•°å€¼ä¸ç¨³å®š
- æ—©æœŸæ—¶é—´æ­¥ä¿¡å™ªæ¯”ä½

**é€‚ç”¨**: å¿«é€ŸåŸå‹éªŒè¯ï¼Œå¯¹è§£é‡Šæ€§è¦æ±‚é«˜çš„åœºæ™¯

### Îµ-Prediction (æ ‡å‡†) ğŸ”´
**ä¼˜åŠ¿**: 
- DDPMæ ‡å‡†æ–¹æ³•
- ç†è®ºç ”ç©¶å……åˆ†

**ç¼ºç‚¹**: 
- åæœŸæ—¶é—´æ­¥ä¿¡å™ªæ¯”ä½
- è®­ç»ƒä¸å¤Ÿç¨³å®š

**é€‚ç”¨**: ä¸å·²æœ‰DDPMæ–¹æ³•å¯¹æ¯”çš„åœºæ™¯

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. æ˜¾å­˜ä¼˜åŒ–
```bash
--use_amp          # æ··åˆç²¾åº¦ï¼Œçœ30-50%æ˜¾å­˜
--chunk_size 10    # åˆ†å—é‡‡æ ·ï¼Œæ§åˆ¶å³°å€¼æ˜¾å­˜
--batch_size 16    # é€‚å½“å‡å°æ‰¹æ¬¡å¤§å°
```

### 2. è®­ç»ƒåŠ é€Ÿ
```bash
--use_ddim         # DDIMé‡‡æ ·ï¼Œæ¨ç†é€Ÿåº¦æå‡20å€
--ddim_steps 20    # å‡å°‘é‡‡æ ·æ­¥æ•°
--diffusion_steps 100  # å‡å°‘æ‰©æ•£æ­¥æ•°ï¼ˆå¯èƒ½å½±å“è´¨é‡ï¼‰
```

### 3. è´¨é‡æå‡
```bash
--use_mom          # Median-of-Meansï¼ŒMSEé™ä½8.3%
--parameterization v  # æœ€ç¨³å®šçš„å‚æ•°åŒ–
--training_mode end_to_end  # æ¢¯åº¦è¿é€šï¼Œæ›´å¥½æ€§èƒ½
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: æ˜¾å­˜ä¸è¶³ (OOM)**
```bash
# è§£å†³æ–¹æ¡ˆ
--use_amp --chunk_size 5 --batch_size 8
```

**Q: è®­ç»ƒä¸ç¨³å®š**
```bash
# è§£å†³æ–¹æ¡ˆ  
--parameterization v --learning_rate 5e-5
```

**Q: æ¨ç†é€Ÿåº¦æ…¢**
```bash
# è§£å†³æ–¹æ¡ˆ
--use_ddim --ddim_steps 20 --n_samples 50
```

**Q: æ¦‚ç‡é¢„æµ‹è´¨é‡å·®**
```bash
# è§£å†³æ–¹æ¡ˆ
--use_mom --n_samples 200 --diffusion_steps 1000
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
models/
â”œâ”€â”€ iTransformerDiffusionDirect.py  # ä¸»æ¨¡å‹å®ç°
layers/
â”œâ”€â”€ Diffusion_layers.py             # æ‰©æ•£ç»„ä»¶åº“
â”œâ”€â”€ Embed.py                       # åµŒå…¥å±‚
â”œâ”€â”€ SelfAttention_Family.py        # æ³¨æ„åŠ›æœºåˆ¶
â””â”€â”€ Transformer_EncDec.py          # ç¼–è§£ç å™¨
exp/
â”œâ”€â”€ exp_diffusion_forecast.py      # è®­ç»ƒå™¨
scripts/diffusion_forecast/
â””â”€â”€ ETT_script/                    # è®­ç»ƒè„šæœ¬
tests/
â”œâ”€â”€ test_iTransformerDiffusion*.py # å•å…ƒæµ‹è¯•
```

## ğŸ“š ç›¸å…³è®ºæ–‡

- **iTransformer**: [iTransformer: Inverse Transformers Are Effective for Time Series Forecasting](https://arxiv.org/abs/2310.06625) (ICLR 2024)
- **DDPM**: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (NeurIPS 2020)
- **DDIM**: [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502) (ICML 2021)
- **v-Prediction**: [Improved Denoising Diffusion Models](https://arxiv.org/abs/2202.00512) (ICLR 2022)

## ğŸ“„ License

æœ¬é¡¹ç›®éµå¾ª MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªStarï¼**